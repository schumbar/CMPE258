{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQjvc7mgttjEBl/WIQoF6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schumbar/CMPE258/blob/chumbar%2Fassignment_02/assignment_02/CMPE258_assignment02_part_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CMPE 258 Assignment 02 - Part A: Tokenization\n",
        "\n",
        "By Shawn Chumbar"
      ],
      "metadata": {
        "id": "IwwTF8yDVeG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Description\n",
        "In this assignment, we are tasked with replicating a series of experiments conducted with closed and open-source Language Model Models (LLMs) as presented in the references section.\n",
        "\n",
        "Your primary objective is to replicate the experiments and modify them to perform alternative tasks. Instead of using the provided SQL dataset, you will be utilizing different datasets available at [Hugging Face](https://huggingface.co/knowrohit07) to demonstrate your adaptability and creativity.\n",
        "\n",
        "This portion is for the Tokenization portion of the assignment.\n",
        "\n",
        "#### Tasks:\n",
        "1. Reproduce the experiments showcased in the provided YouTube videos, ensuring that you implement all aspects such as prompt-based generation, fine-tuning, Retrieval-Augmented Generation (RAG), local interpreter, function calling, and llama.cpp CPU inference.\n",
        "\n",
        "2. Creatively adapt the Colab notebooks from the YouTube videos to highlight innovative use cases and applications of LLMs.\n",
        "\n",
        "3. If necessary, make the required adjustments to the Colab notebooks to ensure they function correctly. Document these modifications in detail.\n",
        "\n",
        "4. Produce a demonstration video that showcases the functionality of your modified Colab notebooks and your innovative use cases. Ensure the video is linked in your documentation.\n",
        "\n",
        "Please note that it is essential to reference the summarized versions of the YouTube videos available at [Summarize.tech](https://www.summarize.tech/) as they provide an overview of the content and serve as a starting point for your experiments.\n",
        "\n",
        "Your assignment should reflect your ability to replicate and creatively extend the experiments, as well as your capacity to document and present your work effectively. Please feel free to reach out if you encounter any issues with the Colab notebooks that require minor adjustments.\n",
        "\n",
        "### References Used\n",
        "1. [A Hackers' Guide to Language Models](https://www.youtube.com/watch?v=jkrNMKz9pWU): This video showcases various experiments using LLMs.\n",
        "2. [A hacker's guide to open source LLMs - posit::conf(2023)](https://www.youtube.com/watch?v=sYliwvml9Es): This video provides additional insights into LLMs.\n",
        "3. [Summary of A Hackers' Guide to Language Models](https://www.summarize.tech/www.youtube.com/watch?v=jkrNMKz9pWU): Summarized version video titled \"A Hackers' Guide to Language Models\".\n",
        "4. [Summary of A hacker's guide to open source LLMs - posit::conf(2023)](https://www.summarize.tech/www.youtube.com/watch?v=sYliwvml9Es): Summarized version of video titled \"Summary of A hacker's guide to open source LLMs - posit::conf(2023)\".\n",
        "5. [Hugging Face Datasets](https://huggingface.co/knowrohit07): Alternative datasets for experiments."
      ],
      "metadata": {
        "id": "lXUyT_ogVIZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "cCY6l_LLVZOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtJufqljkAgG",
        "outputId": "16fe7aca-63cf-49b8-947a-632b33533259"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9758b30f-4b44-4a30-9322-b6de476c8942",
        "outputId": "cec5ef4f-ec4e-424e-a9b0-810f434340a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1135, 389, 1016, 284, 262, 3650, 284, 2822, 262, 649, 17180, 5761, 386, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from tiktoken import encoding_for_model\n",
        "enc = encoding_for_model(\"text-davinci-003\")\n",
        "toks = enc.encode(\"We are going to the store to buy the new apple vision pro!\")\n",
        "toks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2589d6e7-a9ea-40c1-b1fd-214e19115a28",
        "outputId": "83a92866-2a77-48d9-bb02-e2a8f67860b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We',\n",
              " ' are',\n",
              " ' going',\n",
              " ' to',\n",
              " ' the',\n",
              " ' store',\n",
              " ' to',\n",
              " ' buy',\n",
              " ' the',\n",
              " ' new',\n",
              " ' apple',\n",
              " ' vision',\n",
              " ' pro',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "[enc.decode_single_token_bytes(o).decode('utf-8') for o in toks]"
      ]
    }
  ]
}