{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOxM26bdJJBGei6C7Dn+A6P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schumbar/CMPE258/blob/chumbar%2Fassignment_02/assignment_02/CMPE258_assignment02_part_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CMPE 258 Assignment 02 - Part D: RAG\n",
        "\n",
        "By Shawn Chumbar"
      ],
      "metadata": {
        "id": "IwwTF8yDVeG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Description\n",
        "In this assignment, we are tasked with replicating a series of experiments conducted with closed and open-source Language Model Models (LLMs) as presented in the references section.\n",
        "\n",
        "Your primary objective is to replicate the experiments and modify them to perform alternative tasks. Instead of using the provided SQL dataset, you will be utilizing different datasets available at [Hugging Face](https://huggingface.co/knowrohit07) to demonstrate your adaptability and creativity.\n",
        "\n",
        "This portion is for the Tokenization portion of the assignment.\n",
        "\n",
        "#### Tasks:\n",
        "1. Reproduce the experiments showcased in the provided YouTube videos, ensuring that you implement all aspects such as prompt-based generation, fine-tuning, Retrieval-Augmented Generation (RAG), local interpreter, function calling, and llama.cpp CPU inference.\n",
        "\n",
        "2. Creatively adapt the Colab notebooks from the YouTube videos to highlight innovative use cases and applications of LLMs.\n",
        "\n",
        "3. If necessary, make the required adjustments to the Colab notebooks to ensure they function correctly. Document these modifications in detail.\n",
        "\n",
        "4. Produce a demonstration video that showcases the functionality of your modified Colab notebooks and your innovative use cases. Ensure the video is linked in your documentation.\n",
        "\n",
        "Please note that it is essential to reference the summarized versions of the YouTube videos available at [Summarize.tech](https://www.summarize.tech/) as they provide an overview of the content and serve as a starting point for your experiments.\n",
        "\n",
        "Your assignment should reflect your ability to replicate and creatively extend the experiments, as well as your capacity to document and present your work effectively. Please feel free to reach out if you encounter any issues with the Colab notebooks that require minor adjustments.\n",
        "\n",
        "### References Used\n",
        "1. [A Hackers' Guide to Language Models](https://www.youtube.com/watch?v=jkrNMKz9pWU): This video showcases various experiments using LLMs.\n",
        "2. [A hacker's guide to open source LLMs - posit::conf(2023)](https://www.youtube.com/watch?v=sYliwvml9Es): This video provides additional insights into LLMs.\n",
        "3. [Summary of A Hackers' Guide to Language Models](https://www.summarize.tech/www.youtube.com/watch?v=jkrNMKz9pWU): Summarized version video titled \"A Hackers' Guide to Language Models\".\n",
        "4. [Summary of A hacker's guide to open source LLMs - posit::conf(2023)](https://www.summarize.tech/www.youtube.com/watch?v=sYliwvml9Es): Summarized version of video titled \"Summary of A hacker's guide to open source LLMs - posit::conf(2023)\".\n",
        "5. [Hugging Face Datasets](https://huggingface.co/knowrohit07): Alternative datasets for experiments.\n",
        "\n",
        "Please note that I re-used some code that I created for my CMPE 256 team project last semester."
      ],
      "metadata": {
        "id": "lXUyT_ogVIZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "jS7avoUcYaOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = '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'"
      ],
      "metadata": {
        "id": "1i1grTEnbkpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL9mv2Z9X0UR",
        "outputId": "56ef357f-2d04-4f35-f20a-00ff6e542140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install dotenv"
      ],
      "metadata": {
        "id": "ekBK_gdJXz-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed54bec-ea93-49a7-d26e-8c836e1bd510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=c8c2e373ee82ecf2769ff9ee03583adb44e372a199e22c4e2bf23a30f9c269d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.109.2 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.0 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.7.0 posthog-3.4.1 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.36.3 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.8-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
            "  Downloading langchain_community-0.0.21-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.24 (from langchain)\n",
            "  Downloading langchain_core-0.1.24-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
            "  Downloading langsmith-0.1.2-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.24->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.24->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.8 langchain-community-0.0.21 langchain-core-0.1.24 langsmith-0.1.2 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.0.5.tar.gz (2.4 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qke9Et73LEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3058418e-13e6-472c-f40f-a573fd2fa2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.7-py3-none-any.whl (5.6 kB)\n",
            "Collecting llama-index-agent-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.1.1-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.0 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.7-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.2-py3-none-any.whl (9.5 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.1-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.2-py3-none-any.whl (4.3 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.1-py3-none-any.whl (3.1 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.3-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.2.14)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.12.3)\n",
            "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
            "  Downloading PyMuPDF-1.23.24-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
            "  Downloading pypdf-4.0.2-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.14.1)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.6.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index)\n",
            "  Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.7.0)\n",
            "Collecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama-index)\n",
            "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama-index) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.0->llama-index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Installing collected packages: dirtyjson, pypdf, PyMuPDFb, httpcore, tiktoken, pymupdf, httpx, bs4, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-embeddings-openai, llama-index-multi-modal-llms-openai, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDFb-1.23.22 bs4-0.0.2 dirtyjson-1.0.8 httpcore-1.0.3 httpx-0.26.0 llama-index-0.10.7 llama-index-agent-openai-0.1.1 llama-index-core-0.10.7 llama-index-embeddings-openai-0.1.3 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.2 llama-index-multi-modal-llms-openai-0.1.1 llama-index-program-openai-0.1.2 llama-index-question-gen-openai-0.1.1 llama-index-readers-file-0.1.3 llamaindex-py-client-0.1.13 openai-1.12.0 pymupdf-1.23.24 pypdf-4.0.2 tiktoken-0.6.0\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.19.1-py3-none-any.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.109.2)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.10.0 (from gradio)\n",
            "  Downloading gradio_client-0.10.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.1)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.1.7 (from gradio)\n",
            "  Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.10.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.10.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.36.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=5c49baaeac0e7c2e9a253daf1fa3d40a0a60e34c8e21308e6ad05f40751e2450\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, colorama, aiofiles, gradio-client, gradio\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 12.0\n",
            "    Uninstalling websockets-12.0:\n",
            "      Successfully uninstalled websockets-12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 colorama-0.4.6 ffmpy-0.3.2 gradio-4.19.1 gradio-client-0.10.0 orjson-3.9.14 pydub-0.25.1 python-multipart-0.0.9 ruff-0.2.2 semantic-version-2.10.0 shellingham-1.5.4 tomlkit-0.12.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip3 install llama-index\n",
        "!pip3 install pypdf\n",
        "!pip3 install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pypdf faiss-cpu"
      ],
      "metadata": {
        "id": "3uKRozlBKPU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3885b497-acb0-426f-b7bf-cff5dfbb76fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.21)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.24)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.2)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.24->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.24->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = \"/content/drive/MyDrive/Spring2024/CMPE258/Assignment02/data/\""
      ],
      "metadata": {
        "id": "E7d0p0ctkJ9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Dl_Ln3JwCBoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593192d8-327a-4647-f078-3278a9972f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "import os"
      ],
      "metadata": {
        "id": "vEXcaeY8J-hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "gRn8Qa7dZB4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code\n",
        "This section encompasses the code that we wrote for this project. Please note that the section below is all the helper functions that we created for this project, and that each helper function will be documented in regards to what it does."
      ],
      "metadata": {
        "id": "Lwi9qY0VBSbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions"
      ],
      "metadata": {
        "id": "4baAJ0JlNptS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_filenames(directory):\n",
        "    try:\n",
        "        # List all files and directories in the given directory\n",
        "        filenames = os.listdir(directory)\n",
        "        # Filter out directories if you want only files\n",
        "        filenames = [file for file in filenames if os.path.isfile(os.path.join(directory, file))]\n",
        "        return filenames\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "W_I6c0X2Ebd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file\n",
        "def extract_text(file_path):\n",
        "  pdf_file_path = file_path\n",
        "  with open(pdf_file_path, 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "  # Create a PDF object\n",
        "    pdf_text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "      page = pdf_reader.pages[page_num]\n",
        "      pdf_text += page.extract_text()\n",
        "  return pdf_text\n"
      ],
      "metadata": {
        "id": "ux_5VocyEtRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_all_PDF_files(file_list):\n",
        "  all_text = ''\n",
        "  full_directory_prefix = directory_path\n",
        "  for each_file_name in file_list:\n",
        "    print_name = full_directory_prefix + each_file_name\n",
        "    print(print_name)\n",
        "    all_text = all_text + extract_text(full_directory_prefix + each_file_name)\n",
        "  return all_text"
      ],
      "metadata": {
        "id": "Cl1-itGkEyFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = list_filenames(directory_path)"
      ],
      "metadata": {
        "id": "y6WeTeTwN23x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_list)"
      ],
      "metadata": {
        "id": "pDl3UviNN-Gw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1432ea-c4ba-4379-f01f-88c0a52803ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RP_1.pdf', 'RP_4.pdf', 'RP_2.pdf', 'RP_3.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = extract_text_from_all_PDF_files(file_list)"
      ],
      "metadata": {
        "id": "n7u0mtkbbCSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4982fd92-fcc4-429e-e451-45cd5d74e8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Spring2024/CMPE258/Assignment02/data/RP_1.pdf\n",
            "/content/drive/MyDrive/Spring2024/CMPE258/Assignment02/data/RP_4.pdf\n",
            "/content/drive/MyDrive/Spring2024/CMPE258/Assignment02/data/RP_2.pdf\n",
            "/content/drive/MyDrive/Spring2024/CMPE258/Assignment02/data/RP_3.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean text data\n",
        "\"\"\"\n",
        "function clean_text\n",
        "  input: text - text to be cleaner\n",
        "  output: returns text after performing the following actions:\n",
        "    1. lowercase text\n",
        "    2. remove special chars and numbers\n",
        "    3. Strip extra white spaces\n",
        "\"\"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Function to clean the text data. This includes:\n",
        "    - Lowercasing the text\n",
        "    - Removing special characters and numbers\n",
        "    - Stripping extra white spaces\n",
        "    \"\"\"\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra white spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "bZHJpYujqG1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "function tokenize_and_remove_stopwords\n",
        "input: text to remove stop words from\n",
        "output: returns the text without any stop words within the text.\n",
        "Please note that this only removes the english stopwords.\n",
        "\"\"\"\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    return [word for word in tokens if word not in stop_words]"
      ],
      "metadata": {
        "id": "T6lLXWyQqKys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_text)"
      ],
      "metadata": {
        "id": "q5dYSaUfFGja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0034e1fd-3556-4e15-acfd-7e8d1b08d791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Towards Hetero-Client Federated Multi-Task Learning\n",
            "Yuxiang Lu*, Suizhi Huang*, Yuwen Yang, Shalayiding Sirejiding, Yue Ding, Hongtao Lu\n",
            "Department of Computer Science and Engineering, Shanghai Jiao Tong University\n",
            "Abstract\n",
            "Federated Learning (FL) enables joint training across\n",
            "distributed clients using their local data privately. Fed-\n",
            "erated Multi-Task Learning (FMTL) builds on FL to han-\n",
            "dle multiple tasks, assuming model congruity that identical\n",
            "model architecture is deployed in each client. To relax this\n",
            "assumption and thus extend real-world applicability, we in-\n",
            "troduce a novel problem setting, Hetero-Client Federated\n",
            "Multi-Task Learning (HC-FMTL), to accommodate diverse\n",
            "task setups. The main challenge of HC-FMTL is the model\n",
            "incongruity issue that invalidates conventional aggregation\n",
            "methods. It also escalates the difficulties in accurate model\n",
            "aggregation to deal with data and task heterogeneity inher-\n",
            "ent in FMTL. To address these challenges, we propose the\n",
            "FEDHCA2framework, which allows for federated training\n",
            "of personalized models by modeling relationships among\n",
            "heterogeneous clients. Drawing on our theoretical insights\n",
            "into the difference between multi-task and federated opti-\n",
            "mization, we propose the Hyper Conflict-Averse Aggrega-\n",
            "tion scheme to mitigate conflicts during encoder updates.\n",
            "Additionally, inspired by task interaction in MTL, the Hyper\n",
            "Cross Attention Aggregation scheme uses layer-wise cross\n",
            "attention to enhance decoder interactions while alleviating\n",
            "model incongruity. Moreover, we employ learnable Hyper\n",
            "Aggregation Weights for each client to customize person-\n",
            "alized parameter updates. Extensive experiments demon-\n",
            "strate the superior performance of FEDHCA2in various\n",
            "HC-FMTL scenarios compared to representative methods.\n",
            "Our code will be made publicly available.\n",
            "1. Introduction\n",
            "Federated Learning (FL) [31] has emerged as a prominent\n",
            "paradigm in distributed training, gaining attention in both\n",
            "academic and industrial fields [4, 5, 17, 27, 79]. The FL\n",
            "framework empowers to collaboratively train models across\n",
            "multiple clients, like mobile devices or distributed data cen-\n",
            "ters, while preserving data privacy and reducing communi-\n",
            "cation costs. The impetus behind FL lies in the recogni-\n",
            "tion that harnessing a broader dataset can improve model\n",
            "*Equal contribution.\n",
            "C1 C2\n",
            "Task 1C3 C4\n",
            "Task 2&3 Task 2&3\n",
            "(b)\n",
            "C1 C2 C3 C4\n",
            "Task 1 Task 2 Task 3&4 Task 5&6&7\n",
            "(c)C1 C2\n",
            "(a)Task 1 Task 2 Task 1\n",
            "Aggregation\n",
            "Task SetupsFigure 1. Comparison of different settings in FMTL. (a) Each\n",
            "client is dedicated to a single task. (b) Clients are grouped with\n",
            "peers, and peers in the same group share identical task setting. (c)\n",
            "Our proposed HC-FMTL setting that enables flexible collabora-\n",
            "tion among clients with different task setups.\n",
            "performance, but it also introduces the data heterogeneity\n",
            "issue, as clients often collect samples from non-i.i.d. data\n",
            "distributions. Nevertheless, most FL research is centered on\n",
            "single-task scenarios, overlooking applications that demand\n",
            "simultaneous multi-task processing, e.g., autonomous driv-\n",
            "ing [23]. This gap has led to the integration of Multi-Task\n",
            "Learning (MTL) with FL, giving rise to Federated Multi-\n",
            "Task Learning (FMTL) [22, 54]. While existing FMTL ap-\n",
            "proaches primarily address statistical challenges [51, 65],\n",
            "recent studies [8, 12, 92] have highlighted the importance of\n",
            "task heterogeneity , particularly for dense predictions such\n",
            "as semantic segmentation and depth estimation [11, 58, 74].\n",
            "However, these FMTL methods often assume model\n",
            "congruity among clients, i.e., all participants either engage\n",
            "in a single task or aggregate with peers handling identical\n",
            "task sets, as shown in Fig. 1a, 1b. Considering the dis-\n",
            "crepancy of heterogeneous tasks in practical applications as\n",
            "well as the expensive labor of annotating task-specific la-\n",
            "bels, clients often have different task setups in different en-\n",
            "vironments. Here task setup describes a set of tasks that can\n",
            "vary in both number and type. We define this as a new prob-\n",
            "lem setting: Hetero-Client Federated Multi-Task Learning\n",
            "(HC-FMTL), as depicted in Fig. 1c. HC-FMTL relaxes the\n",
            "constraints on model congruity, facilitating more flexible\n",
            "collaborative learning of various tasks across diverse private\n",
            "1arXiv:2311.13250v1  [cs.CV]  22 Nov 2023data domains and making FMTL scenarios more universally\n",
            "applicable.\n",
            "As a more pervasive setting relaxed from FMTL, HC-\n",
            "FMTL introduces an additional challenge of model incon-\n",
            "gruity , which exacerbates client heterogeneity. This issue\n",
            "arises from clients having different task setups, coupled\n",
            "with the prevalent use of encoder-decoder architectures in\n",
            "vision tasks, leading to a disparity in multi-task model struc-\n",
            "tures. Model incongruity not only increases the complexity\n",
            "of model aggregation but also coexists with the data and\n",
            "task heterogeneity inherent in FMTL. Data heterogeneity\n",
            "is a consequence of clients encountering distinct data do-\n",
            "mains, as clients tend to use data from different domains\n",
            "to handle different target tasks without any overlap, which\n",
            "can result in performance degradation of collective learning.\n",
            "Meanwhile, task heterogeneity, which assigns different ob-\n",
            "jectives for each task, could impede joint optimization and\n",
            "magnify the influence of data heterogeneity.\n",
            "In this paper, we propose a novel framework named\n",
            "FEDHCA2, designed for HC-FMTL. Our goal is to adap-\n",
            "tively discern the relationships among heterogeneous clients\n",
            "and learn personalized yet globally collaborative models\n",
            "that benefit from both synergies and distinctions among\n",
            "clients and tasks. Since model incongruity precludes\n",
            "the straightforward application of conventional aggregation\n",
            "methods in FL, our approach involves the server disassem-\n",
            "bling client models into encoders and decoders for inde-\n",
            "pendent aggregation. For the encoders, we design the Hy-\n",
            "per Conflict-Averse Aggregation scheme to alleviate up-\n",
            "date conflicts among clients. The motivation behind this\n",
            "is grounded in our theoretical analysis (see Theorem 1) that\n",
            "the optimization processes of MTL and FL are closely con-\n",
            "nected and share similarities. By incorporating an approxi-\n",
            "mated gradient smoothing technique, we can find an appro-\n",
            "priate update direction for all clients that mitigates the nega-\n",
            "tive effects of conflicting parameter updates caused by data\n",
            "and task heterogeneity. When aggregating the decoders,\n",
            "we devise the Hyper Cross Attention Aggregation scheme\n",
            "to accommodate client heterogeneity. We draw inspiration\n",
            "from the modeling of task interaction in MTL [55, 77] and\n",
            "apply it to FL. Specifically, we implement a layer-wise cross\n",
            "attention mechanism to model the interplay between client\n",
            "decoders, enabling the capture of both the commonalities\n",
            "and discrepancies among different tasks in a fine-grained\n",
            "manner and thereby alleviating the incongruity at the model\n",
            "level. In addition, the personalized parameter updates for\n",
            "each client are tailored by learnable Hyper Aggregation\n",
            "Weights, which encourage encoders and decoders to adap-\n",
            "tively assimilate knowledge from peers that offer helpful\n",
            "complementary information.\n",
            "Our contributions are summarized as follows:\n",
            "• We introduce a novel setting of Hetero-Client Fed-\n",
            "erated Multi-Task Learning (HC-FMTL) alongside theFEDHCA2framework. It supports collaborative training\n",
            "across clients, each with its unique task setups, address-\n",
            "ing the complexities of data and task heterogeneity, and\n",
            "the newly identified challenge of model incongruity. The\n",
            "relaxed setting broadens the FMTL’s applicability to in-\n",
            "clude a wider variety of clients, tasks, and data situations.\n",
            "• We reveal the connection between the optimization of\n",
            "MTL and FL in Theorem 1 and underscore the impor-\n",
            "tance of circumventing update conflicts among clients,\n",
            "which are exacerbated by data and task heterogeneity in\n",
            "HC-FMTL. We propose a Hyper Conflict-Averse Aggre-\n",
            "gation scheme, designed to alleviate the adverse effects\n",
            "on encoders when absorbing shared knowledge.\n",
            "• We develop a Hyper Cross Attention Aggregation scheme\n",
            "to facilitate task interaction in decoders by modeling the\n",
            "fine-grained cross-task relationships among each decoder\n",
            "layer, tackling both intra- and inter-client heterogeneity.\n",
            "• We evaluate F EDHCA2using a composite of two bench-\n",
            "mark datasets, PASCAL-Context and NYUD-v2, for vari-\n",
            "ous HC-FMTL scenarios. Extensive experiments demon-\n",
            "strate that our approach outperforms existing methods.\n",
            "2. Related Work\n",
            "2.1. Personalized Federated Learning\n",
            "Federated Learning (FL) can be broadly classified into tra-\n",
            "ditional and personalized types, depending on the charac-\n",
            "teristics of data distribution [42, 68]. Traditional Feder-\n",
            "ated Learning, exemplified by the widely used FedAvg [53],\n",
            "has undergone refinements to tackle challenges such as data\n",
            "heterogeneity [1, 33, 34, 72, 73, 83, 85, 91], communi-\n",
            "cation efficiency [18, 29, 32, 52, 90], and privacy con-\n",
            "cerns [3, 25]. In contrast, personalized Federated Learn-\n",
            "ing (pFL) emerges as a specialized variant designed to\n",
            "cater to individual client needs and address data hetero-\n",
            "geneity more effectively [65, 68]. Techniques like meta-\n",
            "learning [19], regularization [24, 35, 67], personalized-\n",
            "head methods [2, 10, 15, 59], and other innovative ap-\n",
            "proaches [36, 37, 76] are widely employed in pFL. In\n",
            "essence, both traditional FL and pFL aim to grapple with\n",
            "the inherent challenge of data heterogeneity.\n",
            "2.2. Multi-Task Learning\n",
            "Multi-Task Learning (MTL) aims to improve overall per-\n",
            "formance while reducing parameters and speeding up train-\n",
            "ing or inference compared to training individual models\n",
            "for each task in isolation [9, 16, 60, 70]. The main di-\n",
            "rections of MTL research can be roughly categorized into\n",
            "network architecture design and multi-task optimization\n",
            "strategy [16]. Network structure design employs methods\n",
            "such as parameter sharing [28, 47, 50, 61, 66], task in-\n",
            "teraction [44, 64, 78, 86, 87, 89], and prediction distilla-\n",
            "tion [69, 81, 82, 88]. Regarding multi-task optimization,\n",
            "2  Client   Client   Client \n",
            "EncoderDecoderHead\n",
            "EncoderDecoderHead\n",
            "EncoderDecoderHead\n",
            "Local Dataset Local Dataset Local Dataset Task Task Task \n",
            "... ...Task\n",
            "Heterogeneity   Client \n",
            "EncoderDecoderHead\n",
            "DecoderHead\n",
            "Local Dataset Task Task       Client \n",
            "EncoderDecoderHead\n",
            "DecoderHead\n",
            "DecoderHead\n",
            "Local Dataset Task Task Task \n",
            "...\n",
            "...Hyper Cross Attention AggregationServerHyper Conflict-Averse Aggregation\n",
            "... ...\n",
            "updateupdate\n",
            "... ...Task\n",
            "Interaction\n",
            "...\n",
            "Model\n",
            "Incongruity\n",
            "Data\n",
            "HeterogeneityHyper Aggregation Weights:layer-wiseFigure 2. Illustration of the HC-FMTL setting and our proposed F EDHCA2framework. HC-FMTL enables clients to have different\n",
            "task setups, from single-task ( e.g. client C1, C2, C3) to multi-task ( e.g. client Ci, CN). HC-FMTL faces three main challenges: model\n",
            "incongruity due to different client model structures, data heterogeneity from different local data domains, and task heterogeneity from\n",
            "varied target tasks. The FL system includes a server and several clients. Our framework decomposes model aggregation into two parts:\n",
            "Hyper Conflict-Averse Aggregation for encoders and Hyper Cross Attention Aggregation for decoders. Learnable Hyper Aggregation\n",
            "Weights are employed to customize personalized parameter updates and are iteratively updated by local model updates from clients.\n",
            "strategies are differentiated into loss balancing and gradient\n",
            "balancing. Loss balancing techniques are designed to pro-\n",
            "duce suitable loss weights to reduce conflicts among mul-\n",
            "tiple tasks [30, 40, 41, 80]. Gradient balancing, on the\n",
            "other hand, addresses task interference by directly adjust-\n",
            "ing gradients, with recent methods concentrating on the for-\n",
            "mulation of a unified gradient vector subject to diverse con-\n",
            "straints [13, 14, 26, 38, 62, 75, 84]. In essence, MTL is\n",
            "dedicated to addressing the intrinsic challenges associated\n",
            "with the heterogeneity of tasks.\n",
            "2.3. Federated Multi-Task Learning\n",
            "It is essential to note that conventional Federated Multi-\n",
            "Task Learning (FMTL) is a branch of personalized Fed-\n",
            "erated Learning that primarily deals with data heterogene-\n",
            "ity across clients [22, 39, 54]. Representative works like\n",
            "MOCHA [65] and FedEM [51] attempt to train models\n",
            "across clients with diverse data distributions within an MTL\n",
            "setting. Recent advancements, including FedBone [12],\n",
            "MAS [92], and MaT-FL [8], have aimed to address both task\n",
            "and data heterogeneity in FMTL. FedBone aggregates the\n",
            "encoders by gradients uploaded from each client, enhancing\n",
            "feature extraction capability. MaT-FL uses dynamic group-\n",
            "ing to combine different client models. MAS distributes\n",
            "varied multi-task models to clients and aggregates models\n",
            "among those with the same task sets. Nevertheless, Fed-\n",
            "Bone and MaT-FL are limited to each client managing a\n",
            "single task (Fig. 1a). MAS supports multi-task clients but isstill limited to identical task sets for aggregation (Fig. 1b).\n",
            "In contrast, our proposed framework enables aggregation\n",
            "across clients with varying numbers and types of tasks, of-\n",
            "fering a more flexible collaboration.\n",
            "3. Methodology\n",
            "3.1. Preliminary\n",
            "Within Hetero-Client Federated Multi-Task Learning (HC-\n",
            "FMTL), clients are assigned flexible task setups, spanning\n",
            "from single-task to multi-task configurations, with an arbi-\n",
            "trary number of tasks per client. Formally, given a pool of\n",
            "Nclients, with client Ciaddressing task sets Tion a cor-\n",
            "responding local dataset Di={(xn,yn)}|Di|\n",
            "n=1, where xnis\n",
            "the input sample and yn=S\n",
            "t∈Tiyn,tcontains the ground-\n",
            "truth labels for all tasks in Ti.\n",
            "In line with FMTL, the objective of HC-FMTL is to train\n",
            "client-specific models θ={θ1, . . . , θ N}that benefit from\n",
            "collaborative optimization with other clients, thus improv-\n",
            "ing performance on their local tasks. The learning objective\n",
            "is to optimize personalized client models with Multi-Task\n",
            "Learning, formulated as follows:\n",
            "min\n",
            "θiX\n",
            "t∈TiLi,t(θi),∀i∈ {1, . . . , N }, (1)\n",
            "where Li,tis the loss function computed over client Ci’s\n",
            "local dataset Difor task t.\n",
            "33.2. Architecture Overview\n",
            "The overall architecture of our proposed F EDHCA2is de-\n",
            "picted in Fig. 2. It contains a pool of clients that perform\n",
            "local training on their private datasets and a server that coor-\n",
            "dinates the aggregation of models from these clients. Con-\n",
            "cerning dense prediction tasks, each client Ciutilizes an\n",
            "encoder-decoder structure consisting of a shared encoder\n",
            "θE\n",
            "i, task-specific decoders {θD,1\n",
            "i, . . . , θD,|Ti|\n",
            "i}and predic-\n",
            "tion heads for each task type they handle. In each commu-\n",
            "nication round r, after all clients finish their local training,\n",
            "they send the model parameters of previous round θ(r−1)\n",
            "and the updates in current round ∆θ(r)to the server. The\n",
            "server first disassembles these models into encoders and\n",
            "decoders and then performs independent aggregation pro-\n",
            "cesses. The prediction heads, due to their varying parame-\n",
            "ter dimensions tailored to specific task outputs, are excluded\n",
            "from the aggregation process and remain localized to indi-\n",
            "vidual clients. The encoder parameters from all Nclients\n",
            "undergo Hyper Conflict-Averse Aggregation. Meanwhile,\n",
            "the server aggregates the parameters of all K=PN\n",
            "i=1Ti\n",
            "decoders through Hyper Cross Attention Aggregation. The\n",
            "entire pipeline of our framework is outlined in Algorithm 1.\n",
            "Algorithm 1 Pseudo-codes for F EDHCA2\n",
            "Input: Nclients {C1, . . . , C N}with private local datasets\n",
            "{D1, . . . ,DN}, client Ciaddresses tasks Ti, total communi-\n",
            "cation rounds R, local epoch E, learning rate η\n",
            "Output: Trained models θ(R)={θ(R)\n",
            "1, . . . , θ(R)\n",
            "N}\n",
            "1:Clients initialize models θ(0)={θ(0)\n",
            "1, . . . , θ(0)\n",
            "N}, each model\n",
            "θiconsists of a shared encoder θE\n",
            "iand|Ti|task-specific de-\n",
            "codersS|Ti|\n",
            "j=1θD,j\n",
            "iand heads\n",
            "2:Server initializes Hyper Aggregation Weights αandβ\n",
            "3:procedure SERVER UPDATE\n",
            "4: foreach communication round r∈ {1, . . . , R }do\n",
            "5: foreach client Ciin parallel do\n",
            "6: ∆θ(r)\n",
            "i←CLIENT UPDATE (θ(r−1)\n",
            "i)\n",
            "7: end for\n",
            "8: Server gathers updates of client models ∆θ(r)\n",
            "9: Update α,βusing ∆θ(r)with Eq. (17)\n",
            "10: θ(r)←AGGREGATION (θ(r−1),∆θ(r))\n",
            "11: end for\n",
            "12:end procedure\n",
            "13:procedure CLIENT UPDATE (θ(r−1)\n",
            "i )\n",
            "14: θi←θ(r−1)\n",
            "i\n",
            "15: foreach local epoch e∈ {1, . . . , E }do\n",
            "16: formini-batch Bi⊂ D ido\n",
            "17: Compute losses Li=P|Ti|\n",
            "j=1Lj\n",
            "i(θi;Bi)\n",
            "18: Update model θi←θi−η∇θiLi\n",
            "19: end for\n",
            "20: end for\n",
            "21: return ∆θ(r)\n",
            "i=θi−θ(r−1)\n",
            "i\n",
            "22:end procedure\n",
            "Shared\n",
            "EncoderDecoder1Head1\n",
            "Decoder2Head2\n",
            "Encoder1Decoder1Head1\n",
            "Decoder2Head2\n",
            "Encoder2Client 1 Client 2\n",
            "AggregationBackward\n",
            "(a) (b)Figure 3. Comparison of optimization in MTL and FL. (a) The\n",
            "shared encoder in MTL is updated by gradient accumulation from\n",
            "all tasks. (b) The clients’ encoders are updated independently and\n",
            "then aggregated in FL.\n",
            "3.3. Hyper Conflict-Averse Aggregation\n",
            "In MTL, the encoder typically employs a parameter-sharing\n",
            "mechanism to capture common task-agnostic information,\n",
            "thereby serving as a general feature extractor for all tasks\n",
            "and enhancing their generalization capabilities. Within our\n",
            "encoder aggregation process, we anticipate that encoders\n",
            "from various clients, each addressing distinct tasks on dif-\n",
            "ferent data domains, are able to acquire general knowledge\n",
            "from other client encoders akin to MTL. To elucidate this,\n",
            "we begin with a theoretical analysis of the correlation be-\n",
            "tween the optimization process of MTL and FL.\n",
            "As depicted in Fig. 3a, consider an MTL scenario where\n",
            "Ntasks are learned simultaneously using a standard multi-\n",
            "decoder architecture. In each mini-batch, the network back-\n",
            "propagates the loss functions L1, . . . ,LNonto the shared\n",
            "encoder θEto calculate its gradient and update:\n",
            "g=NX\n",
            "i=1gi=NX\n",
            "i=1∇θELi, (2)\n",
            "∆θE=−ηg=−ηNX\n",
            "i=1gi, (3)\n",
            "where grepresents the cumulative gradient on the encoder\n",
            "andηsignifies the learning rate. By updating through the\n",
            "summation of gradients from diverse tasks, the encoder as-\n",
            "similates knowledge from various task domains, aligning\n",
            "with the objective of MTL. Meanwhile, in an FL setting\n",
            "shown in Fig. 3b, suppose there are Nclients, each using\n",
            "separate networks with the same architectures as the multi-\n",
            "task model but with independent encoders θE\n",
            "1, . . . , θE\n",
            "Nto\n",
            "learn the same Ntasks. Assuming identical initial weights\n",
            "θEwith MTL, and they are trained for only one mini-batch\n",
            "to obtain gradients gi=∇θELi. FL typically aggregates\n",
            "these encoders by averaging the parameters of all clients:\n",
            "˜θE\n",
            "i=1\n",
            "NNX\n",
            "i=1θE\n",
            "i, (4)\n",
            "where ˜θE\n",
            "iis the aggregated encoder parameters. Consider-\n",
            "4ing its change from the initial weight:\n",
            "∆˜θE\n",
            "i=1\n",
            "NNX\n",
            "i=1∆θE\n",
            "i=1\n",
            "N(−η)NX\n",
            "i=1gi, (5)\n",
            "it means the update for the aggregated encoder mirrors the\n",
            "update of the shared encoder in MTL, if we regard the opti-\n",
            "mizer as capable of automatically scaling the learning rate η\n",
            "in Eq. (3). While FL typically aggregates client models af-\n",
            "ter several local training epochs in a communication round,\n",
            "this implies that there are differences between the learning\n",
            "processes of MTL and FL:\n",
            "Theorem 1 (Difference in optimizing MTL and FL)\n",
            "Given clients with a shared encoder and task-specific\n",
            "decoder structure, the gradient descent in the shared\n",
            "encoder of MTL is equivalent to averaging parameter\n",
            "aggregation in FL, adding an extra term that maximizes\n",
            "the inner product of gradients between all pairs of tasks in\n",
            "each iteration.\n",
            "We provide proofs and in-depth analysis in Appendix A. As\n",
            "the inner product of gradients is a measurement of accor-\n",
            "dance, maximizing the inner product is equal to reducing\n",
            "the conflict of gradients [48]. Hence, Theorem 1 states the\n",
            "necessity of integrating optimization techniques to mitigate\n",
            "gradient conflicts during encoder aggregation in HC-FMTL.\n",
            "Inspired by CAGrad [38], for each communication round,\n",
            "we aim to find an optimal aggregated update ˜Ufor the en-\n",
            "coder that minimizes conflicts while optimizing the main\n",
            "objective with optimization problem:\n",
            "max\n",
            "˜Umin\n",
            "i⟨∆θE\n",
            "i,˜U⟩s.t.∥˜U−∆¯θE∥ ≤c∥∆¯θE∥,(6)\n",
            "where ∆¯θE=1\n",
            "NPN\n",
            "i=1∆θE\n",
            "iis the average parameter up-\n",
            "date and c∈[0,1)is a hyper-parameter controlling the\n",
            "convergence rate. Here mini⟨∆θE\n",
            "i,˜U⟩measures the maxi-\n",
            "mum conflict between client updates and the target update,\n",
            "which is an approximation to the conflict between gradi-\n",
            "ents, as the server only receives parameter updates after\n",
            "several local training epochs rather than the gradients in\n",
            "each iteration. Therefore, maximizing this term can min-\n",
            "imize the conflict in parameter optimization, which is con-\n",
            "sistent with our findings in Theorem 1. With constraintPN\n",
            "i=1wi= 1, wi≥0, solving this problem using La-\n",
            "grangian simplifies to:\n",
            "min\n",
            "wF(w) =U⊤\n",
            "w∆¯θE+p\n",
            "ϕ∥Uw∥, (7)\n",
            "where Uw=1\n",
            "NNX\n",
            "i=1wi∆θE\n",
            "i, ϕ=c2∥∆¯θE∥2.(8)\n",
            "Upon finding the optimum w∗and the optimal λ∗=\n",
            "||Uw∗||/ϕ1/2, we have the unified aggregated update:\n",
            "˜U= ∆¯θE+Uw∗/λ∗= ∆¯θE+√ϕ\n",
            "∥Uw∥Uw. (9)3.4. Hyper Cross Attention Aggregation\n",
            "The significance of task interaction in MTL is well-\n",
            "established [7, 20, 55, 69, 77], as it allows for exchanging\n",
            "knowledge among tasks and benefiting from complemen-\n",
            "tary information. In representative methods [55, 77], task\n",
            "interaction is facilitated by adding the target task’s feature\n",
            "with those from source tasks in decoders, formulated as:\n",
            "zD\n",
            "i=NX\n",
            "j=1γi,j(θD\n",
            "j)⊤zE,∀i∈ {1, . . . , N }, (10)\n",
            "where zEdenotes the output feature of the shared encoder,\n",
            "θD\n",
            "jrepresents the decoder of task j, and (θD\n",
            "j)⊤zEyields\n",
            "task-specific feature from the decoder. The coefficient γi,j\n",
            "manages the flow of features from source to target tasks\n",
            "within the interaction and is usually a learnable parameter.\n",
            "To emulate this task interaction within the FL context, we\n",
            "intuitively aggregate the decoder parameters as follows:\n",
            "˜θD\n",
            "i=NX\n",
            "j=1γi,jθD\n",
            "j. (11)\n",
            "Due to model incongruity in the HC-FMTL environment,\n",
            "the decoder parameters sent to the server originate from\n",
            "diverse clients with heterogeneous tasks. This intricacy\n",
            "leads to a complex landscape where decoders may align\n",
            "or diverge in both data domain and task type, requiring\n",
            "the aggregation process to discern the nuanced relation-\n",
            "ships among them. Our approach improves the decoder ag-\n",
            "gregation by adopting a cross attention mechanism to fur-\n",
            "ther promote the exchange of inter-task knowledge among\n",
            "clients with model incongruity. It calculates dependencies\n",
            "among the local updates of Kdecoders, thereby model-\n",
            "ing the interplay among tasks. Recognizing that decoders\n",
            "often exhibit varied utilities across different network lay-\n",
            "ers [6, 21, 46, 71], we apply a layer-wise strategy [49] to\n",
            "precisely capture the cross-task attention at each decoder\n",
            "layer, allows for a more fine-grained personalized aggrega-\n",
            "tion that can benefit the transfer of task-specific knowledge.\n",
            "The computation of cross attention is defined as:\n",
            "Vl= [∆θD\n",
            "1,l, . . . , ∆θD\n",
            "K,l]⊤, (12)\n",
            "˜Ai,l=Softmax (∆θD\n",
            "i,lV⊤\n",
            "l/√\n",
            "d)Vl, (13)\n",
            "where [·,·]indicates concatenation, ∆θi,land˜Ai,lare the\n",
            "original update and aggregated update for the l-th layer of\n",
            "thei-th decoder, with a dimension of d.\n",
            "3.5. Hyper Aggregation Weights\n",
            "As pointed out by pFL, a unified update for all clients is re-\n",
            "stricted in addressing client heterogeneity. Hence, we pro-\n",
            "pose Hyper Aggregation Weights, which adaptively assess\n",
            "the importance of the aggregated parameters from peers and\n",
            "empower clients with analogous data domains and task ob-\n",
            "5Table 1. Comparison to representative methods using PASCAL-Context for five Single- Task clients and NYUD-v2 for one Multi-Task\n",
            "client. ‘ ↑’ means higher is better and ‘ ↓’ means lower is better. ‘ ∆m%’ denotes the average performance drop w.r.t. local baseline.\n",
            "MethodPASCAL-Context ( ST) NYUD-v2 ( MT)\n",
            "∆m%↑ SemSeg Parts Sal Normals Edge SemSeg Depth Normals Edge\n",
            "mIoU ↑ mIoU ↑ maxF ↑ mErr ↓ odsF↑ mIoU ↑ RMSE ↓ mErr ↓ odsF↑\n",
            "Local 51.69 49.94 80.91 15.76 71.95 41.86 0.6487 20.59 76.46 0.00\n",
            "FedAvg [53] 39.98 37.33 77.56 18.27 69.17 38.94 0.7858 21.62 75.77 -11.76\n",
            "FedProx [34] 44.42 38.10 77.26 18.03 69.39 39.19 0.8068 21.52 76.03 -10.68\n",
            "FedPer [2] 54.51 46.56 78.85 16.95 71.00 44.02 0.6467 21.19 76.61 -1.11\n",
            "Ditto [35] 46.23 39.69 77.99 17.52 69.77 41.49 0.6508 20.60 76.45 -5.57\n",
            "FedAMP [24] 55.98 52.05 80.79 15.74 72.02 41.67 0.6428 20.54 76.40 1.47\n",
            "MaT-FL [8] 57.45 48.63 79.26 17.26 71.23 40.99 0.6352 20.65 76.59 -0.46\n",
            "FEDHCA257.55 52.30 80.71 15.60 72.08 41.47 0.6281 20.53 76.50 2.18\n",
            "jectives to have higher aggregation weights. This enhance-\n",
            "ment reinforces the mutual contribution from complemen-\n",
            "tary information, thus serving as high-level guidance in har-\n",
            "monizing the local updates with the collaborative updates.\n",
            "Specifically, the server maintains a dedicated set of weights\n",
            "for each client, which are applied as follows in the person-\n",
            "alized aggregation:\n",
            "θ(r)\n",
            "i=θ(r−1)\n",
            "i + ∆θ(r)\n",
            "i+ψi˜θi, (14)\n",
            "where ψidenotes the hyper weights for client Ci,i.e.αifor\n",
            "encoder or βifor decoder, and ˜θiis the aggregated update\n",
            "˜Ufrom Eq. (9) or ˜Ai,lfrom Eq. (13). It is worth noting\n",
            "that we implement distinct weights for each decoder layer\n",
            "rather than a single weight value to be consistent with the\n",
            "layer-wise computation of cross attention.\n",
            "Furthermore, we design Hyper Aggregation Weights\n",
            "to be learnable parameters that are dynamically updated\n",
            "throughout the training phase. This adaptability ensures that\n",
            "the weights are optimized in conjunction with the system’s\n",
            "overall objective. By employing the chain rule, we can de-\n",
            "rive the gradient of ψias follows:\n",
            "∇ψiLi= (∇ψiθ(r)\n",
            "i)⊤∇θ(r)\n",
            "iLi= (˜θi)⊤∇θ(r)\n",
            "iLi.(15)\n",
            "To better align this update rule with the FL paradigm, we\n",
            "can reformulate Eq. (15) by substituting gradients with\n",
            "model updates, which is the negative accumulation of gra-\n",
            "dients over batches:\n",
            "∆αi= (˜U(r))⊤∆θE,(r)\n",
            "i, (16)\n",
            "∆βi,l= (˜A(r)\n",
            "i,l)⊤∆θD,(r)\n",
            "i,l. (17)\n",
            "It indicates that the update of Hyper Aggregation Weights\n",
            "can be attained by the alteration in model parameters fol-\n",
            "lowing local training in subsequent communication rounds.\n",
            "4. Experiments\n",
            "4.1. Experimental Setup\n",
            "Datasets. We conduct experiments with two estab-\n",
            "lished benchmark datasets for multi-task dense prediction:\n",
            "PASCAL-Context [56] and NYUD-v2 [63]. The PASCAL-\n",
            "Context dataset contains 4,998 images for training and\n",
            "/g481a /g482/g481 b /g482Figure 4. Evaluation results during training. (a) Parts from\n",
            "PASCAL-Context on single-task client. (b) Normals from NYUD-\n",
            "v2 on multi-task client.\n",
            "5,105 for testing, annotated for five tasks: edge detection\n",
            "(‘Edge’), semantic segmentation (‘SemSeg’), human parts\n",
            "segmentation (‘Parts’), surface normal estimation (‘Nor-\n",
            "mals’), and saliency detection (‘Sal’). The NYUD-v2\n",
            "dataset consists of 795 training images and 654 testing im-\n",
            "ages, all depicting indoor scenes, and provides annotations\n",
            "for four tasks: edge detection, semantic segmentation, sur-\n",
            "face normal estimation, and depth estimation (‘Depth’).\n",
            "To evaluate our algorithm, we configure two HC-FMTL\n",
            "benchmark scenarios: 1) Five single-task clients address\n",
            "five tasks in PASCAL-Context, and one multi-task client ad-\n",
            "dresses four tasks in NYUD-v2; 2) Conversely, four single-\n",
            "task clients address four tasks in NYUD-v2, and one multi-\n",
            "task client addresses five tasks in PASCAL-Context. Fol-\n",
            "lowing MaT-FL [8], we set an equal number of data samples\n",
            "among the respective clients through random partitioning.\n",
            "Implementation. Our client architecture employs a pre-\n",
            "trained Swin-T [43] backbone coupled with simple FCN\n",
            "decoders and heads. Considering the varying capacities of\n",
            "datasets, we use one local epoch for PASCAL-Context and\n",
            "four for NYUD-v2, setting the total number of communi-\n",
            "cation rounds to 100 and the batch size to 8. We train all\n",
            "models using AdamW optimizer [45] with an initial learn-\n",
            "ing rate and weight decay rate set at 1e-4. We implement\n",
            "all methods with PyTorch [57] and run experiments on two\n",
            "NVIDIA RTX4090 GPUs. To adapt existing methods to the\n",
            "HC-FMTL setting, we decouple the models into encoders\n",
            "and decoders for separate aggregation across all methods.\n",
            "Metrics. We adhere to established evaluation metrics.\n",
            "Specifically, we measure semantic segmentation and hu-\n",
            "man parts segmentation using the mean Intersection over\n",
            "6Table 2. Comparison to representative methods using NYUD-v2 for four single-task clients and PASCAL-Context for one multi-task client.\n",
            "MethodNYUD-v2 ( ST) PASCAL-Context ( MT)\n",
            "∆m%↑ SemSeg Depth Normals Edge SemSeg Parts Sal Normals Edge\n",
            "mIoU ↑ RMSE ↓ mErr ↓ odsF↑ mIoU ↑ mIoU ↑ maxF ↑ mErr ↓ odsF↑\n",
            "Local 33.59 0.7129 23.22 75.02 65.80 55.01 83.23 14.21 71.89 0.00\n",
            "FedAvg [53] 25.80 0.8295 24.85 75.31 64.63 52.88 81.08 15.56 68.95 -7.56\n",
            "FedProx [34] 25.96 0.8316 25.20 75.34 64.97 50.78 81.29 15.83 69.81 -8.12\n",
            "FedPer [2] 35.93 0.7460 23.75 75.53 67.78 54.75 82.50 14.75 71.90 -0.16\n",
            "Ditto [35] 28.15 0.7482 23.96 75.42 65.99 51.45 81.74 15.29 69.96 -4.67\n",
            "FedAMP [24] 34.75 0.7103 23.31 75.03 66.08 54.10 83.35 14.20 71.88 0.27\n",
            "MaT-FL [8] 35.05 0.7504 23.39 75.33 67.90 54.78 82.84 14.58 71.94 -0.16\n",
            "FEDHCA234.95 0.7018 23.19 75.03 65.81 55.01 83.18 14.08 71.97 0.75\n",
            "Table 3. Ablation study on our proposed aggregation schemes. ‘+Enc’ and ‘+Dec’ denote the integration of Hyper Conflict-Averse\n",
            "Aggregation for the encoders and Hyper Cross Attention Aggregation for the decoders, respectively.\n",
            "MethodPASCAL-Context ( ST) NYUD-v2 ( MT)∆m%↑SemSeg ↑ Parts↑ Sal↑ Normals ↓ Edge ↑ SemSeg ↑ Depth ↓ Normals ↓ Edge ↑\n",
            "Local 51.69 49.94 80.91 15.76 71.95 41.86 0.6487 20.59 76.46 0.00\n",
            "+Enc 58.38 51.64 80.44 15.65 72.09 41.21 0.6377 20.55 76.50 1.89\n",
            "+Dec 57.39 51.65 80.75 15.69 72.06 41.48 0.6344 20.56 76.41 1.80\n",
            "+Enc+Dec 57.55 52.30 80.71 15.60 72.08 41.47 0.6281 20.53 76.50 2.18\n",
            "Union (mIoU). Saliency detection is evaluated with the\n",
            "maximum F-measure (maxF), while surface normal esti-\n",
            "mation is assessed by the mean error (mErr). Edge de-\n",
            "tection utilizes the optimal-dataset-scale F-measure (odsF),\n",
            "and depth estimation uses the Root Mean Square Error\n",
            "(RMSE). To provide an overall evaluation of different al-\n",
            "gorithms, we calculate the average per-task performance\n",
            "drop [50] relative to the local training baseline, which is\n",
            "trained without aggregation. The formula is as follows:\n",
            "∆m=1\n",
            "NPN\n",
            "i=1(−1)liMFed,i−MLocal,i\n",
            "MLocal,i,where Nis the count\n",
            "of tasks, MFed,iandMLocal ,icorrespond to the perfor-\n",
            "mance of task ifor federated methods and the local base-\n",
            "line, respectively. li= 1if a lower metric value is better for\n",
            "taski, and li= 0otherwise.\n",
            "4.2. Main Results\n",
            "To evaluate the performance of our method, we compare\n",
            "with representative works including two traditional FL ap-\n",
            "proaches FedAvg [53] and FedProx [34], three pFL meth-\n",
            "ods FedPer [2], Ditto [35], FedAMP [24], and one FMTL\n",
            "method MaT-FL [8]. The results presented in Tab. 1 and\n",
            "Tab. 2 demonstrate that F EDHCA2consistently delivers the\n",
            "best performance across most metrics. More importantly,\n",
            "it outperforms all representative methods when considering\n",
            "the average per-task performance drop, which is a widely\n",
            "acknowledged indicator for assessing the overall perfor-\n",
            "mance of MTL. In addition, Fig. 4 shows that F EDHCA2\n",
            "converges faster to a better result on different tasks.\n",
            "4.3. Indepth Analysis\n",
            "Ablation Study. An ablation study is conducted to dis-\n",
            "cern the individual contributions of each component within\n",
            "FEDHCA2, as shown in Tab. 3. The results indicate that in-\n",
            "corporating either encoder or decoder aggregation enhances\n",
            "performance relative to the baseline. The simultaneous em-Table 4. Comparison between different settings. ‘ST+Local’\n",
            "and ‘ST+Ours’ denote the setting with four single-task clients on\n",
            "NYUD-v2, trained with local baseline and F EDHCA2, respec-\n",
            "tively. ‘ST+MT+Ours’ denotes the setting in Tab. 2 trained with\n",
            "our framework. ‘ ∆m’ is calculated w.r.t. ‘ST+Local’ baseline.\n",
            "Setting SemSeg ↑Depth ↓Normals ↓Edge ↑∆m%↑\n",
            "ST+Local 33.59 0.7129 23.22 75.02 0.00\n",
            "ST+Ours 34.71 0.7170 23.25 74.98 0.64\n",
            "ST+MT+Ours 34.95 0.7018 23.19 75.03 1.44\n",
            "Table 5. Comparison to local baseline on the setting with only\n",
            "multi-task clients on two datasets.\n",
            "MethodPASCAL-Context ( MT) NYUD-v2 ( MT)∆m%↑SemSeg ↑Parts↑Normals ↓SemSeg ↑Normals ↓\n",
            "Local 64.87 53.34 14.07 39.81 20.65 0\n",
            "Ours 64.17 54.25 14.01 40.26 20.55 0.53\n",
            "ployment of both Hyper Conflict-Averse and Hyper Cross\n",
            "Attention Aggregations enables F EDHCA2to achieve opti-\n",
            "mal performance across the evaluated configurations. This\n",
            "result supports the idea that using these two aggregation\n",
            "schemes together enhances cooperation among different\n",
            "clients while simultaneously reducing negative conflicts be-\n",
            "tween various tasks.\n",
            "Impact of different FMTL scenarios. To further verify\n",
            "the necessity of introducing our new setting, we conduct ex-\n",
            "periments comparing two scenarios: 1) each client handles\n",
            "a single task, and 2) HC-FMTL encompasses both single-\n",
            "task and multi-task clients. As Tab. 4 illustrates, while\n",
            "FEDHCA2improves upon the local baseline in the single-\n",
            "task client scenario, integrating the multi-task client results\n",
            "in a greater enhancement. This improvement is attributed\n",
            "to the expanded pool of data and the knowledge jointly\n",
            "learned from additional tasks. Further experiments are car-\n",
            "ried out on another scenario of HC-FMTL setting which\n",
            "exclusively involves multi-task clients. Specifically, we se-\n",
            "lect three tasks from PASCAL-Context and two tasks from\n",
            "NYUD-v2 to create two multi-task client setups. The out-\n",
            "7Clients ScaleFigure 5. The performance changes of different methods with the\n",
            "number of clients scaling to 2 and 4 times. ‘ ∆m’ is calculated w.r.t.\n",
            "corresponding local baseline of 1C, 2C, or 4C. When the number\n",
            "of clients increases, our method can consistently provide superior\n",
            "performance, and an overall growth trend could be observed.\n",
            "/g481a /g482mutual\n",
            "speci ﬁc\n",
            "/g481b /g482\n",
            "Figure 6. Hyper Aggregation Weights αfor encoders of the client\n",
            "models. (a) Weights of five single-task clients. (b) Weights of the\n",
            "multi-task client which differs in two stages.\n",
            "comes presented in Tab. 5 align with our primary findings\n",
            "in Sec. 4.2 that nearly all metrics surpass the local baseline,\n",
            "further confirming the efficacy of our approach in this spe-\n",
            "cialized setting.\n",
            "Impact of the number of clients. To assess the effective-\n",
            "ness of F EDHCA2across varying client counts, we conduct\n",
            "tests by scaling the number of clients per task by factors of\n",
            "2 and 4, with the datasets evenly split. As depicted in Fig. 5,\n",
            "FEDHCA2consistently outperforms all comparative meth-\n",
            "ods, exhibiting a positive correlation between the number of\n",
            "clients and performance improvement. This trend contrasts\n",
            "with the performance decline seen with other methods as the\n",
            "client count increases—a result typically attributed to the\n",
            "diminished dataset available to each client and the increased\n",
            "decentralization within the federated learning system. The\n",
            "success of F EDHCA2substantiates the efficacy of the Hy-\n",
            "per Conflict-Averse Aggregation and Hyper Cross Attention\n",
            "Aggregation schemes, especially in scenarios characterized\n",
            "by pronounced data and task heterogeneity.\n",
            "Interaction between tasks. We investigate the dynamic\n",
            "learning process of Hyper Aggregation Weights for both\n",
            "encoders and decoders, aiming to understand their role in\n",
            "facilitating personalized aggregation for different clients.\n",
            "Fig. 6a reveals that the evolution of weights for encoders in\n",
            "single-task clients shows a rising trend, suggesting a consis-\n",
            "L1\n",
            "L2\n",
            "L3L4\n",
            "L5L6\n",
            "STSemSeg STParts\n",
            "STNormals STEdgeSTSal\n",
            "MTDepthMTEdge\n",
            "MTNormalMTSemSegFigure 7. Learned Hyper Aggregation Weights βacross decoders\n",
            "for different tasks, spanning layers from L1 to L6.\n",
            "tent uptake of knowledge from peers throughout the train-\n",
            "ing period. In contrast, the encoder weight of the multi-\n",
            "task client, as depicted in Fig. 6b, exhibits two stages. Ini-\n",
            "tially, the multi-task client mutually assimilates knowledge\n",
            "from single-task clients, a process that is crucial for rapid\n",
            "model convergence. The mutual learning for the multi-task\n",
            "client reaches its peak at about 20 rounds when the encoder\n",
            "weights are comparable. Subsequently, in the second phase,\n",
            "due to the heterogeneity in data and tasks, the multi-task\n",
            "client tends to enhance its feature extraction capabilities\n",
            "specific to its own data domain.\n",
            "Weights for decoders, as shown in Fig. 7, vary signif-\n",
            "icantly across different tasks and decoder layers. From\n",
            "a layer-oriented perspective, the layer closest to the out-\n",
            "put head, i.e., L6, depends least on cross-task information,\n",
            "which ensures that the final output is finely tuned to the\n",
            "specific task. In terms of task-related differences, a phe-\n",
            "nomenon markedly distinct from encoders is observed. For\n",
            "decoders of multi-task client, there is a persistent informa-\n",
            "tion integration from other tasks until the end of training.\n",
            "This empirical evidence substantiates the significance of\n",
            "employing task interaction in decoder aggregation.\n",
            "5. Conclusion\n",
            "In conclusion, this paper addresses the challenges of het-\n",
            "erogeneity in the novel Hetero-Client Federated Multi-Task\n",
            "Learning (HC-FMTL) setting through the novel F EDHCA2\n",
            "framework. By recognizing and tackling the issues of\n",
            "model incongruity, data heterogeneity, and task heterogene-\n",
            "ity, F EDHCA2learns personalized models with synergies\n",
            "of the proposed Hyper Conflict-Averse Aggregation, Hy-\n",
            "per Cross Attention Aggregation, and Hyper Aggregation\n",
            "Weights. Theoretical insights and extensive experiments\n",
            "confirm the effectiveness of our methodology. Our work\n",
            "opens possibilities for more flexible FL systems in diverse\n",
            "and realistic settings. For future work, we aim to delve into\n",
            "greater model heterogeneity that accommodates varied net-\n",
            "work structures across clients, and to integrate specific mod-\n",
            "ules into multi-task clients to further enhance task interac-\n",
            "tion, drawing on advancements in MTL.\n",
            "8References\n",
            "[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew\n",
            "Mattina, Paul Whatmough, and Venkatesh Saligrama. Fed-\n",
            "erated learning based on dynamic regularization. In ICLR ,\n",
            "2021. 2\n",
            "[2] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Ku-\n",
            "mar Singh, and Sunav Choudhary. Federated learning with\n",
            "personalization layers. CoRR , abs/1912.00818, 2019. 2, 6, 7\n",
            "[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah\n",
            "Estrin, and Vitaly Shmatikov. How to backdoor federated\n",
            "learning. In AISTATS , pages 2938–2948, 2020. 2\n",
            "[4] Xiang Bai, Hanchen Wang, Liya Ma, Yongchao Xu, et al.\n",
            "Advancing COVID-19 diagnosis with privacy-preserving\n",
            "collaboration in artificial intelligence. Nature Machine In-\n",
            "telligence , 3(12):1081–1089, 2021. 1\n",
            "[5] Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert, and\n",
            "Shadi Albarqouni. Federated disentangled representation\n",
            "learning for unsupervised brain anomaly detection. Nature\n",
            "Machine Intelligence , 4(8):685–695, 2022. 1\n",
            "[6] David Br ¨uggemann, Menelaos Kanakis, Stamatios Geor-\n",
            "goulis, and Luc Van Gool. Automated search for resource-\n",
            "efficient branched multi-task networks. In BMVC , page 359,\n",
            "2020. 5\n",
            "[7] David Br ¨uggemann, Menelaos Kanakis, Anton Obukhov,\n",
            "Stamatios Georgoulis, and Luc Van Gool. Exploring re-\n",
            "lational context for multi-task dense prediction. In ICCV ,\n",
            "pages 15869–15878, 2021. 5\n",
            "[8] Ruisi Cai, Xiaohan Chen, Shiwei Liu, Jayanth Srinivasa,\n",
            "Myungjin Lee, Ramana Kompella, and Zhangyang Wang.\n",
            "Many-task federated learning: A new problem setting and\n",
            "a simple baseline. In CVPR , pages 5037–5045, 2023. 1, 3,\n",
            "6, 7\n",
            "[9] Rich Caruana. Multitask learning. Machine learning , 28(1):\n",
            "41–75, 1997. 2\n",
            "[10] Hong-You Chen and Wei-Lun Chao. On bridging generic\n",
            "and personalized federated learning for image classification.\n",
            "InICLR , 2022. 2\n",
            "[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\n",
            "Schroff, and Hartwig Adam. Encoder-decoder with atrous\n",
            "separable convolution for semantic image segmentation. In\n",
            "ECCV , pages 801–818, 2018. 1\n",
            "[12] Yiqiang Chen, Teng Zhang, Xinlong Jiang, Qian Chen,\n",
            "Chenlong Gao, and Wuliang Huang. Fedbone: Towards\n",
            "large-scale federated multi-task learning. arXiv preprint\n",
            "arXiv:2306.17465 , 2023. 1, 3\n",
            "[13] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An-\n",
            "drew Rabinovich. Gradnorm: Gradient normalization for\n",
            "adaptive loss balancing in deep multitask networks. In ICML ,\n",
            "pages 794–803, 2018. 3\n",
            "[14] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong,\n",
            "Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov.\n",
            "Just pick a sign: Optimizing deep multitask models with gra-\n",
            "dient sign dropout. In NeurIPS , 2020. 3\n",
            "[15] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay\n",
            "Shakkottai. Exploiting shared representations for personal-\n",
            "ized federated learning. In ICML , pages 2089–2099, 2021.\n",
            "2[16] Michael Crawshaw. Multi-task learning with deep neural\n",
            "networks: A survey. arXiv preprint arXiv:2009.09796 , 2020.\n",
            "2\n",
            "[17] Ittai Dayan, Holger R. Roth, Aoxiao Zhong, Ahmed\n",
            "Harouni, Amilcare Gentili, et al. Federated learning for pre-\n",
            "dicting clinical outcomes in patients with COVID-19. Nature\n",
            "Medicine , 27(10):1735–1743, 2021. 1\n",
            "[18] Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Com-\n",
            "putation and communication efficient federated learning for\n",
            "heterogeneous clients. In ECCV , 2021. 2\n",
            "[19] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Per-\n",
            "sonalized federated learning with theoretical guarantees: A\n",
            "model-agnostic meta-learning approach. NeurIPS , 33:3557–\n",
            "3568, 2020. 2\n",
            "[20] Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L\n",
            "Yuille. Nddr-cnn: Layerwise feature fusing in multi-task\n",
            "cnns by neural discriminative dimensionality reduction. In\n",
            "CVPR , pages 3205–3214, 2019. 5\n",
            "[21] Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learn-\n",
            "ing to branch for multi-task learning. In ICML , pages 3854–\n",
            "3863, 2020. 5\n",
            "[22] Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Mu-\n",
            "rali Annavaram, and Salman Avestimehr. Spreadgnn: De-\n",
            "centralized multi-task federated learning for graph neural\n",
            "networks on molecular data. In AAAI , pages 6865–6873,\n",
            "2022. 1, 3\n",
            "[23] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,\n",
            "Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai\n",
            "Wang, et al. Planning-oriented autonomous driving. In\n",
            "CVPR , pages 17853–17862, 2023. 1\n",
            "[24] Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang,\n",
            "Jiangchuan Liu, Jian Pei, and Yong Zhang. Personalized\n",
            "cross-silo federated learning on non-iid data. In AAAI , pages\n",
            "7865–7873, 2021. 2, 6, 7\n",
            "[25] Yangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and\n",
            "Sanjeev Arora. Evaluating gradient inversion attacks and de-\n",
            "fenses in federated learning. NeurIPS , 34:7232–7241, 2021.\n",
            "2\n",
            "[26] Adri ´an Javaloy and Isabel Valera. Rotograd: Gradient ho-\n",
            "mogenization in multitask learning. In ICLR , 2022. 3\n",
            "[27] Peter Kairouz, H. Brendan McMahan, Brendan Avent, et al.\n",
            "Advances and open problems in federated learning. Found.\n",
            "Trends Mach. Learn. , 14(1-2):1–210, 2021. 1\n",
            "[28] Menelaos Kanakis, David Bruggemann, Suman Saha, Sta-\n",
            "matios Georgoulis, Anton Obukhov, and Luc Van Gool.\n",
            "Reparameterizing convolutions for incremental multi-task\n",
            "learning without task interference. In ECCV , pages 689–707,\n",
            "2020. 2\n",
            "[29] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,\n",
            "Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha\n",
            "Suresh. SCAFFOLD: stochastic controlled averaging for\n",
            "federated learning. In ICML , pages 5132–5143, 2020. 2\n",
            "[30] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\n",
            "learning using uncertainty to weigh losses for scene geome-\n",
            "try and semantics. In CVPR , pages 7482–7491, 2018. 3\n",
            "[31] Jakub Kone ˇcn´y, Brendan McMahan, and Daniel Ramage.\n",
            "Federated optimization: Distributed optimization beyond the\n",
            "datacenter. CoRR , abs/1511.03575, 2015. 1\n",
            "9[32] Jakub Kone ˇcn´y, H. Brendan McMahan, Felix X. Yu, Peter\n",
            "Richt ´arik, Ananda Theertha Suresh, and Dave Bacon. Fed-\n",
            "erated learning: Strategies for improving communication ef-\n",
            "ficiency. CoRR , abs/1610.05492, 2016. 2\n",
            "[33] Qinbin Li, Bingsheng He, and Dawn Song. Model-\n",
            "contrastive federated learning. In CVPR , pages 10713–\n",
            "10722, 2021. 2\n",
            "[34] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,\n",
            "Ameet Talwalkar, and Virginia Smith. Federated optimiza-\n",
            "tion in heterogeneous networks. In MLSys , 2020. 2, 6, 7\n",
            "[35] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia\n",
            "Smith. Ditto: Fair and robust federated learning through per-\n",
            "sonalization. In ICML , pages 6357–6368, 2021. 2, 6, 7\n",
            "[36] Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp,\n",
            "and Qi Dou. FedBN: Federated learning on non-IID features\n",
            "via local batch normalization. In ICLR , 2021. 2\n",
            "[37] Xin-Chun Li, De-Chuan Zhan, Yunfeng Shao, Bingshuai Li,\n",
            "and Shaoming Song. Fedphp: Federated personalization\n",
            "with inherited private models. In ECML PKDD , pages 587–\n",
            "602, 2021. 2\n",
            "[38] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang\n",
            "Liu. Conflict-averse gradient descent for multi-task learning.\n",
            "InNeurIPS , pages 18878–18890, 2021. 3, 5\n",
            "[39] Ken Liu, Shengyuan Hu, Steven Z Wu, and Virginia Smith.\n",
            "On privacy and personalization in cross-silo federated learn-\n",
            "ing. NeurIPS , 35:5925–5940, 2022. 3\n",
            "[40] Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin\n",
            "Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang.\n",
            "Towards impartial multi-task learning. In ICLR , 2021. 3\n",
            "[41] Shikun Liu, Edward Johns, and Andrew J. Davison. End-\n",
            "to-end multi-task learning with attention. In CVPR , pages\n",
            "1871–1880, 2019. 3\n",
            "[42] Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin\n",
            "He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, and Qiang\n",
            "Yang. Vertical federated learning. CoRR , abs/2211.12814,\n",
            "2022. 2\n",
            "[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\n",
            "Zhang, Stephen Lin, and Baining Guo. Swin transformer:\n",
            "Hierarchical vision transformer using shifted windows. In\n",
            "ICCV , pages 10012–10022, 2021. 6\n",
            "[44] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S\n",
            "Yu. Learning multiple tasks with multilinear relationship net-\n",
            "works. NeurIPS , 30, 2017. 2\n",
            "[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\n",
            "regularization. In ICLR , 2019. 6\n",
            "[46] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng,\n",
            "Tara Javidi, and Rogerio Feris. Fully-adaptive feature shar-\n",
            "ing in multi-task networks with applications in person at-\n",
            "tribute classification. In CVPR , pages 5334–5343, 2017. 5\n",
            "[47] Yuxiang Lu, Shalayiding Sirejiding, Yue Ding, Chun-\n",
            "lin Wang, and Hongtao Lu. Prompt guided trans-\n",
            "former for multi-task dense prediction. arXiv preprint\n",
            "arXiv:2307.15362 , 2023. 2\n",
            "[48] Linhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan\n",
            "Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, and\n",
            "Shirui Pan. MAMDR: A model agnostic learning framework\n",
            "for multi-domain recommendation. In ICDE , pages 3079–\n",
            "3092, 2023. 5[49] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu.\n",
            "Layer-wised model aggregation for personalized federated\n",
            "learning. In CVPR , pages 10092–10101, 2022. 5\n",
            "[50] Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas\n",
            "Kokkinos. Attentive single-tasking of multiple tasks. In\n",
            "CVPR , pages 1851–1860, 2019. 2, 7\n",
            "[51] Othmane Marfoq, Giovanni Neglia, Aur ´elien Bellet, Laetitia\n",
            "Kameni, and Richard Vidal. Federated multi-task learning\n",
            "under a mixture of distributions. In NeurIPS , pages 15434–\n",
            "15447, 2021. 1, 3\n",
            "[52] Brendan McMahan, Eider Moore, Daniel Ramage, Seth\n",
            "Hampson, and Blaise Aguera y Arcas. Communication-\n",
            "efficient learning of deep networks from decentralized data.\n",
            "InAISTATS , pages 1273–1282, 2017. 2\n",
            "[53] Brendan McMahan, Eider Moore, Daniel Ramage, et al.\n",
            "Communication-efficient learning of deep networks from de-\n",
            "centralized data. In AISTATS , pages 1273–1282, 2017. 2, 6,\n",
            "7\n",
            "[54] Jed Mills, Jia Hu, and Geyong Min. Multi-task federated\n",
            "learning for personalised deep neural networks in edge com-\n",
            "puting. TPDS , 33(3):630–641, 2021. 1, 3\n",
            "[55] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar-\n",
            "tial Hebert. Cross-stitch networks for multi-task learning. In\n",
            "CVPR , pages 3994–4003, 2016. 2, 5\n",
            "[56] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\n",
            "Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\n",
            "Alan Yuille. The role of context for object detection and se-\n",
            "mantic segmentation in the wild. In CVPR , pages 891–898,\n",
            "2014. 6\n",
            "[57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\n",
            "James Bradbury, Gregory Chanan, Trevor Killeen, Zem-\n",
            "ing Lin, Natalia Gimelshein, and Luca Antiga. Pytorch:\n",
            "An imperative style, high-performance deep learning library.\n",
            "NeurIPS , 32, 2019. 6\n",
            "[58] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\n",
            "sion transformers for dense prediction. In ICCV , pages\n",
            "12179–12188, 2021. 1\n",
            "[59] Jiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi,\n",
            "et al. Balanced meta-softmax for long-tailed visual recogni-\n",
            "tion. NeurIPS , 33:4175–4186, 2020. 2\n",
            "[60] Sebastian Ruder. An overview of multi-task learning in deep\n",
            "neural networks. arXiv preprint arXiv:1706.05098 , 2017. 2\n",
            "[61] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and\n",
            "Anders Søgaard. Latent multi-task architecture learning. In\n",
            "AAAI , pages 4822–4829, 2019. 2\n",
            "[62] Ozan Sener and Vladlen Koltun. Multi-task learning as\n",
            "multi-objective optimization. In NeurIPS , pages 525–536,\n",
            "2018. 3\n",
            "[63] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\n",
            "Fergus. Indoor segmentation and support inference from\n",
            "rgbd images. In ECCV , pages 746–760, 2012. 6\n",
            "[64] Shalayiding Sirejiding, Yuxiang Lu, Hongtao Lu, and Yue\n",
            "Ding. Scale-aware task message transferring for multi-task\n",
            "learning. In ICME , pages 1859–1864, 2023. 2\n",
            "[65] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and\n",
            "Ameet Talwalkar. Federated multi-task learning. In NeurIPS ,\n",
            "pages 4424–4434, 2017. 1, 2, 3\n",
            "10[66] Guolei Sun, Thomas Probst, Danda Pani Paudel, Nikola\n",
            "Popovi ´c, Menelaos Kanakis, Jagruti Patel, Dengxin Dai, and\n",
            "Luc Van Gool. Task switching network for multi-task learn-\n",
            "ing. In ICCV , pages 8291–8300, 2021. 2\n",
            "[67] Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personal-\n",
            "ized federated learning with moreau envelopes. NeurIPS ,\n",
            "33:21394–21405, 2020. 2\n",
            "[68] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.\n",
            "Towards Personalized Federated Learning. IEEE Transac-\n",
            "tions on Neural Networks and Learning Systems , pages 1–\n",
            "17, 2022. 2\n",
            "[69] Simon Vandenhende, Stamatios Georgoulis, and Luc Van\n",
            "Gool. Mti-net: Multi-scale task interaction networks for\n",
            "multi-task learning. In ECCV , pages 527–543, 2020. 2, 5\n",
            "[70] Simon Vandenhende, Stamatios Georgoulis, Wouter\n",
            "Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc\n",
            "Van Gool. Multi-task learning for dense prediction tasks: A\n",
            "survey. IEEE TPAMI , 44(7):3614–3633, 2021. 2\n",
            "[71] Matthew Wallingford, Hao Li, Alessandro Achille, Avinash\n",
            "Ravichandran, Charless Fowlkes, Rahul Bhotika, and Ste-\n",
            "fano Soatto. Task adaptive parameter sharing for multi-task\n",
            "learning. In CVPR , pages 7561–7570, 2022. 5\n",
            "[72] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris S.\n",
            "Papailiopoulos, and Yasaman Khazaeni. Federated learning\n",
            "with matched averaging. In ECCV , 2020. 2\n",
            "[73] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and\n",
            "H. Vincent Poor. Tackling the objective inconsistency prob-\n",
            "lem in heterogeneous federated optimization. In NeurIPS ,\n",
            "2020. 2\n",
            "[74] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\n",
            "Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\n",
            "Pyramid vision transformer: A versatile backbone for dense\n",
            "prediction without convolutions. In ICCV , pages 568–578,\n",
            "2021. 1\n",
            "[75] Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.\n",
            "Gradient vaccine: Investigating and improving multi-task\n",
            "optimization in massively multilingual models. In ICLR ,\n",
            "2021. 3\n",
            "[76] Chulin Xie, De-An Huang, Wenda Chu, Daguang Xu,\n",
            "Chaowei Xiao, Bo Li, and Anima Anandkumar. Perada:\n",
            "Parameter-efficient and generalizable federated learning per-\n",
            "sonalization with guarantees. CoRR , abs/2302.06637, 2023.\n",
            "2\n",
            "[77] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe.\n",
            "Pad-net: Multi-tasks guided prediction-and-distillation net-\n",
            "work for simultaneous depth estimation and scene parsing.\n",
            "InCVPR , pages 675–684, 2018. 2, 5\n",
            "[78] Yangyang Xu, Xiangtai Li, Haobo Yuan, Yibo Yang, and\n",
            "Lefei Zhang. Multi-task learning with multi-query trans-\n",
            "former for dense prediction. IEEE TCSVT , 2023. 2\n",
            "[79] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.\n",
            "Federated machine learning: Concept and applications. ACM\n",
            "Trans. Intell. Syst. Technol. , 10(2):12:1–12:19, 2019. 1\n",
            "[80] Feiyang Ye, Baijiong Lin, Zhixiong Yue, Pengxin Guo, Qiao\n",
            "Xiao, and Yu Zhang. Multi-objective meta learning. In\n",
            "NeurIPS , pages 21338–21351, 2021. 3[81] Hanrong Ye and Dan Xu. Inverted pyramid multi-task trans-\n",
            "former for dense scene understanding. In ECCV , pages 514–\n",
            "530, 2022. 2\n",
            "[82] Hanrong Ye and Dan Xu. Invpt++: Inverted pyramid multi-\n",
            "task transformer for visual scene understanding. arXiv\n",
            "preprint arXiv:2306.04842 , 2023. 2\n",
            "[83] Fuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di Wang,\n",
            "Chenchen Liu, Zhi Tian, and Xiang Chen. Fed2: Feature-\n",
            "aligned federated learning. In ACM SIGKDD , pages 2066–\n",
            "2074, 2021. 2\n",
            "[84] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine,\n",
            "Karol Hausman, and Chelsea Finn. Gradient surgery for\n",
            "multi-task learning. In NeurIPS , 2020. 3\n",
            "[85] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh,\n",
            "Kristjan H. Greenewald, Trong Nghia Hoang, and Yasaman\n",
            "Khazaeni. Bayesian nonparametric federated learning of\n",
            "neural networks. In ICML , pages 7252–7261, 2019. 2\n",
            "[86] Xiaoya Zhang, Ling Zhou, Yong Li, Zhen Cui, Jin Xie, and\n",
            "Jian Yang. Transfer vision patterns for multi-task pixel learn-\n",
            "ing. In ACM MM , pages 97–106, 2021. 2\n",
            "[87] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang\n",
            "Li, and Jian Yang. Joint task-recursive learning for semantic\n",
            "segmentation and depth estimation. In ECCV , pages 235–\n",
            "251, 2018. 2\n",
            "[88] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe,\n",
            "and Jian Yang. Pattern-affinitive propagation across depth,\n",
            "surface normal and semantic segmentation. In CVPR , pages\n",
            "4106–4115, 2019. 2\n",
            "[89] Ling Zhou, Zhen Cui, Chunyan Xu, Zhenyu Zhang, Chaoqun\n",
            "Wang, Tong Zhang, and Jian Yang. Pattern-structure diffu-\n",
            "sion for multi-task learning. In CVPR , pages 4514–4523,\n",
            "2020. 2\n",
            "[90] Ligeng Zhu, Hongzhou Lin, Yao Lu, Yujun Lin, and Song\n",
            "Han. Delayed gradient averaging: Tolerate the communi-\n",
            "cation latency for federated learning. NeurIPS , 34:29995–\n",
            "30007, 2021. 2\n",
            "[91] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free\n",
            "knowledge distillation for heterogeneous federated learning.\n",
            "InICML , pages 12878–12889, 2021. 2\n",
            "[92] Weiming Zhuang, Yonggang Wen, Lingjuan Lyu, and Shuai\n",
            "Zhang. Mas: Towards resource-efficient federated multiple-\n",
            "task learning. In ICCV , pages 23414–23424, 2023. 1, 3\n",
            "11Preprint\n",
            "MULTI -TASK REINFORCEMENT LEARNING WITH\n",
            "MIXTURE OF ORTHOGONAL EXPERTS\n",
            "Ahmed Hendawy1,2∗, Jan Peters1,2,3,4, Carlo D’Eramo1,2,5\n",
            "1Department of Computer Science, TU Darmstadt, Germany\n",
            "2Hessian Center for Artificial Intelligence (Hessian.ai), Germany\n",
            "3German Research Center for AI (DFKI), Systems AI for Robot Learning\n",
            "4Center for Cognitive Science, TU Darmstadt, Germany\n",
            "5Center for Artificial Intelligence and Data Science, University of W ¨urzburg, Germany\n",
            "ABSTRACT\n",
            "Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem\n",
            "of endowing agents with skills that generalize across a variety of problems. To\n",
            "this end, sharing representations plays a fundamental role in capturing both unique\n",
            "and common characteristics of the tasks. Tasks may exhibit similarities in terms\n",
            "of skills, objects, or physical properties while leveraging their representations\n",
            "eases the achievement of a universal policy. Nevertheless, the pursuit of learn-\n",
            "ing a shared set of diverse representations is still an open challenge. In this paper,\n",
            "we introduce a novel approach for representation learning in MTRL that encapsu-\n",
            "lates common structures among the tasks using orthogonal representations to pro-\n",
            "mote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE),\n",
            "leverages a Gram-Schmidt process to shape a shared subspace of representations\n",
            "generated by a mixture of experts. When task-specific information is provided,\n",
            "MOORE generates relevant representations from this shared subspace. We assess\n",
            "the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid\n",
            "and MetaWorld, showing that MOORE surpasses related baselines and establishes\n",
            "a new state-of-the-art result on MetaWorld.\n",
            "1 I NTRODUCTION\n",
            "Reinforcement Learning (RL) has shown outstanding achievements in a wide array of decision-\n",
            "making problems, including Atari games (Mnih et al., 2013; Hessel et al., 2018a), board games (Sil-\n",
            "ver et al., 2016; 2017), high-dimensional continuous control (Schulman et al., 2015; 2017; Haarnoja\n",
            "et al., 2018), and robot manipulation (Yu et al., 2019). Despite the success of RL, generalizing the\n",
            "learned policy to a broader set of related tasks remains an open challenge. Multi-Task Reinforce-\n",
            "ment Learning (MTRL) is introduced to scale up the RL framework, holding the promise of enabling\n",
            "learning a universal policy capable of addressing multiple tasks concurrently. To this end, sharing\n",
            "knowledge is key in MTRL (Teh et al., 2017; D’Eramo et al., 2020; Sodhani et al., 2021; Sun et al.,\n",
            "2022). However, deciding upon the kind of knowledge to share, and the set of tasks to share that\n",
            "knowledge, is crucial for designing an efficient MTRL algorithm. Human beings exhibit remark-\n",
            "able adaptability across a multitude of tasks by mastering some essential skills as well as having the\n",
            "intuition of physical laws. Similarly, MTRL can benefit from sharing representations that capture\n",
            "unique and diverse properties across multiple tasks, easing the learning of an effective policy.\n",
            "Recently, sharing compositional knowledge (Devin et al., 2017; Calandriello et al., 2014; Sodhani\n",
            "et al., 2021; Sun et al., 2022) has shown potential as an effective form of knowledge transfer in\n",
            "MTRL. For example, Devin et al. (2017) investigates the challenges of knowledge transfer between\n",
            "distinct robots and tasks by sharing a modular structure of the policy. This approach leverages task-\n",
            "specific and robot-specific modules, enabling effective transfer of knowledge. Nevertheless, this ap-\n",
            "proach requires manual intervention to determine the allocation of responsibilities for each module,\n",
            "given some prior knowledge. In contrast, we aim for an end-to-end approach that implicitly learns\n",
            "and shares the prominent components of the tasks for acquiring a universal policy. Furthermore,\n",
            "∗Ahmed Hendawy (ahmed.hendawy@tu-darmstadt.de) is the corresponding author.\n",
            "1arXiv:2311.11385v1  [cs.LG]  19 Nov 2023Preprint\n",
            "CARE (Sodhani et al., 2021) adopts a different strategy by focusing on learning representations of\n",
            "different skills and objects encountered by the tasks through the utilization of context information.\n",
            "However, there is no inherent guarantee of achieving diversity among the learned representations.\n",
            "In this work, our goal is to ensure the diversity of the learned representations to maximize the rep-\n",
            "resentation capacity and avoid collapsing to similar representations.\n",
            "Consequently, we propose a novel approach for representation learning in MTRL to share a set\n",
            "of representations that capture unique and common properties shared by all the tasks. To ensure\n",
            "the richness and diversity of these shared representations, our approach solves a constrained opti-\n",
            "mization problem that orthogonalizes the representations generated by a mixture of experts via the\n",
            "application of the Gram-Schmidt process, thus favoring independence between the representations.\n",
            "Hence, we name our approach, Mixture OfORthogonal Experts (MOORE). Notably, the orthogo-\n",
            "nal representations act as bases that span a subspace of representations leveraged by all tasks where\n",
            "task-relevant properties can be interpolated. More formally, we show that these orthogonal repre-\n",
            "sentations are a set of orthogonal vectors belonging to a particular Riemannian manifold where the\n",
            "inner product is defined, known as Stiefel manifold (James, 1977). Interestingly, the Stiefel mani-\n",
            "fold has recently drawn substantial attention within the field of machine learning (Ozay & Okatani,\n",
            "2016; Huang et al., 2018a; Li et al., 2019; Chaudhry et al., 2020). For example, several works focus\n",
            "on enhancing the generalization and stability of neural networks by solving an optimization prob-\n",
            "lem to learn parameters lying in the Stiefel manifold. Another line of work aims at reducing the\n",
            "redundancy of the learned features by forcing the weights to inhabit the Stiefel manifold. Addition-\n",
            "ally, Chaudhry et al. (2020) proposes a continual learning method that forces each task to learn in a\n",
            "different subspace, thus reducing task interference through orthogonalizing the weights.\n",
            "In this paper, our objective is to ensure diversity among the shared representations across tasks by\n",
            "imposing a constraint that forces these representations to exist within the Stiefel manifold. Thus, we\n",
            "aim to leverage the extracted representations, in combination with deep RL algorithms, to enhance\n",
            "the generalization capabilities of MTRL policies. In the following, we provide a rigorous mathemat-\n",
            "ical formulation of the problem in the form of a Block Contextual Markov Decision Process (MDP)\n",
            "based on latent representations belonging to the Stiefel manifold. Then, we devise our Mixture Of\n",
            "Orthogonal Experts (MOORE) approach for obtaining orthogonal task representations through the\n",
            "application of a Gram-Schmidt process on the latent features extracted from a mixture of experts.\n",
            "We empirically validate MOORE on two widely used and challenging MTRL problems, namely\n",
            "MiniGrid (Chevalier-Boisvert et al., 2023) and MetaWorld (Yu et al., 2019), comparing to recent\n",
            "baselines for MTRL. Remarkably, MOORE establishes a new state-of-the-art performance on the\n",
            "MetaWorld MT10-rand and MT50-rand collections of tasks.\n",
            "To recap, the contribution of this work is threefold: (i) We propose a mathematical formulation of\n",
            "the Block Contextual MDP, that describes the MTRL problem where the state is encoded in the\n",
            "Stiefel manifold through a mapping function. (ii) We devise a novel representation learning method\n",
            "for Multi-Task Reinforcement Learning, that leverages a modular structure of the shared repre-\n",
            "sentations to capture common components across multiple tasks. Our approach, named MOORE,\n",
            "learns a mixture of orthogonal experts by encouraging diversity through the orthogonality of their\n",
            "corresponding representations. (iii) Our approach outperforms related baselines and achieves state-\n",
            "of-the-art results on the MetaWorld benchmark.\n",
            "2 P RELIMINARIES\n",
            "Consider an MDP (Bellman, 1957; Puterman, 1995) defined as a tuple M=<S,A,P, r, ρ, γ > ,\n",
            "where Sis the state space, Ais the action space, P:S × A → S is the transition distribution\n",
            "where P(s′|s, a)is the probability of reaching s′when being in state sand performing action a,\n",
            "r:S × A → Ris the reward function, ρis the initial state distribution, and γ∈(0,1]is the\n",
            "discount factor. A policy πmaps each state sto a probability distribution over the action space\n",
            "A. The goal of RL is to learn a policy that maximizes the expected cumulative discounted return\n",
            "J(π) =Eπ[P∞\n",
            "t=0γtr(st, at)]. We parameterize the policy πθ(at|st)and optimize the parameters θ\n",
            "to maximize J(πθ) =J(θ).\n",
            "2Preprint\n",
            "2.1 M ULTI -TASK REINFORCEMENT LEARNING\n",
            "In MTRL, the agent interacts with different tasks τ∈ T , where each task τis a different MDP\n",
            "Mτ=<Sτ,Aτ,Pτ, rτ, ρτ, γτ>. The goal of MTRL is to learn a single policy πthat maximizes\n",
            "the expected accumulated discounted return averaged across all tasks J(θ) =P\n",
            "τJτ(θ). Tasks can\n",
            "differ in one or more components of the MDP. A class of problems in MTRL assumes only a change\n",
            "in the reward function rτ. This can be exemplified by a navigation task where the agent learns to\n",
            "reach multiple goal positions or a robotic manipulation task where the object’s position changes.\n",
            "In this class, the goal position is usually augmented to the state representation. Besides the reward\n",
            "function, a bigger set of problems deals with changes in other components. In this category, tasks\n",
            "access a subset of the state space Sτ, while the true state space Sis unknown. For example, learning\n",
            "a universal policy that performs multiple manipulation tasks interacting with different objects (Yu\n",
            "et al., 2019). Task information should be provided either in the form of task ID (e.g. one-hot vector)\n",
            "or metadata, e.g., task description (Sodhani et al., 2021).\n",
            "Following Sodhani et al. (2021), we define our MTRL problem as a Block Contextual MDP (BC-\n",
            "MDP). It is defined by 5-tuple <C,S,A, γ,M′>where Cis the context space, Sis the true state\n",
            "space, Ais the action space, while M′is a mapping function that provides the task-specific MDP\n",
            "components given the context c∈ C,M′(c) ={rc,Pc,Sc, ρc}. As of now, we refer to the task τ\n",
            "and its components by the context parameter denoted as c.\n",
            "3 R ELATED WORKS\n",
            "Sharing knowledge among tasks is a key benefit of MTRL over single-task learning, as broadly ana-\n",
            "lyzed by several works that propose disparate ways to leverage the relations between tasks (D’Eramo\n",
            "et al., 2020; Sodhani et al., 2021; Sun et al., 2022; Calandriello et al., 2014; Devin et al., 2017; Yang\n",
            "et al., 2020). Among many, D’Eramo et al. (2020) establishes a theoretical benefit of MTRL over\n",
            "single-task learning as the number of tasks increases, and Teh et al. (2017) learn individual policies\n",
            "while sharing a prior among tasks. However, naive sharing may exhibit negative transfer since not\n",
            "all knowledge should be shared by all tasks. An interesting line of work investigates the task in-\n",
            "terference issue in MTRL from the gradient perspective. For example, Yu et al. (2020) propose a\n",
            "gradient projection method where each task’s gradient is projected to an orthogonal direction of the\n",
            "others. Nevertheless, these approaches are sensitive to the high variance of the gradients. Another\n",
            "approach, known as PopArt (Hessel et al., 2018b), examines task interference focusing on the insta-\n",
            "bility caused by different reward magnitudes, addressing this issue by a normalizing technique on\n",
            "the output of the value function.\n",
            "Recently, sharing knowledge in a modular form has been advocated for reducing task interference.\n",
            "Yang et al. (2020) share a base model among tasks while having a routing network that generates\n",
            "task-specific models. Moreover, Devin et al. (2017) divides the responsibilities of the policy by\n",
            "sharing two policies, allocating one to different robots and the other to different tasks. Additionally,\n",
            "Sun et al. (2022) propose a parameter composition technique where a subspace of policy is shared\n",
            "by a group of related tasks. Moreover, CARE Sodhani et al. (2021) highlight the importance of\n",
            "using metadata for learning a mixture of state encoders that are shared among tasks, based on the\n",
            "claim that the learned encoders produce diverse and interpretable representations through an atten-\n",
            "tion mechanism. Despite the potential of this work, the method is highly dependent on the context\n",
            "information as shown in this recent work (Cheng et al., 2023). However, we argue that all of these\n",
            "approaches lack the guarantee of learning diverse representations.\n",
            "In this work, we promote diversity across a mixture of experts by enforcing orthogonality among\n",
            "their representations. The mixture-of-experts has been well-studied in the RL literature (Akrour\n",
            "et al., 2022; Ren et al., 2021). Moreover, some works dedicate attention to maximizing the diversity\n",
            "of the learned skills in RL (Eysenbach et al., 2018). Previous works leverage orthogonality for dis-\n",
            "parate purposes (Mackey et al., 2018). For example, Bansal et al. (2018) promote orthogonality on\n",
            "the weights by a regularized loss to stabilize training in deep convolutional neural networks. Simi-\n",
            "larly, Huang et al. (2018a) employ orthogonality among the weights for stabilizing the distribution\n",
            "of activation in neural networks. In the context of MTRL, Paredes et al. (2012) enforce the represen-\n",
            "tation obtained from a set of similar tasks to be orthogonal to the one obtained from selected tasks\n",
            "known to be unrelated. Recently, Chaudhry et al. (2020) alleviate catastrophic forgetting in con-\n",
            "tinual learning by organizing task representations in orthogonal subspaces. Finally, Mashhadi et al.\n",
            "(2021) favor diversity in an ensemble of learners via a Gram-Schmidt process. As opposed to it,\n",
            "3Preprint\n",
            "Task\n",
            " Encoderc\n",
            "wc\n",
            "vcV Gram-\n",
            "Schmidt\n",
            "Stiefel Manifold Output Head Mixture of Expertsfθ\n",
            "Figure 1: MOORE illustrative diagram. A state representation is encoded as a set of representa-\n",
            "tions using a mixture of experts. The Gram-Schmidt process orthogonalizes the representations to\n",
            "encourage diversity. Then, the output head processes the representations Vby interpolating the task-\n",
            "specific representations vcusing the task-specific weights wc, from which we compute the output\n",
            "using the output module fθ. In our approach, we employ this architecture in an actor-critic setting\n",
            "for both the actor and the critic.\n",
            "our primary focus lies in the acquisition of a set of orthogonal representations that span a subspace\n",
            "shared by a group of tasks where task-relevant representations can be interpolated.\n",
            "4 S HARING ORTHOGONAL REPRESENTATIONS\n",
            "We aim to obtain a set of rich and diverse representations that can be leveraged to find a universal\n",
            "policy that accomplishes multiple tasks. To this end, we propose to enforce the orthogonality of the\n",
            "representations extracted by a mixture of experts.\n",
            "In the following, we first provide a mathematical formulation from which we derive our approach. In\n",
            "particular, we highlight the connection between our method and the Stiefel manifold theory (Huang\n",
            "et al., 2018b; Chaudhry et al., 2020; Li et al., 2020), together with the description of the role played\n",
            "by the Gram-Schmidt process. Then, we proceed to devise our novel method for Multi-Task Rein-\n",
            "forcement Learning on orthogonal representation obtained from mixture of experts.\n",
            "4.1 O RTHOGONALITY IN CONTEXTUAL MARKOV DECISION PROCESSES\n",
            "We study the optimization of a policy π, given a set of k-orthonormal representations in Rdfor the\n",
            "states. We define the orthonormal representations of state sas a matrix Vs= [v1, ..., v k]∈Rd×k\n",
            "where vi∈Rd,∀i≤k. It can be shown that the orthonormal representations Vsbelong to a\n",
            "topological space known as Stiefel manifold, a smooth and differentiable manifold largely used in\n",
            "machine learning (Huang et al., 2018b; Chaudhry et al., 2020; Li et al., 2020).\n",
            "Definition 4.1 (Stiefel Manifold) Stiefel manifold Vk(Rd)is defined as the set of all orthonormal\n",
            "k-frames in the Euclidean space Rd, where k≤d,Vk(Rd) ={Vs∈Rd×k:VT\n",
            "sVs=Ik,∀s∈ S} .\n",
            "Under this lens, our goal can be interpreted as finding a set of orthogonal representations belonging\n",
            "to the Stiefel manifold, that capture the common components in the true state space. Thus, we\n",
            "propose a novel MDP formulation for MTRL, which we call Stiefel Contextual MDP (SC-MDP),\n",
            "that is inspired by the BC-MDP introduced in Sodhani et al. (2021). An SC-MDP includes a function\n",
            "that maps the state stok-orthonormal representations Vs∈ Vk(Rd), and it is defined as follows.\n",
            "Definition 4.2 (Stiefel Contextual Markov Decision Process) A Stiefel Contextual Markov Decision\n",
            "Process (SC-MDP) is defined as a tuple <C,S,A,M′, φ > whereCis the context space, Sis the\n",
            "state space, Ais the action space. M′is a function that maps a context c∈ Cto MDP parameters\n",
            "and observation space M′(c) ={rc,Pc,Sc, ρc},φis a function that maps every state s∈ S to a\n",
            "k-orthonormal representations Vs∈ Vk(Rd),Vs=φ(s).\n",
            "4Preprint\n",
            "We use a compositional form of the universal policy π, defined as π(a|s, c) =θ(Vswc), where\n",
            "wc∈Rkis the task-specific weight that combines the k-orthogonal representations into a single\n",
            "one and θ∈R|A|×dcombines the task-specific representations for generating actions. To leverage\n",
            "a diverse set of representations across tasks, the mapping function φplays a crucial role. Hence,\n",
            "we learn a mixture of experts hϕ= [hϕ1, ..., h ϕk]with learnable parameters ϕ= [ϕ1, ..., ϕ k]that\n",
            "generate k-representations Us∈Rd×kof state s, while ensuring that the generated representations\n",
            "are orthogonal to enforce diversity. Conveniently, this objective finds a rigorous formulation as a\n",
            "constrained optimization problem where we impose a hard constraint to enforce orthogonality:\n",
            "max\n",
            "Θ={ϕ,θ}J(Θ)\n",
            "s.t. hT\n",
            "ϕ(s)hϕ(s) =Ik∀s∈ S,(1)\n",
            "where h ϕ(s)∈Rd×krepresents a k-orthonormal representations in Rd, and Ik∈Rk×kis the\n",
            "identity matrix. Instead of solving the constrained optimization problem in Eq. 1, we ensure the\n",
            "diversity across experts through the application of the Gram-Schmidt (GS) process to orthogonalize\n",
            "thek-representations Us.\n",
            "Definition 4.3 (Gram-Schmidt Process) Gram-Schmidt process is a method for orthogonalizing a\n",
            "set of linearly independent U={u1, ..., u k:ui∈Rd,∀i≤k}. It maps the vectors to a set of\n",
            "k-orthonormal vectors V={v1, ..., v k:vi∈Rd,∀i≤k}defined as\n",
            "vk=uk−k−1X\n",
            "i=1⟨vi, uk⟩\n",
            "⟨vi, vi⟩vi. (2)\n",
            "where the representation of the i-th expert uiis projected in the orthogonal direction to the represen-\n",
            "tations of all i−1experts. Therefore, we apply GS process to map the generated representations by\n",
            "the mixture of experts Us=hϕ(s)to a set of orthonormal representations Vs=GS(Us), satisfying\n",
            "the hard constraint in Eq. 1.\n",
            "4.2 M ULTI -TASK REINFORCEMENT LEARNING WITH ORTHOGONAL REPRESENTATIONS\n",
            "Following the compositional form of the policy π(a|s, c), each task can interpolate its relevant repre-\n",
            "sentation from the space spanned by the k-orthonormal representations Vs. We train a task encoder\n",
            "to produce the task-specific weights wc∈Rkgiven task information (e.g. task ID). The orthonor-\n",
            "mal representations are combined using the task-specific weight to produce relevant representations\n",
            "vc∈Rdto the task as vc=V wc, where sis dropped for simplicity. The interpolated representation\n",
            "vccaptures the relevant components of the task that can be utilized by the RL algorithm and fed to\n",
            "an output module fθ. The output module can be learned for each task separately (multi-head) or\n",
            "shared by all tasks (single-head) to compute the action components from the representations vc. In\n",
            "conclusion, this approach results in a Mixture OfORthogonal Experts, thus, we call it MOORE,\n",
            "whose extracted representation is used to learn universal policies for MTRL.\n",
            "We adopt two different RL algorithms, namely Proximal Policy Optimization (PPO) and Soft Actor-\n",
            "Critic (SAC), with the purpose of demonstrating that our approach is agnostic to the used RL algo-\n",
            "rithms. PPO (Schulman et al., 2017) is a policy gradient algorithm that has the merit of obtaining\n",
            "satisfactory performance in a wide range of problems while being easy to implement. It is a first-\n",
            "order method that enhances the policy update given the current data by limiting the deviation of\n",
            "the new policy from the current one. In this work, we impose the same compositional structure\n",
            "formerly mentioned for both actor and critic. Moreover, we integrate our approach to SAC, a high-\n",
            "performing off-policy RL algorithm that leverages entropy maximization to enhance exploration.\n",
            "Similar to PPO, the compositional structure is utilized for both the actor (Alg. 1) and critic (Alg. 2).\n",
            "A visual demonstration of our approach is shown in Fig.1.\n",
            "5 E XPERIMENTAL RESULTS\n",
            "In this section, we evaluate MOORE against related baselines on two challenging MTRL bench-\n",
            "marks, namely MiniGrid (Chevalier-Boisvert et al., 2023), a set of visual goal-oriented tasks, and\n",
            "5Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT3\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.60.8Average ReturnMT5\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnMT7\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "(a) Multi-Head Architecture\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT3\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT5\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.6Average ReturnMT7\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "(b) Single-Head Architecture\n",
            "Figure 2: Average return on the three MTRL scenarios of MiniGrid. We utilize both, multi-head and\n",
            "single-head architectures, for our approach MOORE as well as the related baselines. For MOORE\n",
            "and MOE, the number of experts kis 2, 3, and 4 for MT3, MT5, and MT7, respectively. The black\n",
            "dashed line represents the final single-task performance of PPO averaged across all tasks. For the\n",
            "evaluation metric, we compute the accumulated return averaged across all tasks. We report the mean\n",
            "and the 95% confidence interval across 30 different runs.\n",
            "MetaWorld (Yu et al., 2019), a collection of robotic manipulation tasks. The objective is to assess the\n",
            "adaptability of our approach in handling different types of state observations and tackling a variable\n",
            "number of tasks. Moreover, the flexibility of MOORE is evinced by using it in on-policy (PPO for\n",
            "MiniGrid) and off-policy RL (SAC for MetaWorld) algorithms. Additionally, we conduct ablation\n",
            "studies that support the effectiveness of MOORE in various aspects. We want to assess the follow-\n",
            "ing points: the benefit of using Gram-Schmidt to impose diversity across experts, the quality of the\n",
            "learned representations, as well as the transfer capabilities, and the interpretability of the diverse\n",
            "experts.\n",
            "5.1 M INIGRID\n",
            "We consider different tasks in MiniGrid (Chevalier-Boisvert et al., 2023), which is a suite of 2D goal-\n",
            "oriented environments that require solving different mazes while interacting with different objects\n",
            "like doors, keys, or boxes of several colors, shapes, and roles. MiniGrid allows the use of a visual\n",
            "representation of the state, which we adopt for our multi-task setting. We consider the multi-task\n",
            "setting from Jin et al. (2023) that includes three multi-task scenarios. The first scenario, MT3 , in-\n",
            "volves the three tasks: LavaGap, RedBlueDoors, and Memory; the second scenario, MT5 , includes\n",
            "the five tasks: DoorKey, LavaGap, Memory, SimpleCrossing, and MultiRoom. Finally, MT7 com-\n",
            "prises the seven tasks: DoorKey, DistShift, RedBlueDoors, LavaGap, Memory, SimpleCrossing, and\n",
            "MultiRoom. In Sec. A.1, we provide a description of the considered tasks.\n",
            "We compare MOORE against four baselines. The first one is PPO , which is considered as a refer-\n",
            "ence for comparing to single-task performance. The second baseline is Multi-Task PPO (MTPPO) ,\n",
            "an adaptation of PPO (Schulman et al., 2017) for MTRL. Then, we consider MOE , which employs\n",
            "a mixture of experts to generate representations without enforcing diversity across experts. Ad-\n",
            "ditionally, we have PCGrad (Yu et al., 2020), which is an MTRL approach that tackles the task\n",
            "6Preprint\n",
            "0 20 40 60 80 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnMT3  MT5\n",
            "0 20 40 60 80 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnMT5  MT7\n",
            "MOE Transfer-MOE MOORE (ours) Transfer-MOORE (ours)\n",
            "Figure 3: Evaluating MOORE against MOE on the transfer setting. The study is conducted on the\n",
            "two transfer learning scenarios in MiniGrid, employing a multi-head architecture. The number of\n",
            "experts kis 2 and 3 for MT3 →MT5 and MT5 →MT7, respectively. For the evaluation metric,\n",
            "we compute the accumulated return averaged across all tasks. We report the mean and the 95%\n",
            "confidence interval across 30 different runs.\n",
            "interference issue by manipulating the gradient. For a fair comparison, we integrate PCGrad on top\n",
            "of the MOE baseline. As for the MTRL architecture, we utilize multi-head and single-head archi-\n",
            "tectures for all methods, showing their average return across all tasks in Fig. 2(a), and Fig. 2(b)\n",
            "respectively. MOORE outperforms the aforementioned baselines in almost all the MTRL scenar-\n",
            "ios. Notably, our method exhibits faster convergence than the baselines. It is interesting to observe\n",
            "that MOORE outperforms the single-task performance with a significant margin in comparison to\n",
            "the other baselines (Fig.2(a)), which is usually considered as an upper-bound of the MTRL perfor-\n",
            "mance in previous works. This highlights the quality of the learned representations and the role of\n",
            "MOORE representation learning process in MTRL.\n",
            "5.1.1 A BLATION STUDIES\n",
            "2 3 4 5 6 7 8 9 10\n",
            "Number of Experts0.40.50.60.7Average ReturnMOE\n",
            "MOORE (ours)\n",
            "Figure 4: Ablation study on the effect of changing\n",
            "the number of experts. We compare the perfor-\n",
            "mance of MOE and MOORE (ours) on MiniGrid\n",
            "MT7 using a single-head architecture. We report\n",
            "the mean of the evaluation metric across 30 seeds.\n",
            "For the evaluation metric, we compute the accu-\n",
            "mulated return averaged across all tasks.We examine the advantage of transferring the\n",
            "trained experts, on a set of base tasks, to novel\n",
            "tasks, in order to assess the quality and general-\n",
            "ization of these learned experts in comparison\n",
            "to the MOE baseline. We refer to the transfer\n",
            "variant of our approach as Transfer-MOORE\n",
            "while Transfer-MOE for the baseline. More-\n",
            "over, we include the performance of MOORE\n",
            "and MOE as a MTRL reference for learning\n",
            "only the novel tasks, completely isolated from\n",
            "the base tasks. In Fig.3, we show the empir-\n",
            "ical results on two transfer learning scenarios\n",
            "where we transfer a set of experts learned on\n",
            "MT3 to MT5 ( MT3→MT5 ), and on MT5\n",
            "to MT7 ( MT5→MT7 ). MT3 is a subset of\n",
            "MT5 while MT5 is a subset of MT7. First, we\n",
            "train on the base tasks (intersection of the two\n",
            "sets), and then we transfer the learned experts to\n",
            "the novel tasks (the difference between the two\n",
            "sets). As illustrated in Fig. 3, Transfer-MOORE\n",
            "outperforms Transfer-MOE in the two scenar-\n",
            "ios, showing the quality of the learned representations in the context of transfer learning. Moreover,\n",
            "the study demonstrates the ability of our approach as an effective MTRL algorithm, that provides\n",
            "competitive results against the transfer variant (Transfer-MOORE). In contrast, MOE struggles to\n",
            "beat the transfer variant as in the MT3 →MT5 scenario. Consequently, this study advocates the\n",
            "diversification of the shared representations in transfer learning as well as MTRL. We highlight\n",
            "more details in B.2.\n",
            "7Preprint\n",
            "Total Env Steps 1M 2M 3M 5M 10M 15M 20M\n",
            "SAC (Yu et al., 2019) 10.0±8.2 17.7 ±2.1 18.7 ±1.1 20.0 ±2.0 48.0 ±9.5 57.7 ±3.1 61.9 ±3.3\n",
            "MTSAC (Yu et al., 2019) 34.9±12.9 49.3 ±9.0 57.1 ±9.8 60.2 ±9.6 61.6 ±6.7 65.6 ±10.4 62.9 ±8.0\n",
            "SAC + FiLM (Perez et al., 2017) 32.7±6.5 46.9 ±9.4 52.9 ±6.4 57.2 ±4.2 59.7 ±4.6 61.7 ±5.4 58.3 ±4.3\n",
            "PCGrad (Yu et al., 2020) 32.2±6.8 46.6 ±9.3 54.0 ±8.4 60.2 ±9.7 62.6 ±11.0 62.6 ±10.5 61.7 ±10.9\n",
            "Soft-Module (Yang et al., 2020) 24.2±4.8 41.0 ±2.9 47.4 ±5.3 51.4 ±6.8 53.6 ±4.9 56.6 ±4.8 63.0 ±4.2\n",
            "CARE (Sodhani et al., 2021) 26.0±9.1 52.6 ±9.3 63.8 ±7.9 66.5 ±8.3 69.8 ±5.1 72.2 ±7.1 76.0 ±6.9\n",
            "PaCo (Sun et al., 2022) 30.5±9.5 49.8 ±8.2 65.7 ±4.5 64.7 ±4.2 71.0 ±5.5 81.0 ±5.9 85.4 ±4.5\n",
            "MOORE (ours) 37.2±9.9 63.0 ±7.2 68.6 ±6.9 77.3 ±9.6 82.7 ±7.3 88.2 ±5.6 88.7 ±5.6\n",
            "Table 1: Results on MetaWorld MT10 Yu et al. (2019) with random goals (MT10-rand). The results\n",
            "of the baselines are borrowed from Sun et al. (2022). For MOORE, the number of experts kis 4. For\n",
            "all methods, we report the mean and standard deviation of the evaluation metric across 10 different\n",
            "runs. The evaluation metric is the average success rate across all tasks. We highlight with bold text\n",
            "thebest so far.\n",
            "Additionally, we focus on the impact of changing the number of experts on the performance of\n",
            "our approach, as well as on MOE. In Fig.4, we consider different numbers of experts on the MT7\n",
            "scenario. We observe the effect of utilizing more experts in MOORE algorithm compared to MOE.\n",
            "The study shows that MOORE exhibits a noticeable advantage, on average, for an increasing number\n",
            "of experts. On the contrary, a slower enhancement of the performance is encountered by MOE. It\n",
            "is also worth noting that the performance of MOORE with k= 4 slightly outperforms MOE with\n",
            "k= 10 while being comparable to MOE with k= 8(the best setting for MOE). This supports our\n",
            "claim about the efficient utilization of the expert capacity through enforcing diversity.\n",
            "5.2 M ETAWORLD\n",
            "Finally, we evaluate our approach on another challenging MTRL setting with a large number of\n",
            "manipulation tasks. We benchmark against MetaWorld Yu et al. (2019), a widely adopted robotic\n",
            "manipulation benchmark for Multi-Task and Meta Reinforcement Learning. We consider the MT10\n",
            "setting, where a set of 10related manipulation tasks has to be performed by a single robot.\n",
            "For the baselines, we compare our approach against the following algorithms. First, SAC (Haarnoja\n",
            "et al., 2018) is the off-policy RL that is trained on each task separately, thus being a reference\n",
            "for the single-task setting. Second, Multi-Task SAC (MTSAC) is the adaptation of SAC to the\n",
            "MTRL setting, where we employ a single-head architecture with a one-hot vector concatenated with\n",
            "the state. Then, SAC+FiLM is a task-conditional policy that employs the FiLM module (Perez\n",
            "et al., 2017). Furthermore, PCGrad (Yu et al., 2020) is an MTRL approach that tackles the task\n",
            "interference issue by manipulating the gradient. Soft-Module (Yang et al., 2020) utilizes a routing\n",
            "network that proposes weights for soft combining of activations for each task. CARE (Sodhani\n",
            "et al., 2021) is an attention-based approach that learns a mixture of experts for encoding the state\n",
            "while utilizing context information. Finally, PaCo (Sun et al., 2022) is the recent state-of-the-art\n",
            "method for MetaWorld that learns a compositional policy where task-specific weights are utilized\n",
            "for interpolating task-specific policies. On the other hand, our approach uses a similar framework as\n",
            "in the MiniGrid experiment and employs a multi-head architecture.\n",
            "Following Sun et al. (2022), we benchmark against MT10-rand where each task is trained with\n",
            "random goal positions. The goal position is concatenated with the state representation. As a per-\n",
            "formance metric, we compute the success rate averaged across all tasks. For a fair comparison, we\n",
            "run our approach for 10different runs. In Tab. 1, we report the mean and the standard deviations of\n",
            "the metric across the 10different runs and at different learning steps. As stated in Tab.1, MOORE\n",
            "outperforms all the baselines both in terms of convergence speed and asymptotic performance. It is\n",
            "important to mention that all the MTRL uses tricks to enhance the stability of the learning process.\n",
            "For instance, PaCo avoids task and gradient explosion by proposing two empirical tricks, named loss\n",
            "maskout andw-reset , where pruning every task loss that reaches above a certain threshold, besides\n",
            "resetting the task-specific weight for that task. Also, as in Sun et al. (2022), the other baselines re-\n",
            "sort to more expensive tricks, such as terminating and re-launching the training session when a loss\n",
            "explosion is encountered. On the contrary, our approach does not need such tricks to improve the\n",
            "stability of the learning process which can be an indication of the stability of the chosen architecture\n",
            "and the importance of learning distinct experts.\n",
            "8Preprint\n",
            "0 5 10 15 20\n",
            "#Epochs0.000.250.500.75Success Rate\n",
            "MOE\n",
            "MOORE (ours)\n",
            "(a) Success rate in MT10-rand.\n",
            "0 2 4 6 8\n",
            "#Epochs0.00.20.40.6Success RateMOE\n",
            "MOORE (ours) (b) Success rate in MT50-rand.\n",
            "Figure 6: (a) Success rate on MetaWorld MT10-rand comparing MOORE, against MOE, using 4\n",
            "experts. (b) Success rate on MetaWorld MT50-rand comparing MOORE, against MOE, given 6\n",
            "experts. We show the average success rate across all tasks and the 95% confidence interval across\n",
            "10and5different runs for MT10-rand and MT50-rand, respectively.\n",
            "5.2.1 A BLATION STUDIES\n",
            "1.0\n",
            " 0.5\n",
            " 0.0 0.5 1.0 1.5\n",
            "Principal Component 10.6\n",
            "0.4\n",
            "0.2\n",
            "0.00.20.40.6Principal Component 2reach\n",
            "push\n",
            "pick-place\n",
            "door-open\n",
            "drawer-open\n",
            "drawer-close\n",
            "button-press-topdown\n",
            "peg-insert-side\n",
            "window-open\n",
            "window-close\n",
            "Figure 5: Principle Component Analysis (PCA)\n",
            "on the task-specific weights learned by MOORE\n",
            "on MetaWorld MT10-rand for a run with 100%\n",
            "success rate across all tasks. This shows the inter-\n",
            "pretability and diversity of the shared experts.Similarly, we want to evince the advantage\n",
            "of favoring diversity across experts. We con-\n",
            "sider the same architecture of MOORE, but\n",
            "without the Gram-Schmidt process, and refer\n",
            "to it as MOE. We evaluate MOORE against\n",
            "MOE on two MTRL scenarios in MetaWorld.\n",
            "In addition to MT10-rand, we benchmark on\n",
            "MT50-rand, a large-scale MTRL scenario in\n",
            "MetaWorld with 50different but related ma-\n",
            "nipulation tasks. In Fig. 6(a), MOORE ex-\n",
            "hibits superior sample-efficiency compared to\n",
            "MOE. Moreover, MOORE significantly out-\n",
            "performs the baseline also in MT50-rand (Fig.\n",
            "6(b)), evincing the scalability of our approach\n",
            "to large-scale MTRL problems. This study il-\n",
            "lustrates the importance of enforcing diversity\n",
            "across experts in MTRL algorithms.\n",
            "Additionally, we verify the interpretability of\n",
            "the learned representations. Fig. 5 shows an\n",
            "application of PCA on the learned task-specific\n",
            "weights wcthat are used to combine the repre-\n",
            "sentations of the experts. On the one hand, as\n",
            "shown, the pick-place task is close to the peg-\n",
            "insert-side since both tasks require picking up an object. On the other hand, the weights of door-open\n",
            "and window-open tasks are similar as they share the open skill. Therefore, enforcing diversity across\n",
            "experts distributes the responsibilities across them in capturing common components across tasks\n",
            "(e.g. objects or skills). This confirms that the learned experts have some roles that can be inter-\n",
            "pretable.\n",
            "6 C ONCLUSION AND DISCUSSION\n",
            "We proposed a novel MTRL approach for diversifying a mixture of shared experts across tasks.\n",
            "Mathematically, we formulate our objective as a constrained optimization problem where a hard\n",
            "constraint is explicitly imposed to ensure orthogonality between the representations. As a result,\n",
            "the orthogonal representations live on a smooth and differentiable manifold called the Stiefel man-\n",
            "9Preprint\n",
            "ifold. We formulate our MTRL as a novel contextual MDP while mapping each state to the Stiefel\n",
            "manifold using a mapping function, which we learn through a mixture of experts while enforcing\n",
            "orthogonality across their representations with the Gram-Schmidt process, hence satisfying the hard\n",
            "constraint. Our approach demonstrates superior performance against related baselines on two chal-\n",
            "lenging MTRL baselines.\n",
            "Taking advantage of all the experts during inference, our approach has the limitation of potentially\n",
            "suffering from high time complexity, in comparison, for instance, to a sparse selection of one expert.\n",
            "This leads to a trade-off between the representation capacity and time complexity which could be\n",
            "investigated in the future through a selection of a few orthogonal experts. In addition to the transfer\n",
            "learning study we conducted, we are interested in investigating extensions of our approach into a\n",
            "continual learning setting.\n",
            "REFERENCES\n",
            "Riad Akrour, Davide Tateo, and Jan Peters. Continuous action reinforcement learning from a mixture\n",
            "of interpretable experts. IEEE Transactions on Pattern Analysis and Machine Intelligence , 44(10):\n",
            "6795–6806, 2022. doi: 10.1109/TPAMI.2021.3103132.\n",
            "Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regu-\n",
            "larizations in training deep networks? Advances in Neural Information Processing Systems , 31,\n",
            "2018.\n",
            "Richard Bellman. Dynamic Programming . Princeton University Press, Princeton, NJ, USA, 1 edi-\n",
            "tion, 1957.\n",
            "Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli. Sparse multi-task reinforcement\n",
            "learning. In Advances in Neural Information Processing Systems , 2014.\n",
            "Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual learning in low-\n",
            "rank orthogonal subspaces. Advances in Neural Information Processing Systems , 33:9900–9911,\n",
            "2020.\n",
            "Guangran Cheng, Lu Dong, Wenzhe Cai, and Changyin Sun. Multi-task reinforcement learning\n",
            "with attention-based mixture of experts. IEEE Robotics and Automation Letters , 8(6):3812–3819,\n",
            "2023. doi: 10.1109/LRA.2023.3271445.\n",
            "Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem\n",
            "Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modu-\n",
            "lar & customizable reinforcement learning environments for goal-oriented tasks. arXiv preprint\n",
            "arXiv:2306.13831 , 2023.\n",
            "Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowl-\n",
            "edge in multi-task deep reinforcement learning. In International Conference on Learning Repre-\n",
            "sentations , 2020.\n",
            "Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular\n",
            "neural network policies for multi-task and multi-robot transfer. In International Conference on\n",
            "Robotics and Automation , 2017.\n",
            "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\n",
            "Learning skills without a reward function. arXiv preprint arXiv:1802.06070 , 2018.\n",
            "Gene H Golub and Charles F Van Loan. Matrix computations . JHU press, 2013.\n",
            "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\n",
            "maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-\n",
            "ence on Machine Learning , 2018.\n",
            "Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan\n",
            "Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in\n",
            "deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence ,\n",
            "volume 32, 2018a.\n",
            "10Preprint\n",
            "Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van\n",
            "Hasselt. Multi-task deep reinforcement learning with popart. CoRR , abs/1809.04474, 2018b.\n",
            "Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. Orthogonal weight\n",
            "normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural\n",
            "networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018a.\n",
            "Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. Orthogonal weight\n",
            "normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural\n",
            "networks. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018b.\n",
            "I. M. James. The Topology of Stiefel Manifolds . London Mathematical Society Lecture Note Series.\n",
            "Cambridge University Press, 1977. doi: 10.1017/CBO9780511600753.\n",
            "Yonggang Jin, Chenxu Wang, Liuyu Xiang, Yaodong Yang, Jie Fu, and Zhaofeng He. Deep re-\n",
            "inforcement learning with multitask episodic memory based on task-conditioned hypernetwork.\n",
            "arXiv preprint arXiv:2306.10698 , 2023.\n",
            "Jun Li, Li Fuxin, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold\n",
            "via the cayley transform. arXiv preprint arXiv:2002.01113 , 2020.\n",
            "Shuai Li, Kui Jia, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal deep neural networks.\n",
            "IEEE transactions on pattern analysis and machine intelligence , 43(4):1352–1368, 2019.\n",
            "Lester Mackey, Vasilis Syrgkanis, and Ilias Zadik. Orthogonal machine learning: Power and limita-\n",
            "tions. In International Conference on Machine Learning , pp. 3375–3383. PMLR, 2018.\n",
            "Peyman Sheikholharam Mashhadi, Sławomir Nowaczyk, and Sepideh Pashami. Parallel orthogonal\n",
            "deep neural network. Neural Networks , 140:167–183, 2021.\n",
            "V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\n",
            "stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\n",
            "arXiv:1312.5602 , 2013.\n",
            "Mete Ozay and Takayuki Okatani. Optimization on submanifolds of convolution kernels in cnns.\n",
            "arXiv preprint arXiv:1610.07008 , 2016.\n",
            "Bernardino Romera Paredes, Andreas Argyriou, Nadia Berthouze, and Massimiliano Pontil. Ex-\n",
            "ploiting unrelated tasks in multi-task learning. In Artificial intelligence and statistics , pp. 951–\n",
            "959. PMLR, 2012.\n",
            "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. FiLM:\n",
            "Visual reasoning with a general conditioning layer. CoRR , abs/1709.07871, 2017.\n",
            "Martin L Puterman. Markov decision processes: Discrete stochastic dynamic programming. Journal\n",
            "of the Operational Research Society , 1995.\n",
            "Jie Ren, Yewen Li, Zihan Ding, Wei Pan, and Hao Dong. Probabilistic mixture-of-experts for\n",
            "efficient deep reinforcement learning. arXiv preprint arXiv:2104.09122 , 2021.\n",
            "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\n",
            "policy optimization. In International conference on machine learning , pp. 1889–1897. PMLR,\n",
            "2015.\n",
            "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n",
            "optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n",
            "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\n",
            "Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\n",
            "the game of go with deep neural networks and tree search. nature , 529(7587):484–489, 2016.\n",
            "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\n",
            "Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi\n",
            "by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815 ,\n",
            "2017.\n",
            "11Preprint\n",
            "Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with context-\n",
            "based representations. In International Conference on Machine Learning , 2021.\n",
            "Lingfeng Sun, Haichao Zhang, Wei Xu, and Masayoshi Tomizuka. Paco: Parameter-compositional\n",
            "multi-task reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\n",
            "Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL\n",
            "https://openreview.net/forum?id=LYXTPNWJLr .\n",
            "Yee Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nico-\n",
            "las Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances\n",
            "in Neural Information Processing Systems , 2017.\n",
            "Ruihan Yang, Huazhe Xu, YI WU, and Xiaolong Wang. Multi-task reinforcement learning with soft\n",
            "modularization. In Advances in Neural Information Processing Systems , 2020.\n",
            "Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey\n",
            "Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\n",
            "InConference on Robot Learning , 2019.\n",
            "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\n",
            "Gradient surgery for multi-task learning. In Advances in Neural Information Processing Systems ,\n",
            "2020.\n",
            "12Preprint\n",
            "(a) DoorKey\n",
            " (b) DistShift\n",
            " (c) RedBlueDoors\n",
            " (d) LavaGap\n",
            "(e) Memory\n",
            " (f) SimpleCrossing\n",
            " (g) MultiRoom\n",
            "Figure 7: MiniGrid (Chevalier-Boisvert et al., 2023) Tasks, where the red triangle represents the\n",
            "agent, and the green square refers to the goal.\n",
            "A A DDITIONAL DETAILS ON THE EXPERIMENTS\n",
            "In this section, we elaborate on the implementation details of our approach, MOORE, for bench-\n",
            "marking against MiniGrid (Chevalier-Boisvert et al., 2023) and MetaWorld (Yu et al., 2019). Be-\n",
            "sides, we provide additional ablation studies that demonstrate various aspects of our approach.\n",
            "A.1 M INIGRID\n",
            "A.1.1 E NVIRONMENT DETAILS\n",
            "MiniGrid (Chevalier-Boisvert et al., 2023) is a collection of 2D goal-oriented environments where\n",
            "the agent learns how to solve different mazes while interacting with various objects in terms of\n",
            "shape, color, and role. The library of MiniGrid provides multiple choice for state representation.\n",
            "For our MTRL setting, we adopt the visual representation of the state where a 3-dimensional input\n",
            "of shape 7x7x3 is provided. As mentioned in Sec. 5.1, our MTRL setting consists of three scenarios\n",
            "that include 7 tasks in total that are distributed differently. A render example of each of the tasks is\n",
            "demonstrated in Fig. 7. Additionally, the description of each task is provided in Tab. 2.\n",
            "Task Description\n",
            "DoorKey Use the key to open the door and then get to the goal.\n",
            "DistShift Get to the green goal square.\n",
            "RedBlueDoors Open the red door and then the blue door\n",
            "LavaGap Avoid the lava and get to the green goal square.\n",
            "Memory Go to the matching object at the end of the hallway\n",
            "SimpleCrossing Find the opening and get to the green goal square\n",
            "MultiRoom Traverse the rooms to get to the goal.\n",
            "Table 2: MiniGrid (Chevalier-Boisvert et al., 2023) Task Descriptions.\n",
            "13Preprint\n",
            "A.1.2 I MPLEMENTATION DETAILS\n",
            "As an RL algorithm, we use PPO (Schulman et al., 2017), which is considered a state-of-the-art\n",
            "on-policy RL algorithm on many benchmarks. Moreover, it has been used in the official paper of\n",
            "the MiniGrid benchmark (Chevalier-Boisvert et al., 2023). We adapt PPO to the MTRL setting\n",
            "by computing the loss function of both the actor and critic averaged on transitions sampled from\n",
            "all tasks. We refer to this adapted algorithm as MTPPO. In Tab. 3, we highlight the important\n",
            "hyperparameters needed to reproduce the results on MiniGrid.\n",
            "Hyperparameter Value\n",
            "General Hyperparameters\n",
            "Discount factor γ 0.99\n",
            "Number of environments [3,5,7]\n",
            "Steps per environment 1 step per 1 environment\n",
            "Number of epochs 100\n",
            "Steps per epoch 2000\n",
            "Train frequency 2000\n",
            "Number of episodes for evaluation 16\n",
            "PPO Hyperparameters\n",
            "Lambda coefficient in GAE formula 0.95\n",
            "Entropy term coefficient 0.01\n",
            "Clipping Epsilon 0.2\n",
            "Number of epochs for Policy 8\n",
            "Batch Size for Policy 256\n",
            "Number of epochs for Critic 1\n",
            "Batch Size for Critic 2000\n",
            "Critic Loss Mean Squared Error\n",
            "Optimizer Adam\n",
            "Learning rate for Policy 0.001\n",
            "Learning rate for Critic 0.001\n",
            "Table 3: MiniGrid (Chevalier-Boisvert et al., 2023) hyperparameters.\n",
            "We use a similar network architecture for the actor and the critic of our approach as well as the related\n",
            "baselines. In general, the network architecture consists of two main parts, a representation block ,\n",
            "and an output head . For the representation block, we use a Convolutional Neural Network (CNN)\n",
            "to encode the visual input to a latent space. For our MOORE, MOE, and PCGrad, k-CNNs are used\n",
            "to represent the mixture of experts inside the representation block. On the other hand, the output\n",
            "head consists of a task-encoder that generates the task-specific weights wc, in addition to the output\n",
            "module for producing the output of the network.\n",
            "The output module can utilize a single-head or a multi-head architecture. For single-head archi-\n",
            "tecture, the output of the representation block Vis weighted by the task-specific weight wc, then\n",
            "the task representation vcis concatenated with the task information c(e.g. task ID) and fed to\n",
            "the output module fθ. On the contrary, the multi-head architecture has multiple output modules\n",
            "fθ= [fθ1, .., f θ|C|]that can be selected given the context c(e.g. task ID).\n",
            "For regular baselines, we use a single CNN in the representation block while having the same two\n",
            "options of single-head and multi-head for the output module. Since we are using a single expert,\n",
            "there is no need for a task-encoder inside the output head. In Tab. 4, we illustrate the hyperparameters\n",
            "of both the representation block and the output head. It is worth noting that MOORE, MOE, and\n",
            "PCGrad linearly combine the generated representations from different experts before applying the\n",
            "last activation function of the representation block vc=Tanh(V wc).\n",
            "14Preprint\n",
            "Hyperparameter Value\n",
            "Representation Block\n",
            "Number of Experts ( k) {MT3: k= 2, MT5: k= 3, MT7: k= 4}\n",
            "Number of convolution layers 3\n",
            "Channels per layer [16, 32, 64]\n",
            "Kernel size [(2,2), (2,2), (2,2)]\n",
            "Activation functions [ReLU, ReLU, Tanh]\n",
            "Output Module\n",
            "Number of linear layers 2 (x number of tasks |T |)\n",
            "Number of output units [128,|A|for actor and 1 for critic]\n",
            "Activation functions [Tanh, Linear]\n",
            "Task Encoder\n",
            "Number of linear layers 1\n",
            "Number of output units Number of Experts (k)\n",
            "Use bias False\n",
            "Activation function Linear\n",
            "Table 4: Actor and Critic Architecture for PPO\n",
            "Algorithm 1 MOORE for Actor\n",
            "Require: Mixture of experts hϕ, state s, context c,\n",
            "task-specific weights wc, output module fθ.\n",
            "1:Us=hϕ(s)\n",
            "2:Vs=GS(Us) ▷Apply Eq.2\n",
            "3:vc=Vswc\n",
            "4:a∼fθ(vc)\n",
            "5:Return: aAlgorithm 2 MOORE for Critic\n",
            "Require: Mixture of experts hϕ, state-action (s, a), con-\n",
            "textc, task-specific weights wc, output module fθ.\n",
            "1:Us,a=hϕ(s, a)\n",
            "2:Vs,a=GS(Us,a) ▷Apply Eq.2\n",
            "3:vc=Vs,awc\n",
            "4:q=fθ(vc)\n",
            "5:Return: q\n",
            "A.2 M ETAWORLD\n",
            "A.2.1 E NVIRONMENT DETAILS\n",
            "MetaWorld (Yu et al., 2019) is a suite of a large number of robotic manipulation tasks. All tasks\n",
            "require dealing with one or two objects. Moreover, they are similar in terms of the dimensionality\n",
            "of the state space, yet the semantics of the state components differ. The state space consists of\n",
            "the following: the 3D position of the end effector, a normalized measure of how much the gripper\n",
            "is open, the 3D position of the first object, the quaternion of the first object (4D), as well as the\n",
            "3D position and quaternion of the second object (zeroed out, if not needed). Two consecutive data\n",
            "frames are stacked together, in addition to the 3D goal position forming a 39-dimensional state\n",
            "space. On the other hand, the action space is the same which represents the 3D change of the end\n",
            "effector, in addition to the normalized torque applied by the gripper. We benchmark our approach\n",
            "against the MT10 and MT50 scenarios. Following Sun et al. (2022), we randomize the goal position\n",
            "or the object position across all tasks and refer to it as MT10-rand and MT50-rand.\n",
            "A.2.2 I MPLEMENTATION DETAILS\n",
            "In this benchmark, we use SAC (Haarnoja et al., 2018), a state-of-the-art off-policy algorithm that\n",
            "enhances the exploration of the agent by maximizing the entropy. Similar to Yu et al. (2019); Sun\n",
            "et al. (2022), we adapt SAC by computing the actor and the critic losses averaged on transitions\n",
            "sampled from all tasks. We have a replay buffer for each task from which we sample transitions\n",
            "equally. In addition, we disentangle the temperature parameter of SAC by learning separate temper-\n",
            "ature parameters for each task. We refer to this adapted algorithm as MTSAC. Tab. 5, we list the\n",
            "hyperparameters required for reproducing our results on MetaWorld.\n",
            "Similar to MiniGrid, we use a network architecture with a representation block and an output head.\n",
            "The difference is that we use a dense neural network to represent the representation block. For\n",
            "MOORE, MOE, we also use k-dense networks to represent the mixture of experts. We also use a\n",
            "15Preprint\n",
            "task encoder to aggregate the representations Vgenerated by the experts. We apply a Tanh activation\n",
            "function after the linear combination of the representation vc=Tanh(V wc). Then, we fed the task-\n",
            "specific representation vcto the output module f θwhere we employed a multi-head architecture\n",
            "of a single linear layer per task. We use the context cto select the corresponding task-specific\n",
            "output module fθc. We show the MOORE adaptation for the actor and critic in Alg. 1 and Alg. 2,\n",
            "respectively.\n",
            "Hyperparameter Value\n",
            "General Hyperparameters\n",
            "Horizon 150\n",
            "Discount factor γ 0.99\n",
            "Number of environments 10\n",
            "Steps per environment 1 step per 1 environment\n",
            "Number of epochs 20\n",
            "Steps per epoch 100000\n",
            "Train frequency 1\n",
            "Number of episodes for evaluation 10\n",
            "SAC Hyperparameters\n",
            "Batch Size 128\n",
            "Critic Loss Mean Squared Error\n",
            "Disentangled temperature Alpha αTrue\n",
            "Optimizer Adam\n",
            "Learning rate for Policy 3×10−4\n",
            "Learning rate for Critic 3×10−4\n",
            "Learning rate for Alpha 1×10−4\n",
            "Policy minimum standard e−10\n",
            "Policy maximum standard e2\n",
            "Sift target interpolation 5×10−3\n",
            "Exploration steps 1500\n",
            "Replay buffer steps 1×106\n",
            "Table 5: MetaWorld Yu et al. (2019) Hyperparameters.\n",
            "Hyperparameter Value\n",
            "Representation Block\n",
            "Number of Experts ( k) 4\n",
            "Number of Linear layers 3\n",
            "Number of output units [400, 400, 400]\n",
            "Activation functions [ReLU, ReLU, Linear]\n",
            "Output Block\n",
            "Number of linear layers 1 (x number of tasks |T |)\n",
            "Number of output units [|A|for actor and 1 for critic]\n",
            "Activation functions Linear\n",
            "Task Encoder\n",
            "Number of linear layers 1\n",
            "Number of output units Number of Experts (k)\n",
            "Use bias False\n",
            "Activation function Linear\n",
            "Table 6: Actor and Critic Architecture for SAC\n",
            "16Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.75Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.30.40.50.6Average ReturnMemory\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 8: Individual task average return on the MT3 scenario of MiniGrid. We utilize the multi-head\n",
            "architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE, the\n",
            "number of experts kis 2. The black dashed line represents the final single-task performance of PPO\n",
            "averaged across all tasks. For the evaluation metric, we compute the accumulated return averaged\n",
            "across all tasks. We report the mean and the 95% confidence interval across 30 different runs.\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnDoorKey\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.81.0Average ReturnDistShift\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.30.40.50.6Average ReturnMemory\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 9: Individual task average return on the MT5 scenario of MiniGrid. We utilize the multi-head\n",
            "architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE, the\n",
            "number of experts kis 3. The black dashed line represents the final single-task performance of PPO\n",
            "averaged across all tasks. For the evaluation metric, we compute the accumulated return averaged\n",
            "across all tasks. We report the mean and the 95% confidence interval across 30 different runs.\n",
            "B A DDITIONAL EMPIRICAL RESULTS\n",
            "B.1 M INIGRID\n",
            "In Sec.5.1, we present the performance averaged across all the tasks. Here, we want to show the\n",
            "individual task performance of all three scenarios of MiniGrid.\n",
            "17Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnDoorKey\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnDistShift\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.75Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMemory\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnSimpleCrossing\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnMultiRoom\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 10: Individual task average return on the MT7 scenario of MiniGrid. We utilize the multi-\n",
            "head architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE,\n",
            "the number of experts kis 4. The black dashed line represents the final single-task performance\n",
            "of PPO averaged across all tasks. For the evaluation metric, we compute the accumulated return\n",
            "averaged across all tasks. We report the mean and the 95% confidence interval across 30 different\n",
            "runs.\n",
            "B.2 T RANSFER LEARNING WITH MOORE\n",
            "Furthermore, we discuss the experimental details of the Transfer Learning ablation study in Fig 3. In\n",
            "this study, we assess the transfer capability of our approach in utilizing the diverse representations,\n",
            "learned on a set of base tasks, for a set of novel but related tasks. We evaluate our approach,\n",
            "MOORE, against the MOE baseline on MiniGrid. We refer to the transfer learning adaptation of our\n",
            "approach as Transfer-MOORE , and Transfer-MOE for the MOE baseline.\n",
            "We conducted two experiments based on the sets of tasks defined on MiniGrid (MT3, MT5, and\n",
            "MT7). In Fig.3, we show the empirical results on two transfer learning scenarios where we transfer\n",
            "a set of experts learned on MT3 to MT5 ( MT3→MT5 ), and on MT5 to MT7 ( MT5→MT7 ). It is\n",
            "worth noting that MT3 is a subset of MT5, and MT5 is a subset of MT7. We consider the intersection\n",
            "between every two sets (MT3 and MT5 or MT5 and MT7) as base tasks while considering the\n",
            "difference as novel tasks. For instance, in the MT3 →MT5 scenario, the base tasks are LavaGap,\n",
            "RedBlueDoors, and Memory (common for MT3 and MT5), while having DoorKey, and MultiRoom\n",
            "as novel tasks (only in MT5).\n",
            "For Transfer-MOORE, we train on the base tasks, then we use the learned mixture of experts in\n",
            "a frozen state, to learn the novel ones. On the contrary, MOORE is only trained on novel tasks\n",
            "from scratch. This also holds for MOE and Transfer-MOE. In this study, we employ a multi-head\n",
            "18Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.75Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.4Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMemory\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 11: Individual task average return on the MT3 scenario of MiniGrid. We utilize the single-\n",
            "head architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE,\n",
            "the number of experts kis 2. The black dashed line represents the final single-task performance\n",
            "of PPO averaged across all tasks. For the evaluation metric, we compute the accumulated return\n",
            "averaged across all tasks. We report the mean and the 95% confidence interval across 30 different\n",
            "runs.\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnDoorKey\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.6Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.81.0Average ReturnDistShift\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.30.40.50.6Average ReturnMemory\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 12: Individual task average return on the MT5 scenario of MiniGrid. We utilize the single-\n",
            "head architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE,\n",
            "the number of experts kis 3. The black dashed line represents the final single-task performance\n",
            "of PPO averaged across all tasks. For the evaluation metric, we compute the accumulated return\n",
            "averaged across all tasks. We report the mean and the 95% confidence interval across 30 different\n",
            "runs.\n",
            "architecture for the actor and critic, hence each task has a decoupled output head from other tasks,\n",
            "easing the transfer learning experiment. However, they all share the representation stage (mixture\n",
            "of experts). To learn the novel tasks, we add randomly initialized output heads while keeping the\n",
            "mixture of experts frozen. For MT3→MT5 , the number of experts kis 2. On the other hand, for\n",
            "MT5→MT7 , we use 3 experts.\n",
            "B.3 C OSINE SIMILARITY\n",
            "We investigate the ability of MOORE to diversify the shared representations, compared to relaxing\n",
            "the hard constraint in Eq.1. Therefore, we replace the hard constraint with a regularization term\n",
            "19Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnDoorKey\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.751.00Average ReturnDistShift\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.6Average ReturnRedBlueDoors\n",
            "0 25 50 75 100\n",
            "#Epochs0.000.250.500.75Average ReturnLavaGap\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.30.40.50.6Average ReturnMemory\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.6Average ReturnSimpleCrossing\n",
            "0 25 50 75 100\n",
            "#Epochs0.00.20.40.60.8Average ReturnMultiRoom\n",
            "PPO MTPPO PCGrad MOE MOORE (ours)\n",
            "Figure 13: Individual task average return on the MT7 scenario of MiniGrid. We utilize the single-\n",
            "head architecture, for our approach MOORE as well as the related baselines. For MOORE and MOE,\n",
            "the number of experts kis 4. The black dashed line represents the final single-task performance of\n",
            "PPO averaged across all tasks. We show the accumulated return averaged across all tasks. We report\n",
            "the mean and the 95% confidence interval across 30 different runs.\n",
            "equivalent to a cosine similarity loss computed over the set of representations:\n",
            "l=Es∈S[hϕ(s)Thϕ(s)−Ik], (3)\n",
            "where we added a regularization weight which we set to 1. We benchmark MOORE against the\n",
            "Cosine-Similarity on the three scenarios of MiniGrid. As shown in Fig. 14, MOORE outperforms\n",
            "the baseline across all settings, highlighting the advantage of using Gram-Schmidt in diversifying\n",
            "the experts over regularization-based techniques. In addition, our approach is hyperparameter-free ,\n",
            "contrary to the regularization-based techniques that require delicate hyperparameter tuning.\n",
            "C C OMPUTATION AND MEMORY REQUIREMENTS\n",
            "The difference between MOORE and MOE is in the Gram-Schmidt stage, where we orthogonalize\n",
            "thekrepresentations. The time complexity of the Gram-Schmidt process is T=O(k2×d)(Golub\n",
            "& Van Loan, 2013; Mashhadi et al., 2021), where dis the representation dimension and kis the\n",
            "number of experts. MOORE and MOE can be considered as soft-MOE because they both compute\n",
            "the whole krepresentations from all the experts and then aggregate them. On the other hand, sparse-\n",
            "MOE approaches select top-k experts based on soft weights computed using a gating network. The\n",
            "trade-off between the representation capacity and time complexity is well-known. As a future work,\n",
            "we can investigate the adaptation of MOORE to pick only a few orthogonal experts, hence lowering\n",
            "the time complexity. MOORE is similar to the MOE baseline in terms of the memory required for\n",
            "20Preprint\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT3\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT5\n",
            "0 25 50 75 100\n",
            "#Epochs0.20.40.6Average ReturnMT7\n",
            "Cosine-Similarity MOORE (ours)\n",
            "Figure 14: Evaluating the diversity capabilities of our approach, MOORE, against using Cosine-\n",
            "Similarity. The study is conducted on the three MTRL scenarios of MiniGrid employing a single-\n",
            "head architecture. The number of experts kis2,3, and 4for MT3, MT5, and MT7, respectively.\n",
            "For the evaluation metric, we compute the accumulated return averaged across all tasks. We report\n",
            "the mean and the 95% confidence interval across 30different runs.\n",
            "storing all the experts. It is worth noting that it is also similar to PaCo (Sun et al., 2022) regarding the\n",
            "memory requirements; however, in the MetaWorld experiments, we used fewer experts than PaCo.\n",
            "D T HEGRAM -SCHMIDT PROCESS AND THE INITIAL EXPERT\n",
            "0 20 40 60 80 100\n",
            "#Epochs0.20.40.60.8Average ReturnMOORE-u1\n",
            "MOORE-u2\n",
            "MOORE-u3\n",
            "Figure 15: Ablation study on the effect of the\n",
            "initial expert selected for the Gram-Schmidt pro-\n",
            "cess with a multi-head architecture. The number\n",
            "of experts kis where u 1, u2, and u 3represent the\n",
            "representations of the three experts before apply-\n",
            "ing the Gram-Schmidt process. For the evalua-\n",
            "tion metric, we compute the accumulated return\n",
            "averaged across all tasks. We report the mean\n",
            "and the 95% confidence interval across 30differ-\n",
            "ent runs.In MOORE, we consider the first expert’s rep-\n",
            "resentation as the initial vector for the Gram-\n",
            "Schmidt process. In a normal setting, we can\n",
            "expect the process to yield a different set of or-\n",
            "thonormal vectors depending on the initial se-\n",
            "lected vector. We argue that it does not matter\n",
            "in our case since the representations are actually\n",
            "generated from the mixture of experts, that are\n",
            "being trained. We conduct an ablation study on\n",
            "the MT5 scenario of MiniGrid where we utilize\n",
            "3experts. We provide the variations of MOORE\n",
            "based on the initial vector selected for the Gram-\n",
            "Schmidt process. For instance, MOORE-u1 se-\n",
            "lects the representation of the first expert u1 as\n",
            "the initial vector of the Gram-Schmidt process\n",
            "(similar to the whole paper). On the other hand,\n",
            "MOORE-u2 and MOORE-u3 select the repre-\n",
            "sentation of the second u 2and third u 3expert,\n",
            "respectively, as the initial vector for the Gram-\n",
            "Schmidt process. As expected, Fig. 15 shows\n",
            "that the performance is comparable for different\n",
            "selected initial vectors.\n",
            "21Enhancing Robotic Manipulation: Harnessing the Power\n",
            "of Multi-Task Reinforcement Learning\n",
            "and Single Life Reinforcement Learning in Meta-World\n",
            "Ghadi Nehme1, Ishan Sabane1, Tejas Y. Deo1\n",
            "Abstract\n",
            "At present, robots typically require extensive training to successfully accomplish a\n",
            "single task. However, to truly enhance their usefulness in real-world scenarios, robots\n",
            "should possess the capability to perform multiple tasks effectively. To address this\n",
            "need, various multi-task reinforcement learning (RL) algorithms have been developed\n",
            "[1], including multi-task proximal policy optimization (PPO), multi-task trust region\n",
            "policy optimization (TRPO), and multi-task soft-actor critic (SAC). Nevertheless,\n",
            "these algorithms demonstrate optimal performance only when operating within an\n",
            "environment or observation space that exhibits a similar distribution. In reality, such\n",
            "conditions are often not the norm, as robots may encounter scenarios or observations\n",
            "that differ from those on which they were trained.\n",
            "For instance, if a robot is trained to pick and place a sphere from a specific set of\n",
            "locations, it should also possess the capability, during testing, to successfully pick and\n",
            "place a cube from different positions, at least those that are in close proximity to the\n",
            "trained positions. Addressing this challenge, algorithms like Q-Weighted Adversarial\n",
            "Learning (QWALE) [ 2] attempt to tackle the issue by training the base algorithm (gen-\n",
            "erating prior data) solely for a particular task, rendering it unsuitable for generalization\n",
            "across tasks.\n",
            "And so, the aim of this research project is to enable a robotic arm to successfully\n",
            "execute seven distinct tasks within the Meta World environment. These tasks encompass\n",
            "Pick and Place, Window Open and Close, Drawer Open and Close, Push, and Button\n",
            "Press. To achieve this, a multi-task soft actor critic (MT-SAC) is employed to train\n",
            "the robotic arm. Subsequently, the trained model will serve as a source of prior data\n",
            "for the single-life RL algorithm. The effectiveness of this MT-QWALE algorithm will\n",
            "be assessed by conducting tests on various target positions (novel positions), i.e., for\n",
            "example, different window positions in the Window Open and Close task.\n",
            "At the end, a comparison is provided between the trained MT-SAC and the MT-\n",
            "QWALE algorithm where the MT-QWALE performs better. An ablation study demon-\n",
            "strates that MT-QWALE successfully completes tasks with slightly more number of\n",
            "steps even after hiding the final goal position.\n",
            "1Stanford University\n",
            "1arXiv:2311.12854v1  [cs.AI]  23 Oct 20231 Introduction\n",
            "Reinforcement learning (RL) has made significant strides in various domains, including\n",
            "robotic manipulation, Atari games, etc. However, existing state-of-the-art RL methods\n",
            "require significantly more experience than humans to acquire even a single narrowly-defined\n",
            "skill. To make robots genuinely useful in realistic environments, it is crucial to develop\n",
            "algorithms that can reliably and efficiently learn a wide range of skills. Fortunately, in specific\n",
            "domains like robotic manipulation or locomotion, many tasks share common underlying\n",
            "structures, which can be leveraged to learn related tasks more efficiently.\n",
            "For example, most robotic manipulation tasks involve actions such as grasping, or moving\n",
            "objects within the workspace. Although current methods can learn individual skills like pick\n",
            "and place or hanging a mug or hammering a screw, we require algorithms that can efficiently\n",
            "learn and utilize shared structures across multiple related tasks. These algorithms should be\n",
            "able to learn new skills quickly by leveraging the learned structure, such as screwing a jar lid\n",
            "or hanging a bag. Recent advancements in machine learning have demonstrated exceptional\n",
            "generalization capabilities in domains like speech, implying that similar generalizations\n",
            "should be achievable in RL settings encompassing diverse tasks.\n",
            "Emergingapproachesinmeta-learningandmulti-taskreinforcementlearningshowpromise\n",
            "in addressing this challenge. Multi-task RL methods aim to learn a single policy capable of\n",
            "efficiently solving multiple tasks, surpassing the performance of learning individual tasks\n",
            "independently. On the other hand, meta-learning methods train on a multitude of tasks and\n",
            "optimize for rapid adaptation to new tasks.\n",
            "In the context of multi-task and meta-RL settings, it is crucial to address the challenge\n",
            "of agent failure when confronted with new and unseen situations. While a multi-task RL\n",
            "algorithm may exhibit excellent performance across a set of 50 tasks, it can still struggle\n",
            "significantly when faced with an observation space different from its training data. This\n",
            "limitation becomes evident in scenarios where a robotic manipulator, trained to open\n",
            "drawers at specific locations, encounters a completely different drawer position during testing.\n",
            "Similarly, for search-and-rescue disaster relief robots navigating through buildings, the\n",
            "possibility of encountering unfamiliar obstacles further highlights the need for adaptability\n",
            "and effective handling of new circumstances.\n",
            "Traditional RL algorithms aim to optimize the solution for a fixed set of tasks, without\n",
            "considering on-the-fly adaptations without human intervention. On the other hand, the\n",
            "Single Life RL (SLRL) setting encourages the autonomous and successful resolution of novel\n",
            "scenarios within a single trial. However, the current SLRL agents are primarily designed\n",
            "to handle novelties within a single environment, such as pick and place tasks or opening\n",
            "drawers.\n",
            "2 Related Work\n",
            "Many prior works and frameworks have been developed to test the effectiveness of the\n",
            "multi-task RL framework. In Meta-world: A benchmark and evaluation for multi-task and\n",
            "meta reinforcement learning [ 1], a framework has been developed to evaluate the capabilities\n",
            "of current multi-task and meta-reinforcement learning methods and make it feasible to design\n",
            "new algorithms that actually generalize and adapt quickly on meaningfully distinct tasks.\n",
            "Evaluation protocols and task suites in Metaworld are broad enough to enable this sort of\n",
            "generalization while containing sufficient shared structure for generalization to be possible.\n",
            "2You Only Live Once: Single-Life Reinforcement Learning [ 2] introduces a challenge where\n",
            "an agent must successfully complete a task within a single trial without human intervention,\n",
            "adapting to novel situations. Standard episodic RL algorithms struggle in this setting,\n",
            "prompting the proposal of Q-weighted adversarial learning (QWALE). QWALE leverages\n",
            "distribution matching and prior experience to guide the agent in novel states, resulting in\n",
            "20-60% higher success rates in single-life continuous control problems. This work provides\n",
            "valuable insights into autonomously adapting to unfamiliar situations, highlighting the\n",
            "effectiveness of distribution matching-based methods in SLRL. The major drawback of this\n",
            "approach is that the prior data for each task is collected by training a task-specific algorithm\n",
            "like SAC and thus is not able to generalize well on multiple tasks.\n",
            "Multi-task reinforcement learning with soft modularization [ 3] addresses the challenge\n",
            "of multi-task learning in reinforcement learning, emphasizing the complexity of optimizing\n",
            "shared parameters across tasks. To overcome this, an explicit modularization technique\n",
            "is introduced, utilizing a routing network to reconfigure the base policy network for each\n",
            "task. The task-specific policy employs soft modularization, combining possible routes for\n",
            "sequential tasks. Experimental results on robotics manipulation tasks demonstrate significant\n",
            "improvements in sample efficiency and performance compared to strong baselines. Even\n",
            "though the performance was significantly improved due to the soft modularization approach,\n",
            "the agent will still fail badly when there is some distribution shift in the observations.\n",
            "Actor-mimic: Deep multitask and transfer reinforcement learning [4] introduces a novel\n",
            "approach to enable agents to learn and transfer knowledge across multiple tasks. By utilizing\n",
            "deep reinforcement learning and model compression techniques, the proposed \"Actor-Mimic\"\n",
            "method trains a single policy network guided by expert teachers. The learned representations\n",
            "demonstrate the ability to generalize to new tasks without prior expert guidance, speeding\n",
            "up learning in novel environments. Atari games serve as a testing ground to showcase the\n",
            "effectiveness of the approach. Even though the approach is computationally efficient, it does\n",
            "not focus on the behavior of the agent on the same task with novel scenarios/observations.\n",
            "Comparing task simplifications to learn closed-loop object picking using deep reinforce-\n",
            "ment learning [ 5] compares reinforcement learning-based approaches for object picking in\n",
            "unstructured environments with a robotic manipulator. It shows that policies learned through\n",
            "curriculum learning and sparse rewards achieve similar success rates to those initialized on\n",
            "simplified tasks, with successful transfer to the real robot. The robotics arm is controlled by\n",
            "moving the end-effector by a certain displacement vector and changing the torque that the\n",
            "grippy fingers should apply. Thus, the robotics arm is controlled with inverse kinematics.\n",
            "3 Background\n",
            "For each task T, we examine a finite horizon Markov decision process (MDP) where Mtasks\n",
            "exist in total. These MDPs can be denoted as (S, A, P, R, H, γ ), where both the state s∈S\n",
            "and action a∈Aare continuous. The stochastic transition dynamics are represented by\n",
            "P(st+1|st, at), and the reward function is denoted as R(st, at). The horizon is defined as H,\n",
            "andγrepresents the discount factor. To represent the policy parameterized by ϕ, we utilize\n",
            "πϕ(at|st), with the objective being to learn a policy that maximizes the expected return.\n",
            "In the context of multi-task RL, tasks are randomly selected from a distribution p(T), and\n",
            "each task corresponds to a distinct MDP.\n",
            "33.1 Meta-World\n",
            "Meta-World presents an open-source simulated benchmark designed for meta-reinforcement\n",
            "learning and multi-task learning. It encompasses 50 unique robotic manipulation tasks that\n",
            "involve the interaction of a robotic arm with diverse objects possessing varying shapes, joints,\n",
            "and connectivity. Each task necessitates the robot to perform a combination of reaching,\n",
            "pushing, and grasping actions, depending on the specific task at hand.\n",
            "Figure 1: Multi-Task 10 (MT-10) Environment in Meta-World\n",
            "3.1.1 Action Space\n",
            "The action space for Meta-World can be represented with the following vector:\n",
            "a=\u0014δx:3D space of the end-effector\n",
            "τ:normalized torque that the gripper fingers should apply\u0015\n",
            "∈R4\n",
            "In the reinforcement learning context, the action is the output of a trained agent.\n",
            "3.1.2 Observation Space\n",
            "The observation space for Meta-World can be represented with the following vector:\n",
            "o=\n",
            "x(i)\n",
            "E:3D Cartesian coordinates of End-Effector\n",
            "δ(i)\n",
            "G:Measurement of how open the gripper is\n",
            "x(i)\n",
            "1:3D position of the first object\n",
            "λ(i)\n",
            "1:Quaternion of the first object\n",
            "x(i)\n",
            "2:3D position of the second object\n",
            "λ(i)\n",
            "2:Quaternion of the second object\n",
            "xGoal:3D position of the goal\n",
            "∈R39, i∈ {t, t−1}\n",
            "In the reinforcement learning context, the observation is the output of a trained agent.\n",
            "3.1.3 Reward Functions\n",
            "In Meta-World, reward functions have two key factors. First, tasks must be manageable\n",
            "by existing single-task reinforcement learning algorithms. This is crucial for evaluating\n",
            "4multi-task and meta-reinforcement learning algorithms. Second, the reward functions should\n",
            "exhibit shared structure across tasks. This promotes the transfer of knowledge and skills\n",
            "between tasks, enhancing the effectiveness of multi-task and meta-reinforcement learning in\n",
            "Meta-World.\n",
            "3.2 Soft Actor-Critic for Reinforcement Learning\n",
            "This paper introduces Soft Actor-Critic (SAC) as the training method for policy optimization.\n",
            "SAC is a deep reinforcement learning approach that combines off-policy actor-critic techniques\n",
            "and aims to achieve task success while maximizing randomness in action selection. The\n",
            "parameterized soft Q-function, denoted as Qθ(st, at), where θrepresents the parameters, is\n",
            "considered. SAC involves optimizing three types of parameters: the policy parameters ϕ, the\n",
            "parameters of the Q-function θ, and a temperature parameter α. The objective of policy\n",
            "optimization can be defined as follows:\n",
            "Jπ(ϕ) =Est∼D\u0002\n",
            "Eat∼πϕ[αlogπϕ(at|st)−Qθ(st, at)]\u0003\n",
            "(1)\n",
            "Here, αis a learnable temperature that serves as an entropy penalty coefficient. It can\n",
            "be learned to maintain the desired entropy level of the policy, using the following equation:\n",
            "J(α) =Eat∼πϕh\n",
            "−αlogπϕ(at|st)−α˜Hi\n",
            "where ˜Hrepresents the desired minimum expected entropy. If the optimization of\n",
            "logπt(at|st)increases its value and reduces entropy, αis adjusted accordingly to increase\n",
            "the process.\n",
            "3.3 Multi-task Reinforcement Learning\n",
            "To extend SAC from single-task to multi-task scenarios, we introduce a single, task-\n",
            "conditioned policy π(a|s, z), where zrepresents a task embedding. The policy is optimized\n",
            "to maximize the average expected return across all tasks sampled from the distribution p(T).\n",
            "The objective of policy optimization is defined as follows:\n",
            "Jπ(ϕ) =ET ∼p(T)[Jπ,T(ϕ)],\n",
            "where Jπ,T(ϕ)is directly adopted from Equation 1, incorporating the specific task T.\n",
            "Similarly, for the Q-function, the objective is as follows:\n",
            "JQ(θ) =ET ∼p(T)[JQ,T(θ)]\n",
            "3.4 Single Life Reinforcement Learning\n",
            "Single-life reinforcement learning enables autonomous task completion in a single trial, in\n",
            "the presence of a novel distribution shift, by leveraging prior data.\n",
            "The prior data Dpriorconsists of transition data from transitions from source MDP\n",
            "Msource. The agent interacts with the target MDP defined by Mtarget (S, A, ˜P, R, ˜ρ, γ)\n",
            "The target MDP is assumed to have a novelty that is not present in the source MDP\n",
            "such as different dynamics or differences in the initial and final state distribution ˜ρ. The\n",
            "more similar the source and the target domains are, the effectiveness of the SLRL algorithm\n",
            "5would be much better. The main aim in the SLRL setting is to maximize J=Ph\n",
            "t=0γtR(st)\n",
            "whilst minimizing the number of steps taken to solve the task.\n",
            "Figure 2: Single Life Reinforcement learning [2]\n",
            "3.5 Q-weighted Adversarial Learning (QWALE)\n",
            "The desired behavior of the algorithm is to guide the agent toward the distribution of prior\n",
            "data, facilitating recovery from out-of-distribution states and encouraging task completion\n",
            "simultaneously. SLRL assumes access to have sub-optimal offline prior data making it\n",
            "agnostic to the quality of the prior data. The main aim of SLRL is to match the state-\n",
            "action pairs that will lead to task completion rather than matching the entire state-action\n",
            "distribution. By learning a discriminator which can classify whether states belong to prior\n",
            "data or not, the algorithm can down-weigh states which are not in the prior data using the Q\n",
            "values for the state action pairs. Thus, from the paper [ 2], Q-weighted adversarial learning\n",
            "(QWALE), minimizes DJS\u0000\n",
            "ρπ∥ρ∗\n",
            "target\u0001\n",
            "as follows:\n",
            "min\n",
            "πDJS\u0000\n",
            "ρπ(s, a)∥ρ∗\n",
            "target (s, a)\u0001\n",
            "= min\n",
            "πmax\n",
            "DEs,a∼ρ∗\n",
            "target[logD(s, a)] +Es,a∼ρπ[log(1 −D(s, a))]\n",
            "= min\n",
            "πmax\n",
            "DE\n",
            "s,a∼ρβ\u0014exp (Qπtarget (s, a)−Vπtarget (s))\n",
            "Eρβ[exp ( Qπtarget (s, a)−Vπtarget (s))]logD(s, a)\u0015\n",
            "+E\n",
            "s,a∼ρπ[log(1 −D(s, a))]\n",
            "≡min\n",
            "πmax\n",
            "DEs,a∼ρβ[exp ( Qπtarget(s, a)−Vπtarget(s)) log D(s, a)] +Es,a∼ρπ[log(1 −D(s, a))],\n",
            "where Eρs[exp ( Qπtarget(s, a)−Vπtarget(s))]canbeignored.\n",
            "QWALE trains a weighted discriminator (D) function using a fixed Q-function Q(s, a)\n",
            "trained in the source MDP to distinguish between useful transitions and the ones that are\n",
            "less useful. The discriminator outputs essentially act as rewards for the agent (actor) taking\n",
            "actions in the target MDP where +1 represents that the agent’s transitions are useful and\n",
            "0 represents that the transitions were not so useful. The reward for the given state action\n",
            "pair is updated using the negative log likelihood of the probability that the current state\n",
            "does not belong to the prior data. The reward for the given state action pair reduces when\n",
            "the state does not fall closer to the prior data, and using this, the online agent learns the\n",
            "dynamically changing environment to accomplish its newer task.\n",
            "64 Methodology\n",
            "4.1 Problem Statement\n",
            "We want to train a robotic arm to be capable of achieving multiple tasks of the MT-10 in a\n",
            "single life. We select seven tasks from the MT-10 (Figure 3). To do that, we first train an\n",
            "MT-SAC agent on these seven tasks and save the final replay buffer. We will explore different\n",
            "task embedding and select the one that optimizes the performance of the Multi-Task SAC\n",
            "on the seven tasks. Then, we will use this trained policy and replay buffer as the prior data\n",
            "of a Multi-Task QWALE.\n",
            "Replay Buffer \n",
            "Meta-World \n",
            "Environment \n",
            "Actor Critic Sample \n",
            "Tuples Sample \n",
            "Tuples \n",
            "Store Tuples \n",
            "Policy Network Improve Policy \n",
            "Evaluate Policy Soft  Update Q𝜃1 ( s, a), Q𝜃2( s, a)\n",
            "at st \n",
            "Q𝜃1 ( s, a), Q𝜃2( s, a)𝜋𝝓( a| s ) Task Embedding \n",
            "Target Q-networks Main Q-networks Tasks MT-SAC \n",
            "MT-QWALE Prior Data Discriminator \n",
            "Replay Buffer \n",
            "Meta-World \n",
            "Environment \n",
            "Actor Critic Sample \n",
            "Tuples Sample \n",
            "Tuples \n",
            "Store Tuples \n",
            "Policy Network Improve Policy \n",
            "Evaluate Policy Soft  Update Q𝜃1 ( s, a), Q𝜃2( s, a)\n",
            "at st \n",
            "Q𝜃1 ( s, a), Q𝜃2( s, a)Task Embedding \n",
            "Target Q-networks Main Q-networks Initialize Update \n",
            "𝜋 ( a| s ) q𝝓(prior | s ) \n",
            "Update rewards \n",
            "Figure 3: Training Multi-Task QWALE\n",
            "Toevaluatethemodel, wewillruntheMulti-TaskQWALEintheseventasksenvironments\n",
            "where the objects are randomly positioned, to introduce novelty in the environments. We\n",
            "will then record the number of successes and the average number of steps to completion and\n",
            "compare it to the performance of MTSAC. Finally, we will do an ablation study by hiding\n",
            "the final goal and observe whether the Multi-Task Agent is still able of achieving the tasks.\n",
            "74.2 Multi-Task SAC\n",
            "4.2.1 Model Architecture\n",
            "We use multi-task SAC as the algorithm to optimize our multi-task agent. For our critic,\n",
            "value and agent networks we use the architectures shown in Figure 4.\n",
            "Thenactions = 4, since the dimension of the action space of Meta-World is 4. Moreover,\n",
            "the dimension of the state space or observation space is 46 since we concatenate the Meta-\n",
            "World observation (39) with the task embedding (7). For training, this model, γ= 0.99,\n",
            "β= 0.0003andα= 0.0003which are the learning rates for the critic and value networks,\n",
            "and the actor-network respectively. We finally use a reward scale rscale= 2andτ= 0.005\n",
            "for the factor by which we are going to modulate the soft update of our target networks.\n",
            "Figure 4: Model Architectures of the Critic, Value, and Actor Networks.\n",
            "4.2.2 Task Embedding\n",
            "We concatenate the observation of the Meta-World with three different types of task embed-\n",
            "ding:\n",
            "•One-Hot Encoding which associates each task a unit basis vector ek.\n",
            "•Sine Encoding whichassociateseachtask ktothecorrespondingvector: [sin(k),sin(2k), . . . , sin(mk)],\n",
            "given that we have mtasks.\n",
            "•Learned Encoding which associates to a each task kthe following task embedding:\n",
            "Mek, where ekis a unit basis vector and Mis a learned matrix.\n",
            "84.3 Multi-task QWALE\n",
            "We introduce Muti-Task QWALE: Leveraging the Multi-Task SAC algorithm which performs\n",
            "poorly on novel tasks. We tweak the QWALE algorithm to allow the agent to handle a\n",
            "variety of tasks. In this approach, we generate the prior data using the trained MT-SAC\n",
            "algorithm on the different tasks in the 7 environments. For a given task, the Multitask\n",
            "QWALE weights the prior data which contains the same task ID more as compared to the\n",
            "data from the other task. The prior data unrelated to the task is weighted 50% less than\n",
            "the task-specific data in the prior buffer. The discriminator training remains the same and\n",
            "continues to classify the state as useful followed by weighting the data point using the Q\n",
            "values. Using this, the Multi-Task QWALE allows us to recover from bad states and to keep\n",
            "proceeding toward the goal states based on the prior data.\n",
            "The discriminator output is used to update the reward for the given state-action pair. The\n",
            "Multi-Task SAC keeps on training online on the observed states using this updated reward\n",
            "function. This allows the model to learn the environment continuously while preferring to\n",
            "visit states which are good if it gets stuck in a bad position or keeps on going to the same\n",
            "states.\n",
            "5 Experiments, Results & Discussion\n",
            "We are building a model capable of completing multiple tasks in asingle trial , in the\n",
            "presence of novel distribution shifts . In our case, the tasks are defined in Fig. 3. The\n",
            "novelties are to randomly vary the positions of the objects in the environments within a\n",
            "perimeter of radius 0.3 around the position it was trained on.\n",
            "5.1 Multi-Task SAC\n",
            "We first train an MT-SAC on the 7 tasks, for 10000 episodes per task to collect the prior\n",
            "data needed for MT-QWALE. We repeat this experiment for each task embedding and select\n",
            "the task embedding that leads to the best performance to use for the Multi-Task QWALE.\n",
            "We observe that using the sine encoding leads to the best performance (Fig. 5). In fact,\n",
            "using a sine encoding leads to sending more signals to the model, compared to a one-hot\n",
            "encoding, which leads to more shared structure between the tasks while still being linearly\n",
            "independent task embedding. Nonetheless, it is quite surprising to see that the learned\n",
            "embeddings achieved a very low success rate. In fact, we expected the model to learn the\n",
            "best representation of each task that will maximize the performance of the algorithm in each\n",
            "task.\n",
            "Figure 5: Success Rates of MTSAC in different environments\n",
            "9We also observe that the algorithm is less accurate when executing tasks that combine\n",
            "grasping and moving. In fact, the model was able to perform better on simple tasks that\n",
            "require the arm to only move for example Window Close or Drawer Close. On the other\n",
            "side, the model performs poorly on more complex tasks that require the combination of one\n",
            "or more elementary tasks like Pick and Place or Drawer Open which both require some sort\n",
            "of grasping and moving actions.\n",
            "5.2 Multi-Task QWALE\n",
            "Now that the prior data is collected, we run MT-QWALE in the 7 environments where\n",
            "objects are initialized in random positions. We run each experiment 10 times and collect the\n",
            "success rates of MT-QWALE in each environment as well as the average number of steps to\n",
            "completion. We obtain the following plots:\n",
            "Figure 6: Success Rate of single life RL experiments in different environments\n",
            "Figure7: AveragenumberofstepsofsinglelifeRLexperimentsindifferentenvironments\n",
            "We observe that all the tasks achieve almost 100% success rate except for the Pick and\n",
            "Place task which failed every single time (Fig. 6). Pick and Place is the most complex task\n",
            "in the subset of Meta-World tasks which explains the observed results. It is a combination\n",
            "of moving, grasping, and placing which is much more complex than the other tasks which\n",
            "require just movement and sometimes some grasping.\n",
            "On the other side, when looking at the average number of steps required to achieve a task\n",
            "10there is a close correlation between the number of elementary tasks required to achieve a task\n",
            "and the average number of steps to complete the task. In fact, in Fig. 7, simple tasks that\n",
            "require only one to two elementary tasks like Window Open, Drawer Close, or Push require\n",
            "less number of steps to complete the task. We also observe that more complex tasks that\n",
            "require a combination of grasping and moving like Drawer Open need more steps to complete\n",
            "the task. An unexpected result is the Window Close Task, which despite being a simple\n",
            "task requiring only a linear movement of the arm in the opposite direction of Window Open\n",
            "takes much more steps to complete compared to Window Open. The reason behind such\n",
            "behavior might be that the prior data as a whole contains knowledge that is more relevant\n",
            "to solving the task Window Open compared to Window Close. Thus, QWALE would be\n",
            "able to complete the former in fewer steps than the latter.\n",
            "5.2.1 Comparing MT-QWALE and MT-SAC in environments with novelty\n",
            "(a) Window Open\n",
            " (b) Drawer Open\n",
            " (c) Press Button\n",
            "(d) Window Close\n",
            " (e) Drawer Close\n",
            " (f) Push\n",
            "Figure 8: Trajectories of end-effector comparing the performance of MT-SAC and MT-\n",
            "QWALE in different environments with novelty\n",
            "We observe that MT-QWALE provides a big improvement in the performance of MT-SAC\n",
            "in environments with novelty, except for the Pick and Place task. In fact, in rare cases, we\n",
            "observe that both algorithms achieve the tasks in a similar number of steps as we see in\n",
            "the window open environment (Fig 8a). In the cases where MT-SAC fails in environments\n",
            "with novelty, we observe two different behaviors of MT-QWALE. In some cases, we observe\n",
            "that MT-QWALE achieves the task in a very little number of steps like in the Drawer Close\n",
            "environment (Fig 8e). This might be the case due to the fact that this is a simple task that\n",
            "requires the arm to just move and thus realize an elementary task. In other cases, we observe\n",
            "that MT-QWALE achieves the task with many steps like in the Drawer Open Environment\n",
            "(Fig (Fig 8b)), where the robotic arm had to both grasp the handle and move to achieve this\n",
            "task. This is a more complex task that requires two elementary tasks which may have led to\n",
            "11this higher number of steps to achieve the task.\n",
            "5.2.2 Ablation Study: MT-QWALE with hidden end goal\n",
            "Figure 9: Success Rate of single life RL experiments in different environments with\n",
            "Goal Masking\n",
            "Figure 10: Average number of steps of single life RL experiments in different environ-\n",
            "ments with Goal Masking\n",
            "In this ablation study, we conducted an experiment by concealing the end goal from the\n",
            "observation vector during action selection. Specifically, we assigned zero values to the end\n",
            "goal position. Our aim was to examine how the performance of the MT-QWALE agent is\n",
            "affected by the absence of end goal information.\n",
            "Interestingly, we found that both masking and not masking the end goal resulted in a\n",
            "comparable success rate. This indicates that the MT-QWALE agent is capable of accomplish-\n",
            "ing the task without direct knowledge of the end goal, relying solely on reward feedback to\n",
            "adjust its actions. However, we did observe that when the end goal was masked, the average\n",
            "number of steps taken was greater compared to when it was not masked. This outcome\n",
            "aligns with our expectations.\n",
            "Essentially, by reducing the input signal pertaining to the end goal, we anticipated\n",
            "that the model would make more errors before successfully completing the tasks, thereby\n",
            "necessitating a greater number of steps.\n",
            "126 Conclusion\n",
            "Inconclusion, wepresentMT-QWALE,anovelapproachformulti-taskQ-weightedadversarial\n",
            "learning. MT-QWALE builds upon QWALE, adapting it to handle multiple tasks within a\n",
            "single trial. We conduct experiments using seven tasks from Meta-World, namely Window\n",
            "Open and Close, Drawer Open and Close, Pick and Place, Push, and Button Press.\n",
            "To collect prior data for MT-QWALE, we train MT-SAC on the seven tasks using the\n",
            "best task embedding method we explored, which is sine encoding. We then evaluate the\n",
            "performance of MT-QWALE in these seven environments, incorporating a novelty factor by\n",
            "randomly placing objects within the environment. In comparison to MTSAC, MT-QWALE\n",
            "demonstrates successful completion of all tasks except for the most complex one, Pick and\n",
            "Place.\n",
            "To further investigate the impact of goal position on MT-QWALE’s performance, we\n",
            "conduct an ablation study. We find that hiding the end goal generally leads to an increase\n",
            "in the number of steps required to complete the task, highlighting MT-QWALE’s ability to\n",
            "navigate solely based on reward feedback.\n",
            "For future research, we have several plans in mind. Firstly, we intend to extend this study\n",
            "to include additional tasks and compare the results of MT-QWALE with QWALE using only\n",
            "prior data from a single task. Additionally, we aim to explore different model architectures\n",
            "for the value, actor, and critic networks, as well as investigate alternative methods for task\n",
            "ID embedding. Another avenue of exploration involves conducting ablation studies on other\n",
            "components of the observation vector.\n",
            "Finally, we are interested in introducing various types of novelties into the environment,\n",
            "such as changing gravity or adding wind, to further enhance the capabilities of our ap-\n",
            "proach. These future endeavors will allow us to deepen our understanding and improve the\n",
            "performance of MT-QWALE in a broader range of scenarios.\n",
            "References\n",
            "[1]Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn,\n",
            "and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta\n",
            "reinforcement learning. In Conference on robot learning , pages 1094–1100. PMLR, 2020.\n",
            "[2]Annie Chen, Archit Sharma, Sergey Levine, and Chelsea Finn. You only live once: Single-\n",
            "life reinforcement learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\n",
            "and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35,\n",
            "pages 14784–14797. Curran Associates, Inc., 2022.\n",
            "[3]Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning\n",
            "with soft modularization. Advances in Neural Information Processing Systems , 33:4767–\n",
            "4777, 2020.\n",
            "[4]Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask\n",
            "and transfer reinforcement learning. arXiv preprint arXiv:1511.06342 , 2015.\n",
            "[5]Michel Breyer, Fadri Furrer, Tonci Novkovic, Roland Siegwart, and Juan Nieto. Com-\n",
            "paring task simplifications to learn closed-loop object picking using deep reinforcement\n",
            "learning. IEEE Robotics and Automation Letters , 4(2):1549–1556, 2019.\n",
            "13MULTI LORA: D EMOCRATIZING LORA FOR BETTER\n",
            "MULTI -TASK LEARNING\n",
            "Yiming Wang, Yu Lin, Xiaodong Zeng, Guannan Zhang\n",
            "Ant Group\n",
            "Shanghai, China\n",
            "ABSTRACT\n",
            "LoRA achieves remarkable resource efficiency and comparable performance when adapting LLMs\n",
            "for specific tasks. Since ChatGPT demonstrated superior performance on various tasks, there\n",
            "has been a growing desire to adapt one model for all tasks. However, the explicit low-rank of\n",
            "LoRA limits the adaptation performance in complex multi-task scenarios. LoRA is dominated by a\n",
            "small number of top singular vectors while fine-tuning decomposes into a set of less important unitary\n",
            "transforms. In this paper, we propose MultiLoRA for better multi-task adaptation by reducing the\n",
            "dominance of top singular vectors observed in LoRA. MultiLoRA scales LoRA modules horizontally\n",
            "and change parameter initialization of adaptation matrices to reduce parameter dependency, thus\n",
            "yields more balanced unitary subspaces. We unprecedentedly construct specialized training data\n",
            "by mixing datasets of instruction follow, natural language understanding, world knowledge, to\n",
            "cover semantically and syntactically different samples. With only 2.5% of additional parameters,\n",
            "MultiLoRA outperforms single LoRA counterparts and fine-tuning on multiple benchmarks and\n",
            "model scales. Further investigation into weight update matrices of MultiLoRA exhibits reduced\n",
            "dependency on top singular vectors and more democratic unitary transform contributions1.\n",
            "1 Introduction\n",
            "In recent years, Large Language Models (LLMs) have manifested unprecedentedly superior performance in various\n",
            "natural language processing tasks[ 1,2,3,4]. As model scales up, high-level multi-task capabilities[ 5] emerges from\n",
            "LLMs. Capabilities such as real-world knowledge, logic reasoning and arithmetic skills from one LLM proves the\n",
            "feasibility of \"one model for all tasks\". However, scaling up LLMs by adding billions of parameters not only bring\n",
            "emergent abilities and grokking, but also dramatically increase training and down-stream adaptation costs. For instance,\n",
            "parameter counts of LLaMA[ 4] series range from 7 billion to 65 billion, and GPT-3[ 2] contains up to 175 billion\n",
            "parameters. Full parameter fine-tuning these models for down-stream adaptation yields huge amount of memory\n",
            "footprint and thus requires prohibitively expensive hardwares.\n",
            "To address the issue of hardware requirements for LLM adaptation, a solution called Parameter Efficient Fine-Tuning\n",
            "(PEFT) has been proposed. PEFT methods reduce VRAM usage of cached optimizer states[ 6] by only optimizing a\n",
            "fraction of model parameters while keeping the rest frozen. Various PEFT methods, such as adapter[ 7], p-tuning[ 8],\n",
            "IA3[9] and LoRA[ 6], have been suggested. Compared to other PEFT methods, LoRA possesses the advantages of: 1)\n",
            "high modularity for distribution, 2) mergeable weights for zero inference overhead. While LoRA has proven successful\n",
            "in single-task adaptation, its performance in more intricate multi-task settings of generative AI remains unexplored.\n",
            "Thus, a crucial question lingers: Can LoRA effectively adapts LLMs to complex multi-task scenarios as full parameter\n",
            "fine-tuning does?\n",
            "Works on applying PEFT methods on multi-task learning scenarios are in literature, albeit with certain limitations[ 10,\n",
            "11,12,13]. These proposed methods manage to improve multi-task benchmark performances with task information\n",
            "sharing or activation routing[ 13,10,11]. However, these dedicated modules add unaffordable overhead to transformer\n",
            "inference, which hinders their industrial application[ 6,14]. Another limitation is that the prior works focused on Natural\n",
            "1Our code is coming to GitHub soon.arXiv:2311.11501v1  [cs.LG]  20 Nov 2023PRIME AI paper\n",
            "Language Understanding (NLU), which may not be suitable for current generative LLMs. A mixture of NLU tasks\n",
            "are commonly used[ 10,11] despite that data samples of these tasks do not present much semantical or syntactical\n",
            "difference among them. More benchmarks on tasks of interest of generative LLMs, like instruction following, logic\n",
            "reasoning should be taken into consideration.\n",
            "Therefore, the research goal of this paper is to adapt LoRA for better multi-task learning while maintaining modularity\n",
            "and zero inference overhead of LoRA. We firstly reveal the fundamental difference between LoRA and full parameter\n",
            "fine-tuning with the help of Singular Value Decomposition (SVD, Section 3.2). We found dominance of top singular\n",
            "vectors in LoRA while fine-tuning is more democratic, as the residual weight decomposes into larger sets of unitary\n",
            "transforms of smaller importance. In order to mitigate the observed dominance, we propose to horizontally scale\n",
            "LoRAmodules (Section 3.3). MultiLoRA horizontally scales lora modules to reduce parameter dependency. Multi-\n",
            "LoRA divides LoRA along the rank, add learnable scaling factor and change the parameter initialization to enhance\n",
            "expressiveness of lora modules. Compared to conventional LoRA, MultiLoRA produces more democratic weight\n",
            "update matrices as those of full parameter fine-tuning.\n",
            "To better demonstrate the effectiveness of MultiLoRA, we constructed a comprehensive dataset composed of various\n",
            "tasks relevant to generative LLMs. A series of datasets from different domains are selected including instruction\n",
            "following[ 15], world knowledge[ 16], arithmetic reasoning[ 17] and NLU[ 18]. Both context and target of samples in\n",
            "aforementioned datasets exhibit strong semantical and syntactical differences, thus augmenting adaptation difficulty.\n",
            "With our multi-task datasets, we conducted extensive empirical experiments on LLaMA ranging from 7B to 65B.\n",
            "On benchmarks of MMLU[ 16] and SuperGLUE[ 18], we found MultiLoRA consistently outperforms LoRA even\n",
            "under smaller parameter budget and can perform on-par with full-parameter fine-tuning. We further dive into obtained\n",
            "weight update matrices with SVD. Side by side comparison to full parameter fine-tuning suggests that MultiLoRA ex-\n",
            "hibit a higher degree of subspace overlapping and more similar singular value distribution, indicating successful\n",
            "democratization of unitary transforms contribution.\n",
            "Therefore, our main contributions can be summarized as follows:\n",
            "•We find dominance of unitary transforms in weight update matrices of LoRA, while fine-tuning produces more\n",
            "democratic contribution distribution.\n",
            "•We propose MultiLoRA to mitigate dominance seen in LoRAand democratize contributions of its unitary\n",
            "transforms.\n",
            "•We propose a multi-task learning scheme based on mixture of tasks of interest of generative LLMs, to cover\n",
            "semantically and syntactically different samples. Our proposed MultiLoRA exhibits stronger consistency than\n",
            "LoRA and can outperform full parameter fine-tuning on various tasks and model scales.\n",
            "2 Related Work\n",
            "2.1 PEFT\n",
            "PEFT methods lowers hardware requirement of model fine-tuning by significantly reducing trainable parameters\n",
            "and consequently optimizer states cached in VRAM. By exploiting the local optimum of a pretrained model, a\n",
            "much smaller solution space brought by reduce trainable parameters helps PEFT methods achieve comparable tuning\n",
            "performance[ 19,20]. PEFT can be classified into two categories: 1) reparameterization-based methods[ 21,22] that\n",
            "retrain a portion of the parameters and 2) addition-based methods that train additional parameters[ 6,23,7]. Recent\n",
            "works in PEFT focus on resource efficiency[ 7,9,6,23]. LoRA[ 6] fits incremental weights by decomposing them\n",
            "into low-rank matrices. (IA)3tunes hidden states with learned multipliers. AdaLoRA[ 23] adds importance-aware\n",
            "pruning mechanisms to further improve resource efficiency. There’re also work focusing on ensemble learning with\n",
            "adapters. AdaMix[ 13] and UniPELT[ 24] integrate existing PEFT methods into a unified framework to boost adaptation\n",
            "performance.\n",
            "2.2 Multi-Task Learning with PEFT\n",
            "In multi-task learning with PEFT, adapter is utilized for code summarization across different programming languages[ 12].\n",
            "HyperFormer[ 10] assigns task-related weights to adapter[ 7] activations using shared hypernets across layers and tasks.\n",
            "Multitask Prompt Tuning[ 11] extends prompt tuning by firstly distilling from source prompts adapted for various tasks\n",
            "and further finetunes with low rank updates. While these methods have shown effectiveness, the additional weights\n",
            "cannot be seamlessly integrated into the base model, resulting in inevitable inference latency that is impractical for LLM\n",
            "serving[ 6,14]. Moreover, the emphasis in the multi-task setting has predominantly been on NLU tasks, disregarding\n",
            "the tasks that are of interest to generative LLMs.\n",
            "2PRIME AI paper\n",
            "3 Method\n",
            "3.1 Background\n",
            "Before formal explanation on design choices of MultiLoRA, a few notations are proposed base on LLaMA and LoRA.\n",
            "3.1.1 LLaMA\n",
            "LLaMA model consists of Lstacked decoder layers, where each block contains two submodules: a multi-head attention\n",
            "(MHA) and a fully connected FFN. Given the input sequence x∈Rn×d, MHA performs the attention function in\n",
            "parallel h heads:\n",
            "head i=Softmax (xWq_proj\n",
            "i (xWk_proj\n",
            "i )⊤\n",
            "√dn)xWv_proj\n",
            "i ,MHA (x) =Concat (head 1, . . . , head n)Wo_proj, (1)\n",
            "where Wq_proj\n",
            "i , Wk_proj\n",
            "i , Wv_proj\n",
            "i ∈Rd×dhare query, key and value projections of ithattention head and Wo_proj∈\n",
            "Rd×dis the output projection to aggregate multi-head outputs. dhis typically set to d/h. The other important module is\n",
            "a MLP which consists of three linear transformations, namely up_proj, down_proj, gate_proj with a SwiGLU activation\n",
            "in between:\n",
            "MLP(x) =SwiGLU (xWup_proj(xWgate _proj))Wdown _proj, (2)\n",
            "where Wup_proj, Wgate _proj∈Rd×dmid, dmid> d andWdown _proj∈Rdmid×d. Layer normalization is applied\n",
            "before and after the attention module.[4]\n",
            "3.1.2 Low-Rank Adaptation\n",
            "Given target module with weight W∈Rd×k, LoRA inserts two sequential low rank matrices to fit the residual weights\n",
            "for adaptation. The forward computation of adapted module writes as follow:\n",
            "y′=y+ ∆y=Wx+BAx, (3)\n",
            "where A∈Rd×r, B∈Rr×kwithr≪min(d, k). Either AorBis initialized with zeroes and the other is initialized\n",
            "with Kaiming Uniform[ 25] to force ∆y= 0at the very beginning. Analysis on weight update matrices suggest that\n",
            "LoRA work by enhancing existing feature transforms in original model weight[6].\n",
            "3.2 Difference between LoRA and fine-tuning\n",
            "Although LoRA achieves comparable performances to fine-tuning on many benchmarks, it is essential to understand the\n",
            "underlying differences between the two approaches. To shed light on this question, We train LLaMA-7B on Alpaca and\n",
            "MMLU using both methods2to get weight update matrices ∆Wand conduct an analysis of the weight update matrices\n",
            "using SVD.\n",
            "Figure 1 illustrates singular value distribution of ∆WFTand∆WLoRA. For better visualization, we plot the negative\n",
            "logarithms of the singular values ( −log(s)) . The empirical distribution of fine-tuning exhibits a bell-shaped curve\n",
            "while the distribution for LoRA falls at both ends of the spectrum. The extreme bimodal distribution of LoRA arises\n",
            "from the constraint that Rank (∆WLoRA)should not exceed r, resulting in at least ( k−r) singular values being zero.\n",
            "Interestingly, we also noticed an inverse trend in the counts of top singular values. In LoRA, the count increased with\n",
            "the magnitude of the singular values, while fine-tuning exhibited the opposite behavior. This suggests that LoRA\n",
            "predominantly relies on a small group of singular vectors, whereas fine-tuning distributes importance more evenly\n",
            "among singular vectors. Such phenomenon can also be observed on LoRA trained on other datasets or publicly available\n",
            "LoRA weights, indicating observed dominance arises from the structural design of LoRA (refer to Appendix B for\n",
            "more examples).\n",
            "Based on these findings, we can infer that the dominance observed in LoRA may limit its adaptation performance,\n",
            "particularly in complex multi-task scenarios that require enhancement of multiple distinct feature transforms. ∆Wof\n",
            "full parameter fine-tuning decomposes into a larger set (more specifically equals rank of original weight matrix) of\n",
            "unitary transforms. In contrast, LoRA’s explicit rank limitation restricts it to decompose into a smaller number ( r) of\n",
            "unitary transforms. As a result, the expressiveness of LoRA may be constrained compared to full parameter fine-tuning.\n",
            "3.3 Scaling LoRA to Democratize Unitary Transform Contribution\n",
            "2LoRA hyperparameters set to r= 64 andα= 64\n",
            "3PRIME AI paper\n",
            "1e1.0 1e2.0 1e3.0 1e4.0 1e5.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "0100200300400500Count\n",
            "LoRA Full Parameter Fine-tuning\n",
            "(a)\n",
            "1e1.0 1e1.5 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "050100150200250300350Count\n",
            "LoRA Full Parameter Fine-tuning (b)\n",
            "Figure 1: Top singular value distribution of weight update matrix of ∆Wv_proj. (a) Complete view of the histogram. (b)\n",
            "Close-up view on top singular values. Both histograms are plotted based on on the negative logarithms of the singular\n",
            "values −log(s), where left end of horizontal axis represents larger singular values. Bell-shape curved of full parameter\n",
            "fine-tuning indicates a democratic composition of a large number of relatively less important unitary transforms. On\n",
            "the hand, LoRA heavily relies on a small group of important unitary transforms, which could hurt complex multi-task\n",
            "adaptation.\n",
            "+\n",
            "Figure 2: Overview of MultiLoRA. Multiple\n",
            "parallel LoRA modules are used to adapt tar-\n",
            "get weight matrix. Parameter initialization and\n",
            "zero-initialized scaling factor are introduced to\n",
            "democratize residual weight updates.In Section 3.2, we observe a small group of top singular vectors dom-\n",
            "inate weight update matrices of LoRA ∆WLoRAwhile top singular\n",
            "vectors contribute more evenly to ∆WFT. In order to match fine-\n",
            "tuning in complex task adaptation, we propose MultiLoRA aiming at\n",
            "producing less polarized weight update matrices ∆W. MultiLoRA in-\n",
            "serts multiple parallel LoRAs to reduce parameter sharing, changes\n",
            "parameter initialization to enable larger optimization search space\n",
            "and implement starting point initialization with a learnable parameter.\n",
            "Figure 2 shows the overview of MultiLoRA. There’re 3 major differ-\n",
            "ence to original LoRA: horizontal scaling of LoRA modules, scaling\n",
            "factors and parameter initialization.\n",
            "3.3.1 Scaling LoRA Horizontally\n",
            "Given that LoRA performs closely despite scaling up the rank r, our\n",
            "key strategy of depolarization is parallelism. Through parallelism,\n",
            "incremental activation ∆yis further decomposed into a series of inde-\n",
            "pendent variables, which allows for more degrees of freedom during\n",
            "optimization. Bear in mind that under the same parameter budget,\n",
            "decomposing one large LoRA module into multiple small LoRAs can-\n",
            "not augment rank of ∆Wasrank(AB)≤min(rank(A), rank (B))\n",
            "andrank(A+B)≤rank(A) +rank(B). Corresponding weight\n",
            "matrices are noted as {Ai∈Rr×k}i∈[1,n],{Bi∈Rd×r}i∈[1,n]. Thus, the forward computation of MultiLoRA writes\n",
            "as:\n",
            "∆y=nX\n",
            "i=1scaling iBiAix, (4)\n",
            "Comparing with the forward of LoRA, MultiLoRA differs in that less parameter dependency brought to {Bi}. Paral-\n",
            "lelism has no effect on Aas intermediate mi=Aixis equivalent to reshape the result of [A⊤\n",
            "0, . . . , A⊤\n",
            "n]⊤x. But here\n",
            "comes the major difference.\n",
            "4PRIME AI paper\n",
            "3.3.2 Parameter Initialization\n",
            "Scaling LoRA horizontally allows for independent feature transform especially the up-projection of {Bi}. To further\n",
            "push the expressiveness of {Bi}, we change its parameter initialization to Kaiming-Uniform [ 25] instead of all zeroes\n",
            "and consequently introduce a learnable scaling factor to implement the starting point initialization.\n",
            "The zero initialization seen in Bof LoRA aims to keep activation unchanged before training. Such practice, we term as\n",
            "Starting Point Initialization , is commonly seen in PEFT methods but may be implemented differently. With starting\n",
            "point initialization, tuning a pretrained LLM essentially becomes optimizing in a much smaller parameter space around\n",
            "the local optimum of pretrained models. However, zero initialization is a double-edged sword. It is also infamous\n",
            "for introducing redundancy and breaks asymmetry[ 26,25], yielding limited expressiveness of networks despite faster\n",
            "convergence speed during adaptation.\n",
            "To take advantage of the starting point initialization and mitigate the drawbacks of zero initialization, Multi-\n",
            "LoRA changes initialization of {Bi}to Kaiming-Uniform and implements stating point initialization with zero-\n",
            "initialized learnable scaling factors scaling i∈Rk. Kaiming-Uniform has been shown to improve the generalization\n",
            "performance of neural networks and is the default parameter initialization method in PyTorch[27] implementation.\n",
            "4 Experiments\n",
            "In this section, we evaluate our proposed method from three aspects, namely memory profile, throughput and downstream\n",
            "performance. All our experiments are conducted with LLaMA series[4], ranging from 7B to 65B.\n",
            "4.1 Experiment Setups\n",
            "Model Size Method MMLU Boolq MultiRC RTE WIC\n",
            "7BZero-Shot 35.1 66.5 42.3 57.0 49.4\n",
            "FT 45.3 87.6 84.5 87.0 71.2\n",
            "LoRA r=96 44.7 86.0 81.7 86.6 67.6\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 45.1 88.7 83.8 85.6 70.2\n",
            "13BZero-Shot 46.9 65.0 43.4 60.6 49.5\n",
            "FT 51.3 87.1 85.7 90.8 74.3\n",
            "LoRA r=96 51.0 87.3 86.1 91.7 69.9\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 51.3 86.7 84.7 91.4 75.4\n",
            "30BZero-Shot 57.8 74.6 46.9 53.4 50.0\n",
            "FT 59.2 89.3 87.9 92.8 74.0\n",
            "LoRA r=96 58.8 89.7 87.0 91.0 74.1\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 59.1 89.5 88.1 93.1 74.1\n",
            "65BZero-Shot 63.5 73.6 48.3 59.6 51.3\n",
            "FT 64.6 91.6 90.1 93.9 75.4\n",
            "LoRA r=96 64.2 91.4 90.0 93.1 74.5\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 63.3 91.0 90.2 93.5 74.7\n",
            "Table 1: Main results on MMLU and SuperGLUE using LLaMA of all scales trained in conventional single dataset\n",
            "setup. MMLU is tested with 5-shot prompts and SuperGLUE are tested with zero-shot. MultiLoRA, LoRA and full\n",
            "parameter fine-tuningproduces similar results on single dataset setup.\n",
            "4.1.1 Training Data\n",
            "To evaluate on tasks of interest of generative LLMs, we build multi-task datasets encompassing Alpaca[ 15] for\n",
            "instruction following, MMLU[ 16] for world knowledge, GSM8K[ 17] for arithmetic reasoning and SuperGLUE[ 18] for\n",
            "NLU. Therefore, our mixture of tasks covers semantically and structurally different samples. In terms of source and\n",
            "target sequence length, samples from MMLU and SuperGLUE consist of single-choice questions with very short target\n",
            "lengths, typically one token. On the other hand, Alpaca and GSM8k contain longer target sequences. From the aspect\n",
            "of task semantic, the subjects covered by each dataset differ. MMLU encompasses real-world knowledge across various\n",
            "domains such as humanities, STEM, and social sciences, offering different levels of difficulty. In contrast, Alpaca\n",
            "focuses primarily on aligning model output with human preferences. GSM8k train the models to generate logical and\n",
            "step-by-step responses to questions.\n",
            "5PRIME AI paper\n",
            "To ensure consistency in evaluation, we follow QLoRA[ 28] and MeZO[ 29] to verbalize samples of MMLU and\n",
            "SuperGLUE, respectively. This verbalization process helps standardize input data across tasks, enabling fair comparisons.\n",
            "During training, we introduce random shuffling to enhance the learning process and prevent any bias that may arise\n",
            "from the ordering of the samples.\n",
            "4.1.2 Baselines\n",
            "We use models from LLaMA[ 4] series as the base model. In our comparative analysis, we consider two baselines: full\n",
            "parameter fine-tuning (referred to as FT) and single LoRA (referred to as LoRA). To establish a strong single LoRA\n",
            "baseline, we incorporate LoRA modules alongside all linear layers of LLaMA. Specifically, we insert LoRA modules in\n",
            "q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj modules in LLaMA. The more layers that are adapted by\n",
            "LoRA, the better down-stream task performances will be[28, 6].\n",
            "For Boolq, MultiRC, RTE and WIC, we report zero-shot performances and we report 5-shot results for MMLU. Instead\n",
            "of reporting the individual best task scores, we report the score of each task when the best average score is achieved to\n",
            "emphasize multi-task capability. The hyperparameter settings employed in our experiments are detailed in Appendix\n",
            "A. All experiments are conducted using 8 A100 80G GPUs . Python library PEFT[ 30] is used to help implement\n",
            "MultiLoRA and LoRA. We use Deepspeed ZeRO-3[ 31] for distributed training and offload optimizer states and model\n",
            "parameters for larger training throughput.\n",
            "4.2 Evaluation Results\n",
            "Model Size Method # Params MMLU Boolq MultiRC RTE WIC A VG.\n",
            "7BFT 100% 49.5 88.4 87.2 85.2 74.0 76.9\n",
            "LoRA r=96 3.6% 47.7 88.2 85.4 83.4 71.6 75.2\n",
            "LoRA r=160 5.9% 50.2 87.7 85.3 83.3 70.1 75.3\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 3.6% 51.2 87.8 88.7 89.7 70.8 77.6\n",
            "MultiLoRAn=5\n",
            "r=32(Ours) 6.0% 51.4 88.5 89.4 89.4 71.4 78.0\n",
            "13BFT 100% 51.4 89.2 89.3 91.3 75.1 79.2\n",
            "LoRA r=96 2.9% 49.7 89.7 88.5 87.0 71.5 77.2\n",
            "LoRA r=160 4.8% 50.4 89.4 88.4 87.6 72.1 77.5\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 2.9% 52.6 89.4 89.9 86.9 74.1 78.5\n",
            "MultiLoRAn=5\n",
            "r=32(Ours) 4.8% 52.9 89.3 89.5 90.3 74.3 79.4\n",
            "30BFT 100% 57.5 90.5 91.0 91.7 75.9 81.3\n",
            "LoRA r=96 2.2% 57.1 90.2 90.5 90.5 74.0 80.4\n",
            "LoRA r=160 3.7% 56.8 90.8 90.1 89.9 73.8 80.2\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 2.3% 58.4 90.6 90.5 91.5 74.9 81.1\n",
            "MultiLoRAn=5\n",
            "r=32(Ours) 3.8% 58.0 91.7 90.6 91.9 75.2 81.2\n",
            "65BFT 100% 66.4 91.7 91.3 93.9 76.5 83.9\n",
            "LoRA r=96 1.8% 65.9 91.3 90.8 92.4 75.1 83.1\n",
            "LoRA r=160 3.1% 65.8 90.9 90.4 93.6 75.5 83.2\n",
            "MultiLoRAn=3\n",
            "r=32(Ours) 1.8% 65.9 91.5 90.5 93.8 76.2 83.5\n",
            "MultiLoRAn=5\n",
            "r=32(Ours) 3.1% 66.3 91.8 90.1 93.3 76.6 83.6\n",
            "Table 2: Evaluation results on MMLU and SuperGLUE using LLaMA of all scales trained on our mixture. We report\n",
            "score of each task when the best average score is achieved throughout training. MMLU is tested with 5-shot prompts\n",
            "and SuperGLUE are tested with zero-shot. MultiLoRA produces better and more consistent results compared to LoRA.\n",
            "Benchmark performances trained on mixed data are listed in Table 2. Comparing these results to those obtained from\n",
            "single datasets (listed in Table 1), training on mixed datasets generally leads to benefits across all tasks and model\n",
            "scales, resulting in improved benchmark scores to varying extents. Based on these findings, we draw the following\n",
            "conclusions:.\n",
            "MultiLoRA consistently outperforms LoRA and achieves better results than full parameter fine-tuning on\n",
            "smaller models. Across all benchmarks and model scales, MultiLoRA demonstrates stronger data fitting capabilities\n",
            "and outperforms the LoRA counterpart with the same parameter budget by a notable margin. For instance, Multi-\n",
            "LoRA improves upon LoRA ’s performance by 3.5% on MMLU for LLaMA-7B and by 5.9% on RTE for the same\n",
            "model. On average, MultiLoRA surpasses LoRA in terms of the evaluated tasks’ average score by 2.8%. Notably,\n",
            "MultiLoRA even outperforms full parameter fine-tuning on smaller models (7B and 13B), only slightly falling behind\n",
            "on larger scales. Specifically, MultiLoRA achieves an average score improvement of 1.1% compared to full parameter\n",
            "6PRIME AI paper\n",
            "fine-tuning on LLaMA-7B, and a slight 0.3% decrease on LLaMA-65B. These significant improvements highlight\n",
            "MultiLoRA ’s superior capability in complex multi-task adaptation.\n",
            "MultiLoRA exhibits small performance fluctuations comparable to full parameter fine-tuning in complex multi-\n",
            "task learning scenarios. On smaller models, LoRA tends to show performance variability, with more frequent\n",
            "fluctuations between different tasks. For example, on LLaMA-7B, compared to full parameter fine-tuning , MultiRC,\n",
            "RTE, and WIC scores exhibit fluctuations of over 3% in LoRA . In contrast, both full parameter fine-tuning and\n",
            "MultiLoRA yield consistent individual task scores. The observed fluctuations in LoRA can be attributed to the\n",
            "dominance of top singular vectors, as noted in Section 3.2, where a small number of unitary transforms carry significant\n",
            "importance.\n",
            "In the single dataset setting, MultiLoRA performs similarity to full parameter fine-tuning and LoRA. Table 1\n",
            "shows evaluation results of models trained on single dataset. Across the 5 tested tasks and 4 scales, full parameter\n",
            "fine-tuning performs the best on 9 combinations, while MultiLoRA and LoRA perform the best in 7 and 5 combinations,\n",
            "respectively (MultiLoRA performs equally to LoRA on WIC for LLaMA-30B). Based on these observations, we\n",
            "cannot definitively declare one approach as superior. However, in the multi-task setting, MultiLoRA and full parameter\n",
            "fine-tuning demonstrate better performances compared to LoRA.\n",
            "4.3 Resources & Throughput Analysis\n",
            "Training throughput, VRAM usage and inference latency are crucial for generative LLMs. In this section, In this\n",
            "section, we thoroughly examine the resource usage and throughput of MultiLoRA as we scale up the number of parallel\n",
            "LoRA modules n. We primarily focus on VRAM usage and throughput during training as MultiLoRA inherits zero\n",
            "inference overhead from original LoRA. Our benchmark protocol involves training LLaMA-7B on sequences of 1024\n",
            "tokens using 8 A100 GPUs and recording the peak VRAM usage and throughput3. Deepspeed ZeRO-3 and model\n",
            "parameter offload are activated to better evaluate impacts brought by MultiLoRA.\n",
            "32 64 96 160 192\n",
            "n×r050100150200250300350400Tokens/(GPU Second)\n",
            "LoRA\n",
            "MultiLoRA\n",
            "Fine-tuning\n",
            "(a) Throughput\n",
            "32 64 96 160 192\n",
            "n×r0102030405060VRAM/GPU (GB)Fine-tuning\n",
            "LoRA\n",
            "MultiLoRA (b) VRAM\n",
            "Figure 3: (a) Throughput and (b) peak VRAM usage benchmarked when training LLaMA-7Bwith sequences of 1024\n",
            "tokens and batch size of 1. n×ron horizontal axis indicates total rank of LoRA and MultiLoRA. Thanks to high\n",
            "parallelism of MultiLoRA, training throughput is almost identical to LoRA. VRAM usage scales up linearly with the\n",
            "number of parallel LoRA modules.\n",
            "Results are listed in Figure 3. On the horizontal axis, n×rdenotes the equal total rank of MultiLoRA ( nparallel\n",
            "LoRA of rank r) and LoRA (one LoRA of rank n×r).\n",
            "Training throughput is one of the advantages of using MultiLoRA as other PEFT methods. A limitation with full\n",
            "parameter fine-tuning lies in the fact that cached optimizer states can consume a significant portion of the VRAM.\n",
            "Specifically, when training a 7B model with AdamW[ 32], cached optimizer states can occupy up to 70% of the available\n",
            "VRAM, and over 48% with SGD[ 33]. Thanks to the significantly reduced number of trainable parameters, finite VRAM\n",
            "can be leveraged to load more data samples, leading to larger training throughput. Additionally, due to the parallelism\n",
            "inherent in MultiLoRA, multiple LoRA modules do not introduce notable latency and the throughput remains close to\n",
            "3We train MultiLoRA with individual rank of 32.\n",
            "7PRIME AI paper\n",
            "that of LoRA, around 400 tokens per GPU per second. In our benchmarking, the throughput of MultiLoRA is almost\n",
            "twice that of full parameter fine-tuning (208 tokens per GPU per second).\n",
            "For VRAM usage, peak memory scales up much faster than LoRA. In order to optimize multiple parallel LoRA modules,\n",
            "multiple copies of activations should be cached in VRAM. Therefore, one major drawback of MultiLoRA is activation\n",
            "VRAM usage scales linearly with number of parallel LoRA modules which can be unaffordable in long sequence\n",
            "training. In our benchmark, training LLaMA-7B with sequences of 1024 tokens with n= 5would use more VRAM\n",
            "than full parameter fine-tuning.\n",
            "5 Understanding MultiLoRA\n",
            "In this section, we apply SVD on weight update matrices trained with LLaMA-7B in Section 4.1 to investigate why\n",
            "MultiLoRA outperforms LoRA in complex task adaptation. Specifically, subspace similarity and magnitudes of singular\n",
            "value are thoroughly studied for MultiLoRA, LoRA and fine-tuning.\n",
            "5.1 Comparison with Fine-tuning\n",
            "to demonstrate a higher degree of similarity to full parameter fine-tuning of MultiLoRA, we utilize SVD to compare\n",
            "weight update matrices ∆Wof LoRA and MultiLoRA. Specifically, we focus on comparing the subspace coverage of\n",
            "singular vectors and the magnitudes of singular values.\n",
            "As for subspace similarity of singular vectors, we follow [ 6] to use ϕ(∆W′,∆W, i, j )in Equation 5, the Frobenius\n",
            "norm of cosine similarity between top-i and top-j singular vectors of two weight update matrices.\n",
            "ϕ(∆W′,∆W, i, j ) =∥U⊤\n",
            "iU′\n",
            "j∥2\n",
            "F\n",
            "min(i, j)∈[0,1], (5)\n",
            "where Ui=U[:,:i]andU′\n",
            "j=U′[:,:j]are stacked top-i and top-j singular vectors.\n",
            "Moreover, the magnitudes of singular values offer valuable insights into the relative importance of each singular\n",
            "vector. Larger singular values signify a greater contribution to the overall data representation. In Section 3.2, we find\n",
            "∆WLoRAis very polarized as proportion of top singular values is largest. By comparing the singular value distribution,\n",
            "we want to find out whether MultiLoRA manages to balance contribution of each singular vectors.\n",
            "5.1.1 Subspace Comparison\n",
            "0 10 20\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "1×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "3×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "5×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "FT,WV\n",
            "FT)\n",
            "0.00.20.40.60.81.0\n",
            "Figure 4: Subspace similarity to fine-tuning of LoRA ( 1), MultiLoRA ( 2, 3) and fine-tuning with a different random seed\n",
            "(4). LoRA ( 2) and MultiLoRA ( 3) share same parameter budget but MultiLoRA exhibits stronger subspace similarity to\n",
            "fine-tuning. Heatmap of MultiLoRAn=3\n",
            "r=32does not differ much from that of MultiLoRAn=5\n",
            "r=32. Only i, j∈[1,30]are\n",
            "presented for better visibility.\n",
            "Orthonormal singular vectors define the \"direction\" of data transform. By measuring the subspace overlapping with\n",
            "ϕ(∆W′,∆W), we can measure the degree of similarity between two transforms. We randomly choose value projection\n",
            "of the 15 thdecoder layer to calculate ϕ(∆WLoRA,∆WFT)andϕ(∆WMultiLoRA,∆WFT). Similarity between\n",
            "fine-tuning of two different runs ϕ(∆WFT′,∆WFT)is also calculated for reference.\n",
            "MultiLoRA resembles fine-tuning more than LoRA in terms of subspace span. According to visualization in Figure\n",
            "4, MultiLoRAn=3\n",
            "r=32exhibits stronger resemblance to fine-tuning than LoRA r=96under the same parameter budget,\n",
            "8PRIME AI paper\n",
            "indicating that subspace of the weight update matrix of MultiLoRA is closer to that of fine-tuning. Heatmap of LoRA is\n",
            "generally dimmer but top singular vectors still present overlapping of subspaces to fine-tuning to some degree.\n",
            "Scaling up ndoes not necessarily augment MultiLoRA subspace similarity to fine-tuning. Another thing to\n",
            "behold is barely visible difference between MultiLoRAn=3\n",
            "r=32and MultiLoRAn=5\n",
            "r=32, meaning that increasing parallel\n",
            "LoRA number ndoes not necessarily make subspace closer to fine-tuning. The same trend can be observed on other\n",
            "weights of different depths in decoder stack (more at Appendix C).\n",
            "5.1.2 Singular Value Distribution Comparison\n",
            "1e1.0 1e1.5 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "101\n",
            "100101102Count\n",
            "1e1.0 1e1.5 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "100101102Count\n",
            "LoRAr=96 Fine-tuning MultiLoRAn=3\n",
            "r=32MultiLoRAn=5\n",
            "r=32\n",
            "Figure 5: Singular value distribution of weight update matrices ∆Wofk_proj (Left) and v_proj (Right ). Our proposed\n",
            "MultiLoRA exhibits higher degree of resemblance to fine-tuning. Scaling up nproduces more democratic unitary\n",
            "transform contributions.\n",
            "In previous Section 5.1.1, we measure the subspace similarity between unitary singular vectors but without knowing the\n",
            "importance of aforementioned singular vectors, we cannot conclude the higher resemblance between MultiLoRA and\n",
            "fine-tuning. Thus, we investigate into singular value distribution by plotting histogram of singular value as in Section\n",
            "3.2.\n",
            "ForΣ =diag(s)obtained from SVD(∆W), we count the number of sover a series of thresholds and average the\n",
            "statistics of the same module over different depths of decoder layers. We calculate negative logarithms −log(s)for\n",
            "better visibility since more than 95% of singular values are within [0,1]. Results are shown in Figure 5.\n",
            "MultiLoRA balances subspace contributions compared to LoRA. MultiLoRA shows similar distribution as fine-\n",
            "tuning where number of singular value of MultiLoRA decreases with its magnitude. Given the explicit low rank\n",
            "r << d , LoRA shows heavy reliance on a small group of top singular vectors but MultiLoRAdemocratizes contributions\n",
            "of singular vectors.\n",
            "Scaling up nmakes MultiLoRA amplify features at a more fine-grained level. Comparing MultiLoRAn=3\n",
            "r=32and\n",
            "MultiLoRAn=5\n",
            "r=32, histograms of {s| −log(s)>1e1.6}are almost identical but MultiLoRAn=5\n",
            "r=32shows wider spectrum\n",
            "as proportion of small singular values increases. A wider spectrum covering small singular values enables more\n",
            "fine-grained fitting of ∆Was fine-tuning.\n",
            "5.2 Comparison among MultiLoRA\n",
            "In this section, to demonstrate MultiLoRA accomplishes the design goal of depolarization, we compare in pair sub-\n",
            "LoRAs. From heatmap, subspace similarity between top-1 singular vectors is around 0.6. Comparison between ∆Wi=5\n",
            "and∆Wi=3shows relatively low similarity. The variance of subspace similarities indicates a more fine-grained pattern\n",
            "decomposition.\n",
            "9PRIME AI paper\n",
            "0 5 10 15\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15(Wn=5\n",
            "i=1,Wn=5\n",
            "i=3)\n",
            "0 5 10 15\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15(Wn=5\n",
            "i=3,Wn=5\n",
            "i=3)\n",
            "0 5 10 15\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15(Wn=5\n",
            "i=4,Wn=5\n",
            "i=3)\n",
            "0 5 10 15\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15(Wn=5\n",
            "i=5,Wn=5\n",
            "i=3)\n",
            "0.00.20.40.60.81.0\n",
            "Figure 6: Subspace similarity between parallel module of MultiLoRA. We analyze the MultiLoRA targeting down_proj\n",
            "in the first decoder layer. Each individual module produces close but not identical subspaces, thus augmenting the\n",
            "general expressiveness of MultiLoRA.\n",
            "5.3 Underlying Mechanisms of LoRA and MultiLoRA\n",
            "In previous sections, we study the difference in subspace similarity and singular value distribution by applying SVD\n",
            "on weight update matrices of experimented methods. Our observations shed light on underlying mechanisms of\n",
            "LoRA and MultiLoRA. From the singular value distribution of fine-tuning, we learn that fine-tuning fits residual\n",
            "weights by aggregating a large number (usually equals to rank of weight matrix) of relatively less important unitary\n",
            "transforms. Given the low rank limitation, LoRA and MultiLoRA fits residual weights with r≪min(d, k)unitary\n",
            "transforms. The low subspace similarity to fine-tuning and dominance in singular value distribution observed in\n",
            "LoRA show that LoRA tends to decompose the residual weights into unitary transforms of large importance. Meanwhile,\n",
            "MultiLoRA democratizes influences of unitary transforms by assigning smaller importance to its unitary transforms\n",
            "similar to fine-tuning. With democratized unitary subspaces, MultiLoRA produces better complex multi-task learning\n",
            "performance.\n",
            "6 Conclusion\n",
            "In conclusion, our study introduces MultiLoRA, a novel approach that enhances multi-task adaptation in language\n",
            "models. By mitigating the dominance of unitary transforms of LoRA, we successfully improve performance in complex\n",
            "multi-task scenarios. Our proposed method focuses on scaling LoRA modules horizontally and modifying parameter\n",
            "initialization to reduce parameter dependency, thereby creating more balanced unitary subspaces. Additionally,\n",
            "we construct a comprehensive dataset covering a wide range of tasks of interest for generative LLMs. Through\n",
            "extensive experimentation, we have demonstrated that MultiLoRA outperforms single LoRA and achieves comparable\n",
            "performance to fine-tuning across multiple benchmarks and model scales. MultiLoRA stabilizes multi-task adaptation\n",
            "especially for smaller models. Furthermore, our investigation into weight update matrices reveals a significant reduction\n",
            "in dependency on top singular vectors and a more equitable contribution of unitary subspaces in MultiLoRA. Overall,\n",
            "MultiLoRA provides an efficient and effective solution for multi-task adaptation in language models.\n",
            "10PRIME AI paper\n",
            "References\n",
            "[1]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional\n",
            "transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of\n",
            "the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis,\n",
            "MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages 4171–4186. Association for Computational\n",
            "Linguistics, 2019.\n",
            "[2]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\n",
            "lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger,\n",
            "Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher\n",
            "Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\n",
            "Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In\n",
            "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing\n",
            "Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\n",
            "[3]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\n",
            "moyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR , abs/1907.11692,\n",
            "2019.\n",
            "[4]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\n",
            "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave,\n",
            "and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv , abs/2302.13971, 2023.\n",
            "[5]Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\n",
            "Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,\n",
            "and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res. , 2022, 2022.\n",
            "[6]Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. Lora:\n",
            "Low-rank adaptation of large language models, 2021.\n",
            "[7]Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo,\n",
            "Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. CoRR , abs/1902.00751, 2019.\n",
            "[8]Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be\n",
            "comparable to fine-tuning universally across scales and tasks. CoRR , abs/2110.07602, 2021.\n",
            "[9]Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\n",
            "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In NeurIPS , 2022.\n",
            "[10] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient\n",
            "multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of\n",
            "the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language\n",
            "Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 565–576.\n",
            "Association for Computational Linguistics, 2021.\n",
            "[11] Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogério Feris, Huan Sun, and Yoon Kim. Multitask prompt\n",
            "tuning enables parameter-efficient transfer learning. In The Eleventh International Conference on Learning\n",
            "Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\n",
            "[12] Deze Wang, Boxing Chen, Shanshan Li, Wei Luo, Shaoliang Peng, Wei Dong, and Xiangke Liao. One adapter for\n",
            "all programming languages? adapter tuning for code search and summarization. In 45th IEEE/ACM International\n",
            "Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023 , pages 5–16. IEEE,\n",
            "2023.\n",
            "[13] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and\n",
            "Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. In Proceedings of the 2022\n",
            "Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\n",
            "Emirates, December 7-11, 2022 , pages 5744–5760. Association for Computational Linguistics, 2022.\n",
            "[14] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji\n",
            "Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. Deepspeed- inference: Enabling efficient\n",
            "inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance\n",
            "Computing, Networking, Storage and Analysis, Dallas, TX, USA, November 13-18, 2022 , pages 46:1–46:15. IEEE,\n",
            "2022.\n",
            "[15] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\n",
            "Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/\n",
            "tatsu-lab/stanford_alpaca , 2023.\n",
            "11PRIME AI paper\n",
            "[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-\n",
            "suring massive multitask language understanding. In 9th International Conference on Learning Representations,\n",
            "ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021.\n",
            "[17] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\n",
            "Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve\n",
            "math word problems. CoRR , abs/2110.14168, 2021.\n",
            "[18] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\n",
            "Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In\n",
            "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing\n",
            "Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pages 3261–3275, 2019.\n",
            "[19] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min\n",
            "Chan, Weize Chen, et al. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained\n",
            "language models. arXiv preprint arXiv:2203.06904 , 2022.\n",
            "[20] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si.\n",
            "On the effectiveness of adapter-based tuning for pretrained language model adaptation. In Proceedings of the 59th\n",
            "Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\n",
            "Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 ,\n",
            "pages 2208–2222. Association for Computational Linguistics, 2021.\n",
            "[21] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for\n",
            "transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association\n",
            "for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 1–9.\n",
            "Association for Computational Linguistics, 2022.\n",
            "[22] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in\n",
            "GPT. In NeurIPS , 2022.\n",
            "[23] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.\n",
            "Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on\n",
            "Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\n",
            "[24] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa.\n",
            "Unipelt: A unified framework for parameter-efficient language model tuning. In Proceedings of the 60th Annual\n",
            "Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\n",
            "May 22-27, 2022 , pages 6253–6264. Association for Computational Linguistics, 2022.\n",
            "[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level\n",
            "performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision, ICCV 2015,\n",
            "Santiago, Chile, December 7-13, 2015 , pages 1026–1034. IEEE Computer Society, 2015.\n",
            "[26] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In\n",
            "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010,\n",
            "Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010 , volume 9 of JMLR Proceedings , pages 249–256. JMLR.org,\n",
            "2010.\n",
            "[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\n",
            "Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary\n",
            "DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\n",
            "Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information\n",
            "Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,\n",
            "December 8-14, 2019, Vancouver, BC, Canada , pages 8024–8035, 2019.\n",
            "[28] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized\n",
            "llms. CoRR , abs/2305.14314, 2023.\n",
            "[29] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora.\n",
            "Fine-tuning language models with just forward passes. CoRR , abs/2305.17333, 2023.\n",
            "[30] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft:\n",
            "State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft , 2022.\n",
            "[31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training\n",
            "trillion parameter models. In Proceedings of the International Conference for High Performance Computing,\n",
            "Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020 , page 20.\n",
            "IEEE/ACM, 2020.\n",
            "12PRIME AI paper\n",
            "[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on\n",
            "Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.\n",
            "[33] Sebastian Ruder. An overview of gradient descent optimization algorithms. CoRR , abs/1609.04747, 2016.\n",
            "[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\n",
            "Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,\n",
            "Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M.\n",
            "Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on\n",
            "Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online, October 2020.\n",
            "Association for Computational Linguistics.\n",
            "13PRIME AI paper\n",
            "A Hyperparameters\n",
            "We list hyperparameters used in our experiments in the Table 3. Batch size of 32 is achieved for LLaMA-30B and\n",
            "LLaMA-65B with gradient accumulation. We use default values give by Huggingface transformers[ 34] trainer for most\n",
            "of the optimizer hyperparameters.\n",
            "Expeiment Hyperparameters Values\n",
            "Batch Size per GPU 32\n",
            "Number of Epochs 2\n",
            "Fine-TuneLearning Rate 5e-6\n",
            "LR Schedule Linear\n",
            "Optimizer AdamW\n",
            "Warmup Ratio 0.05\n",
            "LoRALearning Rate 5e-5\n",
            "LR Schedule Linear\n",
            "Optimizer AdamW\n",
            "Warmup Ratio 0.05\n",
            "MultiLoRALearning Rate 5e-5\n",
            "LR Schedule Linear\n",
            "Optimizer AdamW\n",
            "Warmup Ratio 0.05\n",
            "Table 3: Training Hyperparameters used in our experiments.\n",
            "B Singular Value Distribution\n",
            "1e1.0 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "05101520Count\n",
            "Guanacor=64Alpacar=96MMLUr=96\n",
            "(a)v_proj\n",
            "1e1.0 1e2.0\n",
            "Negative Logarithm of Singular Value log(s)\n",
            "0.02.55.07.510.012.515.017.520.0Count\n",
            "Guanacor=64Alpacar=96MMLUr=96 (b)q_proj\n",
            "Figure 7: Singular Value Distribution of (a) v_proj and (b) q_proj of weight update matrices trained on different datasets.\n",
            "In Section 3.2, we find that LoRA’s weight update matrices of are dominated by small group of unitary transforms.\n",
            "To further support this, we analyzed LoRA modules obtained from training on various datasets or using publicly\n",
            "available resources. We use LoRA modules obtained by training on public available datasets (MMLU and Alpaca) or\n",
            "downloading publicly available resources (Guanaco4).\n",
            "4Downloadable at https://huggingface.co/timdettmers/guanaco-7b/tree/main\n",
            "14PRIME AI paper\n",
            "Figure 7 plots histograms of singular values of weight update matrices of q_proj andv_proj . To enhance visualization,\n",
            "the negative logarithm of the singular values (−log(s))is calculated, given that most values are smaller than 0.1\n",
            "Mean values are used to aggregate statistics across all decoder layers. The histograms for both modules exhibit a striking\n",
            "similarity. The triangular shape of the histograms indicates the dominance of the top singular vectors, as mentioned in\n",
            "Section 3.2. It is worth noting that this dominance arises from the inherent design of LoRA, as we do not deliberately\n",
            "alter its structure or use unconventional datasets.\n",
            "C Subspace Similarities of other modules of different depth\n",
            "In Section 5.1.1, we use cosine similarity between top singular vectors to measure subspace overlap of ∆W. Here, we\n",
            "present more visualizations on different modules from different depths of the decoder stack.\n",
            "0 10 20\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WQ\n",
            "1×LoRA,WQ\n",
            "FT)\n",
            "0 10 20\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WQ\n",
            "3×LoRA,WQ\n",
            "FT)\n",
            "0 10 20\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "5×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WQ\n",
            "FT,WQ\n",
            "FT)\n",
            "0.00.20.40.60.81.0\n",
            "(a)up_proj of MLP in the 28 thdecoder layer.\n",
            "0 10 20\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "1×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "3×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "5×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "FT,WV\n",
            "FT)\n",
            "0.00.20.40.60.81.0\n",
            "(b)q_proj of self attention in the 1 stdecoder layer.\n",
            "0 10 20\n",
            "(1)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "1×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(2)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "3×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(3)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "5×LoRA,WV\n",
            "FT)\n",
            "0 10 20\n",
            "(4)0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25(WV\n",
            "FT,WV\n",
            "FT)\n",
            "0.00.20.40.60.81.0\n",
            "(c)k_proj of self attention in the 14 thdecoder layer.\n",
            "Figure 8: Subspace similarity of LoRA and MultiLoRA to fine-tuningof different modules at different depths.\n",
            "We randomly choose up_proj ,q_proj andk_proj of MLP and self attention module at different depths to compare weight\n",
            "update matrices of LoRA and MultiLoRA to fine-tuning. The heatmap visualization shows higher degree of similarity\n",
            "of MultiLoRA as observed in Section 5.1.1. Our observation sheds light on the mechanism of LoRAthat the residual\n",
            "weight is decomposed into a small number group of unitary transform of large importance. Important unitary transforms\n",
            "15PRIME AI paper\n",
            "of small number hinders the model handling complicated multi-task learning. Meanwhile, MultiLoRA manages to fits\n",
            "the residual weight more similar to fine-tuning which gathers a larger number of relatively less important transforms.\n",
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.create_documents([all_text])"
      ],
      "metadata": {
        "id": "KxOVn-G5J0hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "directory = 'index_store'\n",
        "vector_index = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
        "vector_index.save_local(directory)"
      ],
      "metadata": {
        "id": "87G_3hCQKFhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b48a741-4345-49cd-bbfe-cb2b1762cc5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = FAISS.load_local('index_store', OpenAIEmbeddings())\n",
        "retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":6})\n",
        "qa_interface = RetrievalQA.from_chain_type(llm=ChatOpenAI(),\n",
        "                                           chain_type=\"stuff\",\n",
        "                                           retriever=retriever,\n",
        "                                           return_source_documents=True\n",
        "                                           )"
      ],
      "metadata": {
        "id": "jKzEdhPtLQLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0848ae8b-282a-434c-f9d5-632d7898a4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(question):\n",
        "  response_object = qa_interface(question)\n",
        "  question = response_object['query']\n",
        "  response = response_object['result']\n",
        "  # print(f\"Query: {question}\")\n",
        "  # print(f\"Response: {response}\")\n",
        "  return response\n",
        "query = 'Summarize this research paper'\n",
        "get_response(query)"
      ],
      "metadata": {
        "id": "HY1k5GRvyyiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "b083d5a3-8033-4b74-81e6-780898ef945c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The research paper introduces a framework called FEDHCA2 for Hetero-Client Federated Multi-Task Learning (HC-FMTL), addressing challenges like model incongruity, data heterogeneity, and task heterogeneity. The framework aims to learn personalized models through Hyper Conflict-Averse Aggregation, Hyper Cross Attention Aggregation, and Hyper Aggregation Weights. The paper discusses the importance of collaboration among clients with varying tasks and models. The authors propose a method that optimizes personalized client models using Multi-Task Learning. They also emphasize the significance of task interaction in decoder aggregation. The paper concludes by mentioning plans for future research, including extending the study to more tasks, exploring different model architectures, and enhancing the approach with novelties like changing gravity or adding wind.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts and Responses\n",
        "The section below shows how we can use this to answer questions regarding items that are within the text. When we ask about a random question about a budget speech, you will see the LLM respons that it does not have the ability to create a summary of a specific budget speech."
      ],
      "metadata": {
        "id": "OMO6S37uyHFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"What are the two key factors of reward functions?\"\n",
        "get_response(Query)"
      ],
      "metadata": {
        "id": "KtCgUSYhcabr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "108ba422-4ba9-44cd-cbfc-68799fdabb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The two key factors of reward functions are:\\n1. Tasks must be manageable by existing single-task reinforcement learning algorithms.\\n2. Reward functions should exhibit shared structure across tasks to promote the transfer of knowledge and skills between tasks, enhancing the effectiveness of multi-task and meta-reinforcement learning.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"What is Multi task learning?\"\n",
        "get_response(Query)"
      ],
      "metadata": {
        "id": "09QBZM_Hehv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c8598edc-e31d-4c99-d6a1-72f1f70a04ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multi-Task Learning (MTL) is a machine learning approach that aims to improve overall performance by training a single model to perform multiple tasks simultaneously. This method can reduce the number of parameters needed and speed up training or inference compared to training individual models for each task separately. The research in MTL is focused on network architecture design and optimization strategies for multi-task learning.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Query = ''' Various data imbalances that naturally arise in a multi-territory personalized recommender system can lead to a significant item bias\n",
        "for globally prevalent items. A locally popular item can be overshadowed by a globally prevalent item. Moreover, users’ viewership\n",
        "patterns/statistics can drastically change from one geographic location to another which may suggest to learn specific user embeddings.\n",
        "In this paper, we propose a multi-task learning (MTL) technique, along with an adaptive upsampling method to reduce popularity bias\n",
        "in multi-territory recommendations. Our proposed framework is designed to enrich training examples with active users representation\n",
        "through upsampling, and capable of learning geographic-based user embeddings by leveraging MTL. Through experiments, we\n",
        "demonstrate the effectiveness of our framework in multiple territories compared to a baseline not incorporating our proposed\n",
        "techniques. Noticeably, we show improved relative gain of up to 65.27% in PR-AUC metric. A case study is presented to demonstrate\n",
        "the advantages of our methods in attenuating the popularity bias of global items'''\n",
        "get_response(Query)"
      ],
      "metadata": {
        "id": "RIIiud3Ua85l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "42043a8e-72f4-4479-f94a-de4472e5bf13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The proposed multi-task learning (MTL) technique, along with the adaptive upsampling method, aims to reduce popularity bias in multi-territory recommendations by enriching training examples with active user representations and learning geographic-based user embeddings. Through experiments, the framework has shown improved effectiveness in multiple territories compared to a baseline without these techniques, with a relative gain of up to 65.27% in the PR-AUC metric. Additionally, a case study demonstrates the advantages of these methods in mitigating the popularity bias of globally prevalent items in a personalized recommender system.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conlusion\n",
        "In this portion of the assignment, I succesfully used the langchain model to perform RAG on 4 research papers."
      ],
      "metadata": {
        "id": "rd9LOQ68bWZP"
      }
    }
  ]
}